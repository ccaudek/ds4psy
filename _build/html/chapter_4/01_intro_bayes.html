

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Modellazione bayesiana &#8212; ds4p</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="../_static/a11y.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/custom.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-VMXNE4BCDL"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-VMXNE4BCDL');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_4/01_intro_bayes';</script>
    <link rel="canonical" href="https://ccaudek.github.io/ds4psy/chapter_4/01_intro_bayes.html" />
    <link rel="shortcut icon" href="../_static/increasing.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Pensare ad una proporzione in termini soggettivi" href="02_subj_prop.html" />
    <link rel="prev" title="Inferenza bayesiana" href="introduction_part_4.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="ds4p - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="ds4p - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Benvenuti
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_1/introduction_chapter_1.html">Python</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/01_python_1.html">Python (1)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/02_python_2.html">Python (2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_python.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/03_numpy.html">NumPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_numpy.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/04_pandas.html">Pandas (1)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/05_pandas_aggregate.html">Pandas (2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/06_pandas_functions.html">Pandas (3)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_pandas.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/07_matplotlib.html">Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/08_seaborn.html">Seaborn</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_matplotlib.html">✏️ Esercizi</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_2/introduction_chapter_2.html">Statistica descrittiva</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/01_key_notions.html">Concetti chiave</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_key_notions.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/02_measurement.html">La misurazione in psicologia</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_scales.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/03_freq_distr.html">Dati e frequenze</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_sums.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/04_loc_scale.html">Indici di posizione e di scala</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/05_correlation.html">Le relazioni tra variabili</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/06_causality.html">Correlazione e causazione</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/07_crisis.html">La crisi della generalizzabilità</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_eda.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_mehr_song_spelke.html">✏️ Esercizi</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_3/introduction_chapter_3.html">Probabilità</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/01_intro_prob.html">Introduzione al calcolo delle probabilità</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_prob.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/02_conditional_prob.html">Probabilità condizionata</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_cond_prob.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/03_bayes_theorem.html">Il teorema di Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_bayes_theorem.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/04_expval_var.html">Variabili casuali</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/04a_sampling_distr.html">Stime, stimatori e parametri</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/04b_illusion.html">Incertezza inferenziale e variabilità dei risultati</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_rv_discrete.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/05_joint_prob.html">Probabilità congiunta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_joint_prob.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/06_density_func.html">La funzione di densità di probabilità</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/07_discr_rv_distr.html">Distribuzioni di v.c. discrete</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_binomial.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/08_cont_rv_distr.html">Distribuzioni di v.c. continue</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_gaussian.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_beta_distr.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/09_likelihood.html">La verosimiglianza</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/10_rescorla_wagner.html">Apprendimento per rinforzo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_likelihood.html">✏️ Esercizi</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="introduction_part_4.html">Inferenza bayesiana</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Modellazione bayesiana</a></li>
<li class="toctree-l2"><a class="reference internal" href="02_subj_prop.html">Pensare ad una proporzione in termini soggettivi</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_conjugate_families_1.html">Distribuzioni coniugate (1)</a></li>
<li class="toctree-l2"><a class="reference internal" href="04_conjugate_families_2.html">Distribuzioni coniugate (2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_summary_posterior.html">Sintesi a posteriori</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_conjugate.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="06_balance-prior-post.html">L’influenza della distribuzione a priori</a></li>
<li class="toctree-l2"><a class="reference internal" href="10_metropolis.html">Monte Carlo a Catena di Markov</a></li>
<li class="toctree-l2"><a class="reference internal" href="11_beta_binomial_pymc.html">Inferenza bayesiana con PyMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="12_jax.html">Usare JAX per un campionamento più veloce</a></li>
<li class="toctree-l2"><a class="reference internal" href="13_preliz.html">Scegliere le distribuzioni a priori</a></li>
<li class="toctree-l2"><a class="reference internal" href="16_summary_posterior_pymc.html">Metodi di sintesi della distribuzione a posteriori</a></li>
<li class="toctree-l2"><a class="reference internal" href="17_prediction.html">La predizione bayesiana</a></li>
<li class="toctree-l2"><a class="reference internal" href="18_mcmc_diagnostics.html">Diagnostica delle catene markoviane</a></li>
<li class="toctree-l2"><a class="reference internal" href="19_odds_ratio.html">Analisi bayesiana dell’odds-ratio</a></li>
<li class="toctree-l2"><a class="reference internal" href="20_poisson_model.html">Modello di Poisson</a></li>
<li class="toctree-l2"><a class="reference internal" href="21_poisson_sim.html">Modello di Poisson: derivazione analitica e MCMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_freq.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="22_normal_normal_model.html">Inferenza bayesiana su una media</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_one_mean.html">✏️ Esercizio</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_one_mean_2.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="23_two_groups.html">Confronto tra due gruppi</a></li>
<li class="toctree-l2"><a class="reference internal" href="24_multiple_groups.html">Gruppi multipli</a></li>
<li class="toctree-l2"><a class="reference internal" href="30_entropy.html">Entropia</a></li>
<li class="toctree-l2"><a class="reference internal" href="31_kl.html">La divergenza di Kullback-Leibler</a></li>
<li class="toctree-l2"><a class="reference internal" href="40_hier_beta_binom.html">Modello gerarchico beta-binomiale</a></li>
<li class="toctree-l2"><a class="reference internal" href="41_hier_poisson.html">Modello gerarchico di Poisson</a></li>
<li class="toctree-l2"><a class="reference internal" href="42_hier_gaussian.html">Modello gerarchico gaussiano</a></li>
<li class="toctree-l2"><a class="reference internal" href="hssm.html">Drift Diffusion Model</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_5/introduction_part_5.html">Analisi della regressione</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_1.html">Il modello di regressione lineare</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_2.html">Analisi bayesiana del modello di regressione lineare</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_params_recovery.html">Analisi di simulazione per la stima dei parametri nel modello di regressione</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_3.html">Zucchero sintattico</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_4.html">Confronto tra le medie di due gruppi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_5.html">Il modello lineare gerarchico</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_7.html">Regressione robusta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_simpson.html">Paradosso di Simpson</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_reglin_1.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_reglin_2.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_reglin_3.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_reglin_4.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_logistic_reg.html">Modello di regressione logistica</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_binomial_reg.html">Regressione binomiale</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_covid.html">Inferenza controfattuale: calcolo delle morti in eccesso dovute al COVID-19</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_counterfactual.html">Analisi causale con PyMC</a></li>

<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_stab.html">✏️ Esercizi</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_6/introduction_part_6.html">Inferenza frequentista</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_estimation.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/02_conf_interv.html">Intervallo di confidenza</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/03_test_ipotesi.html">Significatività statistica</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_interpretation_test.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_significato_test.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/04_two_ind_samples.html">Test t di Student per campioni indipendenti</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_test_media_pop.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_medie_pop_ampie.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_medie_pop_piccoli.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_campioni_appaiati.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_confronto_proporzioni.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/05_limiti_stat_frequentista.html">Limiti dell’inferenza frequentista</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/06_s_m_errors.html">Crisi della replicabilità</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../references/bibliography.html">Bibliografia</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_7/introduction_appendix.html">Appendici</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a00_installation.html">Ambiente di lavoro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a01_markdown.html">Jupyter Notebook</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a02_shell.html">La Shell</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a03_virtual_env.html">Ambiente virtuale</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a10_math_symbols.html">Simbologia di base</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a11_numbers.html">Numeri binari, interi, razionali, irrazionali e reali</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a12_sum_notation.html">Simbolo di somma (sommatorie)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a13_sets.html">Insiemi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a14_combinatorics.html">Calcolo combinatorio</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a15_calculus.html">Per liberarvi dai terrori preliminari</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a20_kde_plot.html">Kernel Density Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a30_prob_tutorial.html">Esercizi di probabilità discreta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a40_rng.html">Generazione di numeri casuali</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a44_montecarlo.html">Simulazione Monte Carlo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a45_mcmc.html">Catene di Markov</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a50_lin_fun.html">La funzione lineare</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a51_reglin_1.html">Regressione lineare bivariata</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a52_reglin_2.html">Regressione lineare con Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a53_reglin_4.html">Posterior Predictive Checks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a60_ttest_exercises.html">Esercizi sull’inferenza frequentista</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a70_predict_counts.html">La predizione delle frequenze</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/ccaudek/ds4psy/blob/main/docs/chapter_4/01_intro_bayes.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapter_4/01_intro_bayes.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Modellazione bayesiana</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#esplorazione-dei-fondamenti-dei-metodi-bayesiani-in-psicologia">Esplorazione dei Fondamenti dei Metodi Bayesiani in Psicologia</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#introduzione-ai-principi-bayesiani">Introduzione ai Principi Bayesiani</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#crescita-dei-metodi-bayesiani-nella-psicologia">Crescita dei Metodi Bayesiani nella Psicologia</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#il-ciclo-continuo-dell-apprendimento-bayesiano">Il Ciclo Continuo dell’Apprendimento Bayesiano</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dalla-generazione-dei-dati-alla-modellizzazione-bayesiana">Dalla Generazione dei Dati alla Modellizzazione Bayesiana</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#riallocazione-della-credibilita-e-aggiornamento-sequenziale">Riallocazione della Credibilità e Aggiornamento Sequenziale</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#la-modellizzazione-statistica-collegamento-tra-teoria-e-dati">La Modellizzazione Statistica: Collegamento tra Teoria e Dati</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#elementi-fondamentali-della-modellazione-statistica-bayesiana">Elementi Fondamentali della Modellazione Statistica Bayesiana</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#il-processo-di-sviluppo-di-un-modello-bayesiano">Il Processo di Sviluppo di un Modello Bayesiano</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#riesame-del-teorema-di-bayes">Riesame del Teorema di Bayes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#l-aggiornamento-bayesiano">L’aggiornamento bayesiano</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linguaggi-di-programmazione-probabilistici">Linguaggi di programmazione probabilistici</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#notazione">Notazione</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#la-distribuzione-a-priori">La Distribuzione a Priori</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distribuzioni-a-priori-non-informative">Distribuzioni a Priori Non Informative</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distribuzioni-a-priori-debolmente-informative">Distribuzioni a Priori Debolmente Informative</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distribuzioni-a-priori-informativa">Distribuzioni a Priori Informativa</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#la-verosimiglianza-marginale">La Verosimiglianza Marginale</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metodi-di-estimazione-della-distribuzione-a-posteriori">Metodi di Estimazione della Distribuzione a Posteriori</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#il-flusso-di-lavoro-bayesiano">Il flusso di lavoro bayesiano</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fasi-del-flusso-di-lavoro">Fasi del flusso di lavoro</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#commenti-e-considerazioni-finali">Commenti e considerazioni finali</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="modellazione-bayesiana">
<span id="bayes-workflow-notebook"></span><h1>Modellazione bayesiana<a class="headerlink" href="#modellazione-bayesiana" title="Permalink to this heading">#</a></h1>
<p>L’obiettivo di questo Capitolo è di introdurre il quadro concettuale dela modellizzazione bayesiana.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">uniform</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;retina&#39;
<span class="n">RANDOM_SEED</span> <span class="o">=</span> <span class="mi">8927</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">RANDOM_SEED</span><span class="p">)</span>
<span class="n">az</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;arviz-darkgrid&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<section id="esplorazione-dei-fondamenti-dei-metodi-bayesiani-in-psicologia">
<h2>Esplorazione dei Fondamenti dei Metodi Bayesiani in Psicologia<a class="headerlink" href="#esplorazione-dei-fondamenti-dei-metodi-bayesiani-in-psicologia" title="Permalink to this heading">#</a></h2>
<section id="introduzione-ai-principi-bayesiani">
<h3>Introduzione ai Principi Bayesiani<a class="headerlink" href="#introduzione-ai-principi-bayesiani" title="Permalink to this heading">#</a></h3>
<p>Nel campo della psicologia, l’adozione dei metodi bayesiani ha guadagnato crescente attenzione e utilizzo. La formalizzazione dell’uso delle probabilità, un elemento centrale dell’approccio bayesiano, permette di rappresentare numericamente insiemi di convinzioni razionali e di aggiornare queste convinzioni in base a nuove informazioni attraverso la regola di Bayes. Questo processo, noto come inferenza bayesiana, è particolarmente rilevante in psicologia dove le incertezze e le informazioni parziali sono la norma.</p>
</section>
<section id="crescita-dei-metodi-bayesiani-nella-psicologia">
<h3>Crescita dei Metodi Bayesiani nella Psicologia<a class="headerlink" href="#crescita-dei-metodi-bayesiani-nella-psicologia" title="Permalink to this heading">#</a></h3>
<p>Il paradigma bayesiano in psicologia ha visto un notevole aumento di interesse grazie alla disponibilità di risorse educative e pubblicazioni che facilitano l’integrazione dei modelli bayesiani nell’analisi dei dati psicologici. Autori come <span id="id1">Brooks [<a class="reference internal" href="../references/bibliography.html#id29" title="Stephen P Brooks. Bayesian computation: a statistical revolution. Philosophical Transactions of the Royal Society of London. Series A: Mathematical, Physical and Engineering Sciences, 361(1813):2681–2697, 2003.">Bro03</a>]</span>, <span id="id2">Van De Schoot <em>et al.</em> [<a class="reference internal" href="../references/bibliography.html#id28" title="Rens Van De Schoot, Sonja D Winter, Oisín Ryan, Mariëlle Zondervan-Zwijnenburg, and Sarah Depaoli. A systematic review of bayesian articles in psychology: the last 25 years. Psychological Methods, 22(2):217–239, 2017.">VDSWR+17</a>]</span>, <span id="id3">Kruschke [<a class="reference internal" href="../references/bibliography.html#id136" title="John Kruschke. Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press, 2014.">Kru14</a>]</span>, <span id="id4">McElreath [<a class="reference internal" href="../references/bibliography.html#id109" title="Richard McElreath. Statistical rethinking: A Bayesian course with examples in R and Stan. CRC Press, Boca Raton, Florida, 2nd edition edition, 2020.">McE20</a>]</span>, <span id="id5">Johnson <em>et al.</em> [<a class="reference internal" href="../references/bibliography.html#id88" title="Alicia A. Johnson, Miles Ott, and Mine Dogucu. Bayes Rules! An Introduction to Bayesian Modeling with R. CRC Press, 2022.">JOD22</a>]</span>), hanno contribuito a questa evoluzione, fornendo agli psicologi una solida base per applicare approcci bayesiani ai dati psicologici.</p>
</section>
<section id="il-ciclo-continuo-dell-apprendimento-bayesiano">
<h3>Il Ciclo Continuo dell’Apprendimento Bayesiano<a class="headerlink" href="#il-ciclo-continuo-dell-apprendimento-bayesiano" title="Permalink to this heading">#</a></h3>
<p>Il framework bayesiano costituisce un modello matematico che cattura il processo di apprendimento continuo. Fondato sul teorema di Bayes, questo ciclo iterativo consente l’aggiornamento sequenziale delle informazioni, in cui la distribuzione a posteriori di un ciclo diventa la distribuzione a priori del ciclo successivo. Questa dinamica di aggiornamento costante delle credenze gioca un ruolo cruciale nell’ambito della ricerca e della creatività, come sottolineato <span id="id6">[<a class="reference internal" href="../references/bibliography.html#id18" title="Eric-Jan Wagenmakers, Gilles Dutilh, and Alexandra Sarafoglou. The creativity-verification cycle in psychological science: new methods to combat old idols. Perspectives on Psychological Science, 13(4):418–427, 2018.">WDS18</a>]</span>.</p>
</section>
<section id="dalla-generazione-dei-dati-alla-modellizzazione-bayesiana">
<h3>Dalla Generazione dei Dati alla Modellizzazione Bayesiana<a class="headerlink" href="#dalla-generazione-dei-dati-alla-modellizzazione-bayesiana" title="Permalink to this heading">#</a></h3>
<p>Nel contesto dell’inferenza bayesiana, uno degli aspetti centrali riguarda l’analisi dei processi generativi dei dati, noti come “Data-Generating Processes” (DGP), che sono responsabili della produzione dei dati osservati. L’identificazione di questi meccanismi sottostanti, anche se non direttamente osservabili, riveste un’importanza fondamentale per la comprensione e la rappresentazione delle ipotesi sulla realtà attraverso l’utilizzo di modelli statistici.</p>
</section>
<section id="riallocazione-della-credibilita-e-aggiornamento-sequenziale">
<h3>Riallocazione della Credibilità e Aggiornamento Sequenziale<a class="headerlink" href="#riallocazione-della-credibilita-e-aggiornamento-sequenziale" title="Permalink to this heading">#</a></h3>
<p>La dinamica dell’aggiornamento delle informazioni nel contesto bayesiano è stata esemplificata attraverso l’uso dell’esempio di Sherlock Holmes. Come dimostrato nel libro <span id="id7">Kruschke [<a class="reference internal" href="../references/bibliography.html#id136" title="John Kruschke. Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press, 2014.">Kru14</a>]</span>, questo processo si basa sull’idea che acquisire nuove prove in un’indagine può portare all’esclusione di una causa tra molte possibili, con conseguente redistribuzione delle probabilità tra le cause rimanenti.</p>
<p>Per comprendere meglio questo concetto, consideriamo un’indagine su un crimine con quattro possibili colpevoli, identificati come A, B, C e D. Inizialmente, tutte e quattro le opzioni hanno probabilità simili di essere il colpevole. Tuttavia, man mano che Sherlock Holmes raccoglie nuove prove e informazioni, potrebbe essere in grado di escludere uno dei sospettati, diciamo il colpevole A, sulla base di queste nuove prove. Quando ciò avviene, la probabilità associata a A viene annullata e riallocata tra i rimanenti sospettati, ossia B, C e D. Di conseguenza, la probabilità di ciascuno di loro come colpevole diventa più elevata e, inizialmente, tutti hanno probabilità uguali. Questo processo rappresenta l’essenza dell’aggiornamento bayesiano delle credenze.</p>
<p>Tuttavia, è importante notare che in psicologia e in molti altri contesti, raramente un’ipotesi o una causa viene completamente esclusa. Piuttosto, la sua credibilità viene ridimensionata o modificata in base alle nuove evidenze, aumentando la plausibilità delle alternative. Questo principio è espressamente sintetizzato nella famosa citazione di Dennis Lindley: “Il posterior di oggi diventa il prior di domani”. In altre parole, ciò che impariamo oggi influenzerà le nostre credenze future e il processo di aggiornamento continuo delle nostre conoscenze.</p>
<p>Questa dinamica di riallocazione della credibilità e aggiornamento sequenziale è fondamentale nel contesto dell’applicazione dei metodi bayesiani, inclusi quelli utilizzati nella psicologia, per valutare e rivedere costantemente le nostre ipotesi in base alle nuove evidenze a disposizione.</p>
</section>
</section>
<section id="la-modellizzazione-statistica-collegamento-tra-teoria-e-dati">
<h2>La Modellizzazione Statistica: Collegamento tra Teoria e Dati<a class="headerlink" href="#la-modellizzazione-statistica-collegamento-tra-teoria-e-dati" title="Permalink to this heading">#</a></h2>
<p>La modellizzazione statistica bayesiana svolge un ruolo fondamentale nel collegare la teoria psicologica con i dati empirici. Questo approccio suddivide la complessità dei fenomeni psicologici in componenti più gestibili, rendendo più agevole la comprensione e la simulazione dei comportamenti o dei processi in esame. Tuttavia, la modellizzazione richiede una sinergia tra conoscenza settoriale e competenze statistiche. Questa sinergia è cruciale per tradurre teorie e dati empirici in modelli computazionali efficaci, che possano catturare in modo accurato e significativo i fenomeni psicologici oggetto di studio.</p>
<section id="elementi-fondamentali-della-modellazione-statistica-bayesiana">
<h3>Elementi Fondamentali della Modellazione Statistica Bayesiana<a class="headerlink" href="#elementi-fondamentali-della-modellazione-statistica-bayesiana" title="Permalink to this heading">#</a></h3>
<p>I concetti chiave essenziali per comprendere la modellazione statistica bayesiana sono i seguenti:</p>
<ol class="arabic simple">
<li><p><strong>Variabili Casuali</strong>: Queste rappresentano componenti essenziali in qualsiasi modello statistico bayesiano, fungendo da rappresentazioni quantitative di elementi sconosciuti o incerti. Le variabili casuali sono utilizzate per modellare e stabilire relazioni tra diverse grandezze all’interno del modello. Ciò consente di esprimere in termini probabilistici sia le osservazioni (dati) sia i parametri del modello.</p></li>
<li><p><strong>Distribuzioni Statistiche</strong>: Nel contesto bayesiano, le distribuzioni statistiche sono fondamentali per strutturare e quantificare l’incertezza. In particolare:</p>
<ul class="simple">
<li><p><strong>Distribuzioni Priori</strong>: Queste sono centrali nella modellazione bayesiana. Le distribuzioni priori esprimono le conoscenze o le convinzioni iniziali relative alle variabili incognite prima dell’analisi dei dati. Queste distribuzioni sono poi aggiornate in base alle informazioni ottenute dai dati.</p></li>
<li><p><strong>Distribuzioni Posteriori</strong>: Sono il risultato dell’aggiornamento delle credenze priori alla luce dei dati osservati. Rappresentano la sintesi delle informazioni originali (priori) e quelle nuove (dati), offrendo una visione aggiornata delle stime del modello.</p></li>
</ul>
</li>
<li><p><strong>Aggiornamento Bayesiano</strong>: Questo processo rappresenta il cuore dell’inferenza bayesiana. Utilizzando il teorema di Bayes, le distribuzioni priori vengono aggiornate in base ai dati osservati. L’aggiornamento bayesiano consente di combinare le informazioni preesistenti con nuove evidenze, per affinare le stime dei parametri del modello e ridurre l’incertezza associata. Questo processo iterativo si adatta continuamente all’arrivo di nuove informazioni, permettendo una comprensione sempre più precisa dei fenomeni studiati.</p></li>
</ol>
<p>Questi elementi, insieme, formano il quadro metodologico della modellazione statistica bayesiana, consentendo di interpretare dati complessi e incerti in modo flessibile e informato.</p>
</section>
<section id="il-processo-di-sviluppo-di-un-modello-bayesiano">
<h3>Il Processo di Sviluppo di un Modello Bayesiano<a class="headerlink" href="#il-processo-di-sviluppo-di-un-modello-bayesiano" title="Permalink to this heading">#</a></h3>
<p>La costruzione di un modello bayesiano si articola in tre fasi principali:</p>
<ol class="arabic simple">
<li><p><strong>Progettazione del Modello</strong>: In questa fase iniziale, il modello viene progettato sulla base delle conoscenze preliminari e delle ipotesi relative al meccanismo generativo dei dati.</p></li>
<li><p><strong>Esecuzione dell’Inferenza Bayesiana</strong>: In questo stadio, si applica il teorema di Bayes per rivedere e aggiornare le credenze priori alla luce dei dati osservati, portando alla formulazione di una distribuzione a posteriori.</p></li>
<li><p><strong>Analisi e Confronto dei Modelli</strong>: In questa fase finale, si valuta l’adeguatezza del modello, esaminando la sua coerenza e affidabilità attraverso diversi criteri. Se necessario, si confronta il modello con altre alternative per determinare quale sia il più appropriato.</p></li>
</ol>
<p>Nei successivi capitoli, approfondiremo ciascuna di queste fasi, delineando il processo standard adottato nel flusso di lavoro bayesiano <span id="id8">[<a class="reference internal" href="../references/bibliography.html#id32" title="Beth Baribault and Anne GE Collins. Troubleshooting bayesian cognitive models. Psychological Methods, 2023.">BC23</a>]</span>.</p>
</section>
</section>
<section id="riesame-del-teorema-di-bayes">
<h2>Riesame del Teorema di Bayes<a class="headerlink" href="#riesame-del-teorema-di-bayes" title="Permalink to this heading">#</a></h2>
<p>Prima di discutere il flusso di lavoro bayesiano, esaminiamo nuovamente il teorema di Bayes. Il teorema di Bayes rappresenta un pilastro fondamentale della statistica bayesiana, fornendo il meccanismo attraverso il quale le credenze iniziali (priori) vengono aggiornate alla luce di nuovi dati. Per approfondire la comprensione di questo teorema, è essenziale esaminarne la formulazione sia in termini di probabilità discrete che di distribuzioni di densità di probabilità. La formula chiave del teorema di Bayes è la seguente:</p>
<div class="math notranslate nohighlight">
\[
p(\theta \mid \text{data}) = \frac{p(\text{data} \mid \theta)p(\theta)}{p(\text{data})} \propto p(\text{data} \mid \theta)p(\theta),
\]</div>
<p>dove:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\theta\)</span> rappresenta un insieme di parametri (ad esempio, coefficienti di regressione).</p></li>
<li><p><span class="math notranslate nohighlight">\(p(\theta \mid \text{data})\)</span> è la distribuzione posteriore dei parametri, ottenuta aggiornando la distribuzione a priori <span class="math notranslate nohighlight">\(p(\theta)\)</span> con i dati osservati, espressi nella funzione di verosimiglianza <span class="math notranslate nohighlight">\(p(\text{data} \mid \theta)\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(p(\text{data})\)</span>, la probabilità marginale dei dati, funge da costante di normalizzazione per assicurare che la distribuzione posteriore sommi a 1.</p></li>
</ul>
<p>Nella statistica Bayesiana, le convinzioni aggiornate (posteriori) sui parametri sono utilizzate per l’inferenza. Per esempio, la distribuzione posteriore può essere riassunta per calcolare la probabilità che un parametro rientri in un determinato intervallo.</p>
<p>La specificità dell’inferenza bayesiana è legata all’utilizzo delle distribuzioni a priori, indicate con <span class="math notranslate nohighlight">\(p(\theta)\)</span>. Di solito, i ricercatori basano queste distribuzioni a priori su risultati precedenti, meta-analisi e/o l’expertise di esperti, oppure, in modo ancora più comune, utilizzano distribuzioni non informative note come “distribuzioni di regolarizzazione”. Un aspetto cruciale, che come vedremo rappresenta un notevole vantaggio, è che l’effetto delle distribuzioni a priori è più significativo quando il campione di dati è di dimensioni ridotte. In questa situazione, l’impiego di una distribuzione a priori “di regolarizzazione” ha un impatto conservativo sull’inferenza statistica, contribuendo a mitigare le fluttuazioni dovute alla limitata dimensione del campione.</p>
<p>Passando al contesto della modellazione, consideriamo una variabile casuale <span class="math notranslate nohighlight">\( Y \)</span>, che assume un valore realizzato <span class="math notranslate nohighlight">\( y \)</span>. Ad esempio, il punteggio di uno studente in un esame di Psicometria può essere modellato come <span class="math notranslate nohighlight">\( Y \)</span>, con un insieme di possibili valori. Una volta ottenuto il voto, <span class="math notranslate nohighlight">\( Y \)</span> si realizza come <span class="math notranslate nohighlight">\( y \)</span>. Per modellare come i dati effettivi <span class="math notranslate nohighlight">\( y \)</span> sono stati ottenuti, dobbiamo specificare un modello di probabilità, noto come <em>processo generatore di dati</em> (DGP).</p>
<p>Denotiamo con <span class="math notranslate nohighlight">\( \theta \)</span> un parametro che caratterizza il modello di probabilità di interesse, che può essere uno scalare (come la media o la varianza di una distribuzione) o un vettore (ad esempio, un insieme di coefficienti di regressione). Nell’inferenza statistica, l’obiettivo è stimare questi parametri sconosciuti dai dati. La principale differenza tra l’inferenza frequentista e quella bayesiana è nella concezione di <span class="math notranslate nohighlight">\( \theta \)</span>: nella prima, <span class="math notranslate nohighlight">\( \theta \)</span> è fisso ma sconosciuto; nella seconda, è considerato una variabile casuale con una distribuzione di probabilità a priori.</p>
<p>Nell’inferenza bayesiana, calcoliamo la probabilità congiunta di parametri e dati come funzione della distribuzione condizionale dei dati dati i parametri e della distribuzione a priori dei parametri. La distribuzione posteriore di <span class="math notranslate nohighlight">\( \theta \)</span> dato <span class="math notranslate nohighlight">\( y \)</span> si ottiene moltiplicando la distribuzione dei dati <span class="math notranslate nohighlight">\( p(y \mid \theta) \)</span> per la distribuzione a priori <span class="math notranslate nohighlight">\( p(\theta) \)</span>, normalizzata per <span class="math notranslate nohighlight">\( p(y) \)</span>. In modelli complessi con molti parametri, la distribuzione posteriore è difficile da valutare, richiedendo metodi computazionali avanzati.</p>
<p>Possiamo esprimere la distribuzione congiunta dei parametri e dei dati come:</p>
<div class="math notranslate nohighlight" id="equation-eq-prob-congiunta-bayes">
<span class="eqno">(55)<a class="headerlink" href="#equation-eq-prob-congiunta-bayes" title="Permalink to this equation">#</a></span>\[ 
\begin{equation}
p(\theta, y) = p(y \mid \theta)p(\theta). 
\end{equation}
\]</div>
<p>Utilizzando il teorema di Bayes dell’eq. <a class="reference internal" href="../chapter_3/03_bayes_theorem.html#equation-eq-bayes-cont">(16)</a> otteniamo:</p>
<div class="math notranslate nohighlight" id="equation-eq-bayes-revisited">
<span class="eqno">(56)<a class="headerlink" href="#equation-eq-bayes-revisited" title="Permalink to this equation">#</a></span>\[ 
\begin{equation}
p(\theta \mid y) = \frac{p(\theta, y)}{p(y)} = \frac{p(y \mid \theta)p(\theta)}{p(y)}.
\end{equation}
\]</div>
<p>In questa formulazione, <span class="math notranslate nohighlight">\( p(y \mid \theta) \)</span> è la funzione di verosimiglianza, e l’equazione sopra rappresenta l’essenza dell’inferenza statistica bayesiana, differenziandola dalla statistica frequentista.</p>
<p>Per variabili discrete,</p>
<div class="math notranslate nohighlight">
\[ 
p(y) = \sum_\theta p(y \mid \theta)p(\theta)  
\]</div>
<p>e per variabili continue,</p>
<div class="math notranslate nohighlight">
\[ 
p(y) = \int p(y \mid \theta)p(\theta)d\theta.
\]</div>
<p>La validità dell’approccio bayesiano è sostenuta dai lavori di Cox e Savage, che hanno dimostrato che, se <span class="math notranslate nohighlight">\(p(\theta)\)</span> e <span class="math notranslate nohighlight">\(p(y \mid \theta)\)</span> rappresentano convinzioni razionali, la regola di Bayes è il metodo ottimale per aggiornarle alla luce di nuove informazioni.</p>
</section>
<section id="l-aggiornamento-bayesiano">
<h2>L’aggiornamento bayesiano<a class="headerlink" href="#l-aggiornamento-bayesiano" title="Permalink to this heading">#</a></h2>
<p>Per offrire una illustrazione quantitativa del procedimento di aggiornamento bayesiano, prendiamo in considerazione un esempio pratico: la localizzazione di un aeromobile disperso in mare. In questo capitolo, ci concentriamo esclusivamente sulla comprensione della struttura logica del problema e sulla sua formalizzazione tramite l’impiego di distribuzioni di probabilità. In questa fase, tralasciamo i dettagli implementativi e mettiamo invece l’accento sul significato delle diverse fasi e degli obiettivi del processo di aggiornamento bayesiano. Successivamente, esamineremo le metodologie per conseguire tali obiettivi, approfondendo gli aspetti di natura computazionale.</p>
<p>L’illustrazione coinvolge un ipotetico scenario in cui un aereo è disperso nell’Oceano Pacifico. Ci troviamo in un contesto in cui la latitudine è determinata, ma la longitudine rimane ignota (disponiamo unicamente dell’indicazione della direzione del viaggio, senza conoscere la distanza percorsa). L’obiettivo principale è rifinire la stima della posizione approssimata (<span class="math notranslate nohighlight">\(\theta\)</span>) dell’aereo. Per raggiungere questo fine, gli operatori di soccorso raccolgono dati dai frammenti dei detriti che sono stati individuati.</p>
<p>Iniziamo il processo con una stima iniziale, definita come <em>distribuzione a priori</em>. Questa distribuzione di probabilità rappresenta il nostro grado di conoscenza sulla posizione dell’aereo prima di ricevere ulteriori dati o informazioni.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">dnorm_trunc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ll</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ul</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">sd</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">ul</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">sd</span><span class="p">)</span> <span class="o">-</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">ll</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">sd</span><span class="p">))</span>
    <span class="n">out</span><span class="p">[(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="n">ul</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">x</span> <span class="o">&lt;</span> <span class="n">ll</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="c1"># Data points for x-axis</span>
<span class="n">x_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="c1"># Parameters for the dnorm_trunc function</span>
<span class="n">mean_val</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">sd_val</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="c1"># Calculate the y values using dnorm_trunc function</span>
<span class="n">y_vals</span> <span class="o">=</span> <span class="n">dnorm_trunc</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">mean_val</span><span class="p">,</span> <span class="n">sd_val</span><span class="p">)</span>

<span class="c1"># Plot the curve</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\theta$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;West&quot;</span><span class="p">,</span> <span class="s2">&quot;East&quot;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/7c847ae2620c36eb07e8000518ab46c04d16bb76c051cdcdf6219d4d5edef81d.png" src="../_images/7c847ae2620c36eb07e8000518ab46c04d16bb76c051cdcdf6219d4d5edef81d.png" />
</div>
</div>
<p>Nel contesto bayesiano, una distribuzione di probabilità assume il compito di rappresentare l’incertezza o le convinzioni che nutriamo riguardo ai molteplici valori possibili che un parametro può assumere. Nel caso attuale, il parametro <span class="math notranslate nohighlight">\(\theta\)</span> indica la longitudine associata alla posizione dell’aereo disperso nell’oceano Pacifico. La posizione esatta dell’aereo rimane sconosciuta. Tuttavia, formuliamo delle ipotesi iniziali sulle possibili localizzazioni. Per ogni valore possibile di <span class="math notranslate nohighlight">\(\theta\)</span>, la distribuzione di probabilità attribuisce un livello di fiducia che rispecchia quanto riteniamo probabile che quel valore specifico rappresenti il vero valore del parametro. I valori di <span class="math notranslate nohighlight">\(\theta\)</span> associati a ordinate più elevate nella funzione indicano un grado superiore di fiducia, in quanto riteniamo che tali valori siano più propensi a rappresentare il vero valore del parametro. In contrasto, i valori di <span class="math notranslate nohighlight">\(\theta\)</span> associati a ordinate più basse denotano convinzioni più deboli.</p>
<p>Sull’asse <span class="math notranslate nohighlight">\(x\)</span> del grafico sopra riportato sono indicati i valori di <span class="math notranslate nohighlight">\(\theta\)</span>, cioè i diversi possibili valori della longitudine dell’aereo. Le estremità dell’asse delle ascisse sono etichettate come “West” (Ovest) e “East” (Est), per indicare che <span class="math notranslate nohighlight">\(\theta\)</span> spazia da ovest a est. Lungo l’asse delle ordinate (<span class="math notranslate nohighlight">\(y\)</span>), sono tracciati i valori delle densità di probabilità associati a ciascun valore di <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Nel nostro caso specifico, la distribuzione a priori è centrata intorno a <span class="math notranslate nohighlight">\(\theta = 0.8\)</span>, suggerendo una previsione iniziale che l’aereo sia più verosimilmente situato ad est. Tuttavia, permangono incertezze considerevoli riguardo alla posizione esatta. Possiamo affermare, comunque, che inizialmente riteniamo che sia due volte più probabile che l’aereo si trovi ad est rispetto a ovest.</p>
<p>La scelta della distribuzione a priori rispecchia le convinzioni del ricercatore riguardo al problema in questione. Di conseguenza, diversi ricercatori potrebbero formulare diverse distribuzioni a priori per lo stesso problema, e tale diversità è accettabile, a patto che tali distribuzioni siano ragionevolmente giustificate. Nel prosieguo, scopriremo che nelle analisi Bayesiane, anche con campioni di dimensioni moderate, le varie distribuzioni a priori generano solitamente differenze trascurabili.</p>
<p>Ora, supponiamo di aver raccolto dei detriti nelle posizioni mostrate nel grafico seguente.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">qnorm_trunc</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ll</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ul</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">cdf_ll</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">ll</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sd</span><span class="p">)</span>
    <span class="n">cdf_ul</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">ul</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sd</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">cdf_ll</span> <span class="o">+</span> <span class="n">p</span> <span class="o">*</span> <span class="p">(</span><span class="n">cdf_ul</span> <span class="o">-</span> <span class="n">cdf_ll</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sd</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">rnorm_trunc</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ll</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ul</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">qnorm_trunc</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">sd</span><span class="p">,</span> <span class="n">ll</span><span class="o">=</span><span class="n">ll</span><span class="p">,</span> <span class="n">ul</span><span class="o">=</span><span class="n">ul</span><span class="p">)</span>


<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">101</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">compute_lik</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pts</span><span class="o">=</span><span class="n">grid</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">binwidth</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="n">lik_vals</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="n">pts</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sd</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span>
        <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">pts</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sd</span><span class="p">)</span> <span class="o">-</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">pts</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sd</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">lik</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">lik_vals</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">lik</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">lik</span><span class="p">)</span> <span class="o">/</span> <span class="n">binwidth</span>


<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">dat_x</span> <span class="o">=</span> <span class="n">rnorm_trunc</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">lik_x</span> <span class="o">=</span> <span class="n">compute_lik</span><span class="p">(</span><span class="n">dat_x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">lik_x</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">dat_x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">dat_x</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\theta$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Likelihood (Scaled)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;West&quot;</span><span class="p">,</span> <span class="s2">&quot;East&quot;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/2deb23f512ccfef7be2a8ab85cd94575508cfe621c7d98b66b41abb67afec577.png" src="../_images/2deb23f512ccfef7be2a8ab85cd94575508cfe621c7d98b66b41abb67afec577.png" />
</div>
</div>
<p>Dal Teorema di Bayes, è possibile derivare la distribuzione a posteriori attraverso il seguente procedimento:</p>
<div class="math notranslate nohighlight">
\[
\text{Probabilità a posteriori} \propto \text{Probabilità a priori} \times
                                       \text{Verosimiglianza}.
\]</div>
<p>In altre parole, è sufficiente moltiplicare le probabilità a priori e la verosimiglianza al fine di ottenere la probabilità posteriore per ciascuna posizione. È importante garantire che l’area sotto la curva sia normalizzata a 1. Questo processo è conosciuto come <em>aggiornamento bayesiano</em>.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">update_probs</span><span class="p">(</span><span class="n">prior_probs</span><span class="p">,</span> <span class="n">lik</span><span class="p">,</span> <span class="n">binwidth</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="n">post_probs</span> <span class="o">=</span> <span class="n">prior_probs</span> <span class="o">*</span> <span class="n">lik</span>
    <span class="k">return</span> <span class="n">post_probs</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">post_probs</span><span class="p">)</span> <span class="o">/</span> <span class="n">binwidth</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">101</span><span class="p">)</span>

<span class="c1"># Prior probabilities</span>
<span class="n">prior_probs</span> <span class="o">=</span> <span class="n">dnorm_trunc</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Likelihood values</span>
<span class="n">lik_x</span> <span class="o">=</span> <span class="n">compute_lik</span><span class="p">(</span><span class="n">dat_x</span><span class="p">)</span>

<span class="c1"># Posterior probabilities</span>
<span class="n">posterior_probs</span> <span class="o">=</span> <span class="n">update_probs</span><span class="p">(</span><span class="n">prior_probs</span><span class="p">,</span> <span class="n">lik_x</span><span class="p">)</span>

<span class="c1"># Plotting</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">dnorm_trunc</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Prior&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">lik_x</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;solid&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Likelihood&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">posterior_probs</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Posterior&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">dat_x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">dat_x</span><span class="p">)</span><span class="o">+</span><span class="mf">.1</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.15</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="o">-</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;West&quot;</span><span class="p">,</span> <span class="s2">&quot;East&quot;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/d3f10a8110b7f2c8da50ffce0104b11a4e5c38be3fe5f3e175c811ec497a17e6.png" src="../_images/d3f10a8110b7f2c8da50ffce0104b11a4e5c38be3fe5f3e175c811ec497a17e6.png" />
</div>
</div>
<p>In questa situazione, è evidente che una distribuzione a priori come quella descritta in precedenza, che definiremo “debolmente informativa”, ha un impatto trascurabile e la distribuzione a posteriori risulta quasi indistinguibile dalla verosimiglianza (che è stata normalizzata).</p>
<p>Ora, esamineremo come si comporta una distribuzione a priori maggiormente informativa.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">101</span><span class="p">)</span>

<span class="c1"># Prior probabilities</span>
<span class="n">prior_probs</span> <span class="o">=</span> <span class="n">dnorm_trunc</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Likelihood values</span>
<span class="n">lik_x</span> <span class="o">=</span> <span class="n">compute_lik</span><span class="p">(</span><span class="n">dat_x</span><span class="p">)</span>

<span class="c1"># Posterior probabilities</span>
<span class="n">posterior_probs</span> <span class="o">=</span> <span class="n">update_probs</span><span class="p">(</span><span class="n">prior_probs</span><span class="p">,</span> <span class="n">lik_x</span><span class="p">)</span>

<span class="c1"># Plotting</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">dnorm_trunc</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Prior&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">lik_x</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;solid&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Likelihood&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">posterior_probs</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Posterior&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">dat_x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">dat_x</span><span class="p">),</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.15</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;West&quot;</span><span class="p">,</span> <span class="s2">&quot;East&quot;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/f316c79753b285c7e4b8caba7343bf668fd5d96e45160db5270106b7679898dd.png" src="../_images/f316c79753b285c7e4b8caba7343bf668fd5d96e45160db5270106b7679898dd.png" />
</div>
</div>
<p>Il grafico illustra le tre distribuzioni coinvolte nell’aggiornamento bayesiano.</p>
<ol class="arabic simple">
<li><p>La curva tratteggiata rossa rappresenta la distribuzione a priori di <span class="math notranslate nohighlight">\(\theta\)</span>. Questa curva riflette le nostre credenze iniziali o le aspettative riguardo ai possibili valori di <span class="math notranslate nohighlight">\(\theta\)</span> prima di effettuare qualsiasi osservazione. Nel nostro esempio, la distribuzione a priori è una distribuzione normale con una media di 0.8 e una deviazione standard di 0.1. Questo implica che, prima di raccogliere dati, prevediamo che il valore di <span class="math notranslate nohighlight">\(\theta\)</span> sia prossimo a 0.8, con una limitata variazione intorno a questa media.</p></li>
<li><p>La curva continua blu rappresenta la verosimiglianza dei dati osservati dato un valore specifico di <span class="math notranslate nohighlight">\(\theta\)</span>. In altre parole, essa descrive quanto i dati osservati supportano ogni possibile valore di <span class="math notranslate nohighlight">\(\theta\)</span>. La forma della curva indica quale valore di <span class="math notranslate nohighlight">\(\theta\)</span> risulta più plausibile in base a ciò che è stato osservato. Maggiore è l’altezza della curva in un determinato punto, maggiore è il supporto fornito dai dati a quel valore di <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
<li><p>La curva tratteggiata nera rappresenta la distribuzione a posteriori di <span class="math notranslate nohighlight">\(\theta\)</span>. Questa curva rappresenta la nostra stima aggiornata di <span class="math notranslate nohighlight">\(\theta\)</span> dopo aver incorporato i dati osservati e le credenze iniziali attraverso il calcolo bayesiano. La distribuzione a posteriori combina la distribuzione a priori con la verosimiglianza dei dati, fornendo una stima più precisa e informativa di <span class="math notranslate nohighlight">\(\theta\)</span>. In sostanza, essa riflette la nostra comprensione aggiornata del valore di <span class="math notranslate nohighlight">\(\theta\)</span> più probabile, tenendo conto sia delle informazioni iniziali che dei dati osservati.</p>
<ul class="simple">
<li><p>Il valore di <span class="math notranslate nohighlight">\(\theta\)</span> più probabile nella distribuzione a posteriori corrisponde al punto in cui la curva raggiunge l’apice, ovvero il valore di <span class="math notranslate nohighlight">\(\theta\)</span> in cui la densità è massima. Questo punto rappresenta la stima del parametro <span class="math notranslate nohighlight">\(\theta\)</span> che appare più plausibile alla luce dei dati osservati e delle credenze iniziali.</p></li>
<li><p>Inoltre, la distribuzione a posteriori fornisce indicazioni sulla nostra incertezza riguardo al valore di <span class="math notranslate nohighlight">\(\theta\)</span>. Se la distribuzione a posteriori è concentrata attorno a un valore specifico di <span class="math notranslate nohighlight">\(\theta\)</span> e presenta un picco netto, ciò suggerisce che siamo più sicuri nella stima di <span class="math notranslate nohighlight">\(\theta\)</span> e l’incertezza è ridotta. In altre parole, i dati osservati sono informativi e hanno ridotto l’incertezza sul valore di <span class="math notranslate nohighlight">\(\theta\)</span>. Invece, se la distribuzione a posteriori è ampia e ha una forma meno definita, implica maggiore incertezza nella stima di <span class="math notranslate nohighlight">\(\theta\)</span>. Questo può accadere quando i dati osservati sono scarsi o poco informativi, oppure se la distribuzione a priori era ampia, consentendo una vasta gamma di valori di <span class="math notranslate nohighlight">\(\theta\)</span>. In sintesi, la forma della distribuzione a posteriori riflette quanto i dati raccolti ci abbiano aiutato a restringere le possibili valutazioni di <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
</ul>
</li>
</ol>
</section>
<section id="linguaggi-di-programmazione-probabilistici">
<h2>Linguaggi di programmazione probabilistici<a class="headerlink" href="#linguaggi-di-programmazione-probabilistici" title="Permalink to this heading">#</a></h2>
<p>La precedente illustrazione evidenzia il problema al quale la modellazione bayesiana mira a rispondere, nonché la soluzione proposta per affrontarlo. Finora, abbiamo delineato il concetto dell’aggiornamento bayesiano senza ancora approfondire i dettagli computazionali, i quali saranno esaminati in dettaglio nei capitoli successivi.</p>
<p>È fondamentale notare che l’attuale statistica bayesiana si avvale ampiamente di un linguaggio di programmazione probabilistico, noto come “Probabilistic Programming Language” (PPL), implementato su computer per eseguire l’aggiornamento bayesiano. Questo approccio ha rivoluzionato il modo in cui vengono condotte le analisi statistiche bayesiane, anche solo pochi decenni fa. L’adozione di tali metodi computazionali ha semplificato la formulazione di modelli statistici complessi, abbassando il livello di competenze matematiche e computazionali richieste, rendendo il processo di modellazione bayesiana più accessibile. Inoltre, questi strumenti hanno aperto nuove opportunità per affrontare problemi di analisi dei dati che, in passato, sarebbero stati notevolmente complessi da trattare.</p>
<section id="notazione">
<h3>Notazione<a class="headerlink" href="#notazione" title="Permalink to this heading">#</a></h3>
<p>Per chiarire la notazione, nel seguito utilizzeremo <span class="math notranslate nohighlight">\(y\)</span> per rappresentare i dati e <span class="math notranslate nohighlight">\(\theta\)</span> per indicare i parametri sconosciuti di un modello statistico. Entrambi, <span class="math notranslate nohighlight">\(y\)</span> e <span class="math notranslate nohighlight">\(\theta\)</span>, saranno trattati come variabili casuali. Utilizzeremo invece <span class="math notranslate nohighlight">\(x\)</span> per denotare le quantità note, come ad esempio i predittori di un modello lineare.</p>
<p>Al fine di rappresentare in modo più conciso i modelli probabilistici, adotteremo una notazione specifica. Ad esempio, anziché scrivere <span class="math notranslate nohighlight">\(p(\theta) = Beta(1, 1)\)</span>, scriveremo semplicemente <span class="math notranslate nohighlight">\(\theta \sim Beta(1, 1)\)</span>. Il simbolo “<span class="math notranslate nohighlight">\(\sim\)</span>” viene comunemente letto come “segue la distribuzione di”. Possiamo anche interpretarlo nel senso che <span class="math notranslate nohighlight">\(\theta\)</span> è un campione casuale estratto dalla distribuzione Beta(1, 1). Analogamente, la verosimiglianza di un modello binomiale sarà espressa come <span class="math notranslate nohighlight">\(y \sim \text{Bin}(n, \theta)\)</span>.</p>
</section>
</section>
<section id="la-distribuzione-a-priori">
<h2>La Distribuzione a Priori<a class="headerlink" href="#la-distribuzione-a-priori" title="Permalink to this heading">#</a></h2>
<p>Come anticipato precedentemente, l’approccio bayesiano si distingue per l’utilizzo delle distribuzioni a priori dei parametri sconosciuti oggetto dell’inferenza. In modo intuitivo, l’impiego delle distribuzioni a priori riflette il fatto che nessuno studio viene condotto in assenza totale di conoscenze acquisite da ricerche precedenti. L’analisi statistica bayesiana richiede semplicemente che tali conoscenze pregresse vengano esplicitate, per poi essere confrontate con i dati effettivamente disponibili. Le distribuzioni a priori rappresentano direttamente le nostre assunzioni riguardo ai valori plausibili dei parametri del modello.</p>
<section id="distribuzioni-a-priori-non-informative">
<h3>Distribuzioni a Priori Non Informative<a class="headerlink" href="#distribuzioni-a-priori-non-informative" title="Permalink to this heading">#</a></h3>
<p>Le distribuzioni a priori possono variare in base al grado di certezza con cui il ricercatore attribuisce credibilità a un particolare intervallo di valori dei parametri. Un caso estremo è rappresentato dalle distribuzioni a priori <em>non informative</em>, che indicano una completa mancanza di conoscenza pregressa e assegnano lo stesso grado di credibilità a tutti i valori dei parametri. Questi tipi di priori sono anche noti come priori vaghi o diffusi. Una delle distribuzioni a priori non informative più evidenti è la distribuzione di probabilità uniforme su un intervallo sensato di valori dei parametri. L’adozione della distribuzione uniforme si basa sul <em>Principio della Ragione Insufficiente</em>, formulato per la prima volta da Laplace (1774/1951), il quale sostiene che in assenza di evidenze rilevanti (pregresse), le proprie credenze dovrebbero essere distribuite in modo uniforme tra tutti gli esiti possibili.</p>
</section>
<section id="distribuzioni-a-priori-debolmente-informative">
<h3>Distribuzioni a Priori Debolmente Informative<a class="headerlink" href="#distribuzioni-a-priori-debolmente-informative" title="Permalink to this heading">#</a></h3>
<p>Le distribuzioni a priori debolmente informative sono distribuzioni di probabilità che consentono di incorporare meno informazioni di quanto sia effettivamente disponibile in una specifica situazione. Queste distribuzioni a priori contengono un intervallo di valori dei parametri che riflettono le assunzioni ragionevoli su di essi, ma tengono anche conto delle incertezze presenti in un’analisi. Questo approccio risulta particolarmente utile nel processo di stabilizzazione delle stime di un modello. L’inferenza bayesiana può richiedere notevoli risorse computazionali, soprattutto quando si trattano modelli gerarchici complessi. Pertanto, l’uso di informazioni a priori debolmente informative può contribuire a migliorare la stabilità dell’analisi senza influenzare in modo significativo le conclusioni derivate da essa.</p>
</section>
<section id="distribuzioni-a-priori-informativa">
<h3>Distribuzioni a Priori Informativa<a class="headerlink" href="#distribuzioni-a-priori-informativa" title="Permalink to this heading">#</a></h3>
<p>Ricerche precedenti, opinioni esperte, o entrambe, possono essere utilizzate in modo sistematico per affrontare un problema e possono essere incorporate nelle distribuzioni a priori. Questi tipi di priori sono noti come distribuzioni a priori informative. Queste distribuzioni riflettono informazioni concrete e rilevanti che possono avere un impatto significativo sull’analisi, fornendo una solida base di conoscenza su cui basare l’inferenza bayesiana. Le distribuzioni a priori informative possono derivare da una varietà di fonti e offrono un approccio strutturato per integrare la conoscenza pregressa nel processo di analisi statistica, migliorando la robustezza e l’accuratezza delle conclusioni derivate dai dati.</p>
</section>
</section>
<section id="la-verosimiglianza-marginale">
<h2>La Verosimiglianza Marginale<a class="headerlink" href="#la-verosimiglianza-marginale" title="Permalink to this heading">#</a></h2>
<p>La formula completa della distribuzione a posteriori può essere espressa nel seguente modo:</p>
<div class="math notranslate nohighlight" id="equation-eq-bayes-complete">
<span class="eqno">(57)<a class="headerlink" href="#equation-eq-bayes-complete" title="Permalink to this equation">#</a></span>\[
p(\theta \mid y) = \frac{p(y \mid \theta) p(\theta)}{\int_{\Theta}p(y \mid \theta) p(\theta) \,d\theta} \quad \text{per} \quad \theta \in \Theta, 
\]</div>
<p>dove <span class="math notranslate nohighlight">\(\Theta\)</span> rappresenta l’insieme dei possibili valori del parametro <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Per calcolare <span class="math notranslate nohighlight">\(p(\theta \mid y)\)</span>, è necessario dividere il prodotto tra la distribuzione a priori e la verosimiglianza per una costante di normalizzazione. Questa costante, nota come <em>verosimiglianza marginale</em>, viene introdotta per assicurare che l’area sotto la curva di <span class="math notranslate nohighlight">\(p(\theta \mid y)\)</span> sia unitaria. Tuttavia, l’integrale presente al denominatore della formula <a class="reference internal" href="#equation-eq-bayes-complete">(57)</a> spesso risulta complesso da risolvere analiticamente. Di conseguenza, l’inferenza bayesiana procede generalmente mediante metodi di approssimazione numerica.</p>
</section>
<section id="metodi-di-estimazione-della-distribuzione-a-posteriori">
<h2>Metodi di Estimazione della Distribuzione a Posteriori<a class="headerlink" href="#metodi-di-estimazione-della-distribuzione-a-posteriori" title="Permalink to this heading">#</a></h2>
<p>Esistono due principali strategie per calcolare la distribuzione a posteriori:</p>
<ol class="arabic simple">
<li><p><strong>Metodo Esatto</strong>: Questo approccio è applicabile quando la distribuzione a priori e la funzione di verosimiglianza appartengono alla stessa classe di distribuzioni, note come <em>distribuzioni a priori coniugate</em>. In questi casi, la distribuzione a posteriori può essere calcolata analiticamente, senza la necessità di approssimazioni. Questo metodo è elegante e computazionalmente efficiente, ma ha una portata limitata poiché le distribuzioni a priori coniugate esistono solo per alcune specifiche combinazioni di distribuzioni a priori e verosimiglianze.</p></li>
<li><p><strong>Metodo Approssimato</strong>: Quando il metodo esatto non può essere utilizzato, ad esempio quando le distribuzioni a priori e le verosimiglianze non sono coniugate, è possibile ricorrere al metodo approssimato. Questo implica l’uso di algoritmi computazionalmente intensivi, come le Catene di Markov Monte Carlo (MCMC), per stimare la distribuzione a posteriori. Sebbene questo approccio sia più flessibile e possa essere applicato a una vasta gamma di scenari, richiede più risorse computazionali e può essere più lento rispetto al metodo esatto.</p></li>
</ol>
</section>
<section id="il-flusso-di-lavoro-bayesiano">
<h2>Il flusso di lavoro bayesiano<a class="headerlink" href="#il-flusso-di-lavoro-bayesiano" title="Permalink to this heading">#</a></h2>
<p>Dopo aver esaminato l’esempio introduttivo presentato nelle sezioni precedenti, approfondiamo ora il concetto di “flusso di lavoro bayesiano”. Questo processo, metaforicamente descritto come “girare la manovella bayesiana”, mette in evidenza un approccio iterativo e flessibile nel contesto della ricerca scientifica. Questo procedimento, ben illustrato nella figura tratta dall’articolo di <span id="id9">Baribault and Collins [<a class="reference internal" href="../references/bibliography.html#id32" title="Beth Baribault and Anne GE Collins. Troubleshooting bayesian cognitive models. Psychological Methods, 2023.">BC23</a>]</span>, comprende diverse fasi cruciali, che vanno dalla definizione delle distribuzioni a priori fino all’inferenza basata sui dati a posteriori.</p>
<figure class="align-default" id="bayes-workflow-fig">
<a class="reference internal image-reference" href="../_images/bayesian_workflow.png"><img alt="../_images/bayesian_workflow.png" src="../_images/bayesian_workflow.png" style="height: 550px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">Una rappresentazione abbreviata del flusso di lavoro bayesiano. L’output del modello che non supera il filtro (che rappresenta i necessari controlli computazionali e di coerenza) deve essere respinto. È necessario migliorare la specifica del modello in modo che l’output possa  superare tutti i controlli. Solo allora il modello bayesiano può essere utilizzato come base per l’inferenza. (Figura tratta da <span id="id10">Baribault and Collins [<a class="reference internal" href="../references/bibliography.html#id32" title="Beth Baribault and Anne GE Collins. Troubleshooting bayesian cognitive models. Psychological Methods, 2023.">BC23</a>]</span>).</span><a class="headerlink" href="#bayes-workflow-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<section id="fasi-del-flusso-di-lavoro">
<h3>Fasi del flusso di lavoro<a class="headerlink" href="#fasi-del-flusso-di-lavoro" title="Permalink to this heading">#</a></h3>
<p>Ecco un’illustrazione delle fasi fondamentali nel flusso di lavoro bayesiano:</p>
<ol class="arabic simple">
<li><p><strong>Studio di Simulazione</strong>: In questa fase, l’obiettivo è generare dati sintetici che riproducono in modo accurato il contesto della ricerca. Ciò serve a valutare la robustezza del disegno sperimentale e a garantire che le specifiche del modello siano adeguate.</p></li>
<li><p><strong>Raccolta e Identificazione dei Dati</strong>: Questo passo coinvolge l’acquisizione e l’analisi preliminare dei dati reali, assicurandosi che siano appropriati e adeguatamente preparati per le analisi successive.</p></li>
<li><p><strong>Selezione del Modello Statistico</strong>: In questa fase, viene formulato un modello statistico che rappresenta accuratamente le teorie e le ipotesi alla base della ricerca. Questa scelta si basa su una solida comprensione del fenomeno studiato e su principi statistici ben fondati.</p></li>
<li><p><strong>Definizione delle Distribuzioni a Priori</strong>: Qui, si stabiliscono le distribuzioni a priori dei parametri del modello, basandosi su conoscenze pregresse e un ragionamento teorico robusto.</p></li>
<li><p><strong>Calcolo delle Distribuzioni a Posteriori</strong>: Utilizzando metodi analitici o, più comunemente, tecniche di campionamento come le Catene di Markov Monte Carlo (MCMC), si derivano le distribuzioni a posteriori dei parametri.</p></li>
<li><p><strong>Risoluzione dei Problemi e Diagnostica</strong>: In questa fase, vengono condotti controlli rigorosi per assicurare la convergenza del modello e la validità delle inferenze, attraverso l’uso di metriche e diagnosi specializzate.</p></li>
<li><p><strong>Controlli di Coerenza</strong>: Oltre alla diagnostica tecnica, è essenziale valutare la coerenza e la plausibilità del modello rispetto ai dati e al contesto teorico, compreso un esame predittivo a posteriori.</p></li>
<li><p><strong>Interpretazione e Comunicazione dei Risultati</strong>: Infine, si interpretano i risultati nel contesto della teoria sottostante e si comunicano in modo chiaro ed efficace, integrandoli nell’ambito più ampio della comprensione del fenomeno in studio.</p></li>
</ol>
<p>Attraverso questo processo iterativo e rigoroso, il flusso di lavoro bayesiano mira a costruire modelli statistici robusti e a ottenere inferenze valide, fornendo una base solida per la ricerca scientifica in vari campi, compresa la psicologia.</p>
</section>
</section>
<section id="commenti-e-considerazioni-finali">
<h2>Commenti e considerazioni finali<a class="headerlink" href="#commenti-e-considerazioni-finali" title="Permalink to this heading">#</a></h2>
<p>L’approccio bayesiano offre un approccio distintivo per gestire l’incertezza associata ai parametri di interesse, differenziandosi in modo significativo dalla metodologia classica. A differenza dell’idea che i parametri siano valori fissi e sconosciuti, l’approccio bayesiano li tratta come quantità probabilistiche, assegnando loro una distribuzione a priori che rappresenta le nostre credenze e intuizioni iniziali prima dell’esperimento. Attraverso l’applicazione del teorema di Bayes, queste credenze vengono raffinate e aggiornate in base ai dati raccolti, portando alla definizione della distribuzione a posteriori. Quest’ultima rappresenta una visione aggiornata dell’incertezza, incorporando sia l’evidenza empirica che le informazioni preesistenti.</p>
<p>La forza dell’approccio bayesiano risiede nella sua capacità di integrare le conoscenze pregresse con le nuove osservazioni, fornendo stime dei parametri di interesse che sono non solo più accurate ma anche più significative dal punto di vista interpretativo. Questa metodologia va oltre la mera analisi statistica, guidando il processo decisionale e consentendo di affrontare l’incertezza con una comprensione profonda, unendo conoscenze teoriche ed evidenze empiriche in un quadro coerente. In definitiva, l’approccio bayesiano non è solo un metodo statistico, ma un potente strumento decisionale che promuove l’interazione dinamica tra teoria ed esperienza.</p>
<p>Un svantaggio dell’approccio Bayesiano è che non è sempre veloce e non scala sempre bene con dataset molto grandi. Questo significa che quando si utilizzano metodi basati sulla teoria Bayesiana per l’analisi dei dati, potrebbero verificarsi problemi di efficienza computazionale quando si affrontano insiemi di dati estremamente grandi.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_4"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="introduction_part_4.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Inferenza bayesiana</p>
      </div>
    </a>
    <a class="right-next"
       href="02_subj_prop.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Pensare ad una proporzione in termini soggettivi</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#esplorazione-dei-fondamenti-dei-metodi-bayesiani-in-psicologia">Esplorazione dei Fondamenti dei Metodi Bayesiani in Psicologia</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#introduzione-ai-principi-bayesiani">Introduzione ai Principi Bayesiani</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#crescita-dei-metodi-bayesiani-nella-psicologia">Crescita dei Metodi Bayesiani nella Psicologia</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#il-ciclo-continuo-dell-apprendimento-bayesiano">Il Ciclo Continuo dell’Apprendimento Bayesiano</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dalla-generazione-dei-dati-alla-modellizzazione-bayesiana">Dalla Generazione dei Dati alla Modellizzazione Bayesiana</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#riallocazione-della-credibilita-e-aggiornamento-sequenziale">Riallocazione della Credibilità e Aggiornamento Sequenziale</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#la-modellizzazione-statistica-collegamento-tra-teoria-e-dati">La Modellizzazione Statistica: Collegamento tra Teoria e Dati</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#elementi-fondamentali-della-modellazione-statistica-bayesiana">Elementi Fondamentali della Modellazione Statistica Bayesiana</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#il-processo-di-sviluppo-di-un-modello-bayesiano">Il Processo di Sviluppo di un Modello Bayesiano</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#riesame-del-teorema-di-bayes">Riesame del Teorema di Bayes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#l-aggiornamento-bayesiano">L’aggiornamento bayesiano</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linguaggi-di-programmazione-probabilistici">Linguaggi di programmazione probabilistici</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#notazione">Notazione</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#la-distribuzione-a-priori">La Distribuzione a Priori</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distribuzioni-a-priori-non-informative">Distribuzioni a Priori Non Informative</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distribuzioni-a-priori-debolmente-informative">Distribuzioni a Priori Debolmente Informative</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distribuzioni-a-priori-informativa">Distribuzioni a Priori Informativa</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#la-verosimiglianza-marginale">La Verosimiglianza Marginale</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metodi-di-estimazione-della-distribuzione-a-posteriori">Metodi di Estimazione della Distribuzione a Posteriori</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#il-flusso-di-lavoro-bayesiano">Il flusso di lavoro bayesiano</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fasi-del-flusso-di-lavoro">Fasi del flusso di lavoro</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#commenti-e-considerazioni-finali">Commenti e considerazioni finali</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Corrado Caudek
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>