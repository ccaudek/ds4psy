

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>La verosimiglianza &#8212; ds4p</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="../_static/a11y.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/custom.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-VMXNE4BCDL"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-VMXNE4BCDL');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_3/09_likelihood';</script>
    <link rel="canonical" href="https://ccaudek.github.io/ds4psy/chapter_3/09_likelihood.html" />
    <link rel="shortcut icon" href="../_static/increasing.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Apprendimento per rinforzo" href="10_rescorla_wagner.html" />
    <link rel="prev" title="✏️ Esercizi" href="E_beta_distr.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="ds4p - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="ds4p - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Benvenuti
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_1/introduction_chapter_1.html">Python</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/01_python_1.html">Python (1)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/02_python_2.html">Python (2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_python.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/03_numpy.html">NumPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_numpy.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/04_pandas.html">Pandas (1)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/05_pandas_aggregate.html">Pandas (2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/06_pandas_functions.html">Pandas (3)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_pandas.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/07_matplotlib.html">Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/08_seaborn.html">Seaborn</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_matplotlib.html">✏️ Esercizi</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_2/introduction_chapter_2.html">Statistica descrittiva</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/01_key_notions.html">Concetti chiave</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_key_notions.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/02_measurement.html">La misurazione in psicologia</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_scales.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/03_freq_distr.html">Dati e frequenze</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_sums.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/04_loc_scale.html">Indici di posizione e di scala</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/05_correlation.html">Le relazioni tra variabili</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/06_causality.html">Correlazione e causazione</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/07_crisis.html">La crisi della generalizzabilità</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_eda.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_mehr_song_spelke.html">✏️ Esercizi</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="introduction_chapter_3.html">Probabilità</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01_intro_prob.html">Introduzione al calcolo delle probabilità</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_prob.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="02_conditional_prob.html">Probabilità condizionata</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_cond_prob.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_bayes_theorem.html">Il teorema di Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_bayes_theorem.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="04_expval_var.html">Variabili casuali</a></li>
<li class="toctree-l2"><a class="reference internal" href="04a_sampling_distr.html">Stime, stimatori e parametri</a></li>
<li class="toctree-l2"><a class="reference internal" href="04b_illusion.html">Incertezza inferenziale e variabilità dei risultati</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_rv_discrete.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_joint_prob.html">Probabilità congiunta</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_joint_prob.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="06_density_func.html">La funzione di densità di probabilità</a></li>
<li class="toctree-l2"><a class="reference internal" href="07_discr_rv_distr.html">Distribuzioni di v.c. discrete</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_binomial.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="08_cont_rv_distr.html">Distribuzioni di v.c. continue</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_gaussian.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_beta_distr.html">✏️ Esercizi</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">La verosimiglianza</a></li>
<li class="toctree-l2"><a class="reference internal" href="10_rescorla_wagner.html">Apprendimento per rinforzo</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_likelihood.html">✏️ Esercizi</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_4/introduction_part_4.html">Inferenza bayesiana</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/01_intro_bayes.html">Modellazione bayesiana</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/02_subj_prop.html">Pensare ad una proporzione in termini soggettivi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/03_conjugate_families_1.html">Distribuzioni coniugate (1)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/04_conjugate_families_2.html">Distribuzioni coniugate (2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/05_summary_posterior.html">Sintesi a posteriori</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/E_conjugate.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/06_balance-prior-post.html">L’influenza della distribuzione a priori</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/10_metropolis.html">Monte Carlo a Catena di Markov</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/11_beta_binomial_pymc.html">Inferenza bayesiana con PyMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/12_jax.html">Usare JAX per un campionamento più veloce</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/13_preliz.html">Scegliere le distribuzioni a priori</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/16_summary_posterior_pymc.html">Metodi di sintesi della distribuzione a posteriori</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/17_prediction.html">La predizione bayesiana</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/18_mcmc_diagnostics.html">Diagnostica delle catene markoviane</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/19_odds_ratio.html">Analisi bayesiana dell’odds-ratio</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/20_poisson_model.html">Modello di Poisson</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/21_poisson_sim.html">Modello di Poisson: derivazione analitica e MCMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/E_freq.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/22_normal_normal_model.html">Inferenza bayesiana su una media</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/E_one_mean.html">✏️ Esercizio</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/E_one_mean_2.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/23_two_groups.html">Confronto tra due gruppi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/24_multiple_groups.html">Gruppi multipli</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/30_entropy.html">Entropia</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/31_kl.html">La divergenza di Kullback-Leibler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/40_hier_beta_binom.html">Modello gerarchico beta-binomiale</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/41_hier_poisson.html">Modello gerarchico di Poisson</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/42_hier_gaussian.html">Modello gerarchico gaussiano</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/hssm.html">Drift Diffusion Model</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_5/introduction_part_5.html">Analisi della regressione</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_1.html">Il modello di regressione lineare</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_2.html">Analisi bayesiana del modello di regressione lineare</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_params_recovery.html">Analisi di simulazione per la stima dei parametri nel modello di regressione</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_3.html">Zucchero sintattico</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_4.html">Confronto tra le medie di due gruppi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_5.html">Il modello lineare gerarchico</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_multilevel_modeling.html">A Primer on Bayesian Methods for Multilevel Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_7.html">Regressione robusta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_simpson.html">Paradosso di Simpson</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_reglin_1.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_reglin_2.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_reglin_3.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_reglin_4.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_logistic_reg.html">Modello di regressione logistica</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_binomial_reg.html">Regressione binomiale</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_covid.html">Inferenza controfattuale: calcolo delle morti in eccesso dovute al COVID-19</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_counterfactual.html">Analisi causale con PyMC</a></li>

<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_stab.html">✏️ Esercizi</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_6/introduction_part_6.html">Inferenza frequentista</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_estimation.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/02_conf_interv.html">Intervallo di confidenza</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/03_test_ipotesi.html">Significatività statistica</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_interpretation_test.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_significato_test.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/04_two_ind_samples.html">Test t di Student per campioni indipendenti</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_test_media_pop.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_medie_pop_ampie.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_medie_pop_piccoli.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_campioni_appaiati.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_confronto_proporzioni.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/05_limiti_stat_frequentista.html">Limiti dell’inferenza frequentista</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/06_s_m_errors.html">Crisi della replicabilità</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../references/bibliography.html">Bibliografia</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_7/introduction_appendix.html">Appendici</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/00_installation.html">Ambiente di lavoro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a01_math_symbols.html">Simbologia di base</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a02_numbers.html">Numeri binari, interi, razionali, irrazionali e reali</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a04_summation_notation.html">Simbolo di somma (sommatorie)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a05_sets.html">Insiemi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a06_combinatorics.html">Calcolo combinatorio</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a07_calculus.html">Per liberarvi dai terrori preliminari</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a08_kde_plot.html">Kernel Density Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a09_prob_tutorial.html">Esercizi di probabilità discreta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a12_rng.html">Generazione di numeri casuali</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a14_predict_counts.html">La predizione delle frequenze</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a15_lin_fun.html">La funzione lineare</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a20_reglin_1.html">Regressione lineare bivariata</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a21_reglin_2.html">Regressione lineare con Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a23_reglin_4.html">Posterior Predictive Checks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a30_ttest_exercises.html">Esercizi sull’inferenza frequentista</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/ccaudek/ds4psy/blob/main/docs/chapter_3/09_likelihood.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapter_3/09_likelihood.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>La verosimiglianza</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#concetto-di-verosimiglianza-e-sua-formalizzazione-matematica">Concetto di Verosimiglianza e Sua Formalizzazione Matematica</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#verosimiglianza-nel-contesto-del-modello-binomiale-un-esempio-approfondito">Verosimiglianza nel Contesto del Modello Binomiale: Un Esempio Approfondito</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretazione-della-funzione-di-verosimiglianza">Interpretazione della Funzione di Verosimiglianza</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#la-funzione-di-log-verosimiglianza">La Funzione di Log-Verosimiglianza</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#verosimiglianza-congiunta-di-n-campioni-iid-da-una-distribuzione-binomiale">Verosimiglianza Congiunta di <span class="math notranslate nohighlight">\( n \)</span> Campioni IID da una Distribuzione Binomiale</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modello-gaussiano-e-verosimiglianza">Modello Gaussiano e Verosimiglianza</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#caso-di-una-singola-osservazione">Caso di una Singola Osservazione</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#campione-indipendente-di-osservazioni-da-una-distribuzione-gaussiana">Campione indipendente di osservazioni da una distribuzione gaussiana</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derivazione-formale-della-stima-di-massima-verosimiglianza-per-una-distribuzione-gaussiana">Derivazione Formale della Stima di Massima Verosimiglianza per una Distribuzione Gaussiana</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stima-di-massima-verosimiglianza-per-mu">Stima di Massima Verosimiglianza per <span class="math notranslate nohighlight">\( \mu \)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stima-di-massima-verosimiglianza-per-sigma-2">Stima di Massima Verosimiglianza per <span class="math notranslate nohighlight">\( \sigma^2 \)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusione-e-riflessioni-finali">Conclusione e Riflessioni Finali</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#watermark">Watermark</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <p><a target="_blank" rel="noopener noreferrer" href="https://colab.research.google.com/github/ccaudek/ds4psy_2023/blob/main/225_likelihood.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<section class="tex2jax_ignore mathjax_ignore" id="la-verosimiglianza">
<span id="notebook-likelihood"></span><h1>La verosimiglianza<a class="headerlink" href="#la-verosimiglianza" title="Permalink to this heading">#</a></h1>
<div class="admonition-obiettivi-di-apprendimento admonition">
<p class="admonition-title">Obiettivi di apprendimento</p>
<p>Dopo aver completato questo capitolo, sarai in grado di:</p>
<ul class="simple">
<li><p>Comprendere il concetto di verosimiglianza e il suo ruolo nella dei parametri.</p></li>
<li><p>Generare grafici della funzione di verosimiglianza binomiale.</p></li>
<li><p>Generare grafici della funzione di verosimiglianza del modello gaussiano.</p></li>
<li><p>Interpretare i grafici della funzione di verosimiglianza.</p></li>
<li><p>Comprendere il concetto di stima di massima verosimiglianza.</p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">stats</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;retina&#39;
<span class="n">RANDOM_SEED</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">RANDOM_SEED</span><span class="p">)</span>
<span class="n">az</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;arviz-darkgrid&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="concetto-di-verosimiglianza-e-sua-formalizzazione-matematica">
<h2>Concetto di Verosimiglianza e Sua Formalizzazione Matematica<a class="headerlink" href="#concetto-di-verosimiglianza-e-sua-formalizzazione-matematica" title="Permalink to this heading">#</a></h2>
<p>La funzione di verosimiglianza è una struttura matematica utilizzata per quantificare la congruenza tra un insieme di dati osservati <span class="math notranslate nohighlight">\( y \)</span> e un modello statistico parametrizzato da <span class="math notranslate nohighlight">\( \theta \)</span>. Essa è formalmente definita come proporzionale alla probabilità condizionale dei dati <span class="math notranslate nohighlight">\( y \)</span> dato il vettore dei parametri <span class="math notranslate nohighlight">\( \theta \)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-likelihood-def">
<span class="eqno">(52)<a class="headerlink" href="#equation-eq-likelihood-def" title="Permalink to this equation">#</a></span>\[
\begin{equation}
L(\theta | y) \propto p(y | \theta).
\end{equation}
\]</div>
<p>In questa espressione, <span class="math notranslate nohighlight">\( L(\theta | y) \)</span> rappresenta la funzione di verosimiglianza, mentre <span class="math notranslate nohighlight">\( p(y | \theta) \)</span> è la densità (o funzione di massa nel caso discreto) di probabilità dei dati <span class="math notranslate nohighlight">\( y \)</span> condizionata dai parametri <span class="math notranslate nohighlight">\( \theta \)</span>.</p>
<p>La costante di proporzionalità, spesso omessa, è irrilevante nel contesto dell’ottimizzazione della funzione di verosimiglianza. Ciò è dovuto al fatto che tale costante non dipende dai parametri <span class="math notranslate nohighlight">\( \theta \)</span> e, quindi, non influisce sull’identificazione del valore di <span class="math notranslate nohighlight">\( \theta \)</span> che massimizza la funzione.</p>
<p>Immaginiamo di avere una moneta e di voler valutare se è equilibrata. Dopo 10 lanci, osserviamo 7 facce. Utilizziamo la distribuzione binomiale per modellare questa situazione. La funzione di massa di probabilità (FMP) della distribuzione binomiale è data da:</p>
<div class="math notranslate nohighlight">
\[
p(y | n, p) = \binom{n}{y} \cdot p^y \cdot (1 - p)^{(n - y)},
\]</div>
<p>dove <span class="math notranslate nohighlight">\( n \)</span> è il numero totale di tentativi, <span class="math notranslate nohighlight">\( y \)</span> è il numero di successi, e <span class="math notranslate nohighlight">\( p \)</span> è la probabilità di successo.</p>
<p>Se vogliamo calcolare la verosimiglianza di osservare 7 facce in 10 lanci con una moneta equilibrata (<span class="math notranslate nohighlight">\( p = 0.5 \)</span>), la formula diventa:</p>
<div class="math notranslate nohighlight">
\[
L(p = 0.5 | y = 7, n = 10) \propto \binom{10}{7} \cdot 0.5^7 \cdot (1 - 0.5)^{3}.
\]</div>
<p>Questo calcolo ci permette di quantificare quanto sia verosimile osservare 7 facce in 10 lanci, supponendo che la moneta sia equilibrata. Calcolando la verosimiglianza per diversi valori di <span class="math notranslate nohighlight">\( p \)</span>, possiamo individuare il valore che massimizza questa funzione e, quindi, ottenere una stima più precisa della “vera” probabilità di ottenere una faccia con questa moneta.</p>
<p>È importante notare che la funzione di verosimiglianza e la funzione di probabilità o densità possono essere rappresentate dalla stessa formula matematica, ma la loro interpretazione e applicazione sono differenti. Mentre la funzione di probabilità si concentra sul calcolo della probabilità dei dati <span class="math notranslate nohighlight">\( y \)</span> dati i parametri <span class="math notranslate nohighlight">\( \theta \)</span>, la funzione di verosimiglianza è utilizzata per determinare la plausibilità di differenti valori del parametro <span class="math notranslate nohighlight">\( \theta \)</span> alla luce dei dati osservati <span class="math notranslate nohighlight">\( y \)</span>.</p>
<p>In sintesi, la funzione di verosimiglianza è uno strumento fondamentale nell’inferenza statistica. Essa fornisce un criterio per valutare e confrontare la plausibilità di diversi valori del parametro <span class="math notranslate nohighlight">\( \theta \)</span>, e serve come base per metodi di stima puntuali o intervallari dei parametri del modello.</p>
</section>
<section id="verosimiglianza-nel-contesto-del-modello-binomiale-un-esempio-approfondito">
<h2>Verosimiglianza nel Contesto del Modello Binomiale: Un Esempio Approfondito<a class="headerlink" href="#verosimiglianza-nel-contesto-del-modello-binomiale-un-esempio-approfondito" title="Permalink to this heading">#</a></h2>
<p>Esploriamo ulteriormente l’importanza della funzione di verosimiglianza mediante un esempio concreto basato sulla distribuzione binomiale. Immaginiamo di effettuare un esperimento composto da <span class="math notranslate nohighlight">\( n \)</span> prove indipendenti, ciascuna con due possibili esiti: successo o fallimento (ad esempio, lanci di una moneta). Supponiamo di aver registrato <span class="math notranslate nohighlight">\( y \)</span> successi e <span class="math notranslate nohighlight">\( n - y \)</span> fallimenti. In questo contesto, la funzione di massa di probabilità (FMP) binomiale che governa la probabilità di osservare esattamente <span class="math notranslate nohighlight">\( y \)</span> successi è:</p>
<div class="math notranslate nohighlight">
\[
P(Y = y) = \binom{n}{y} \theta^y (1 - \theta)^{n - y},
\]</div>
<p>dove <span class="math notranslate nohighlight">\( \theta \)</span> rappresenta la probabilità intrinseca di successo in ogni singola prova di Bernoulli.</p>
<p>La funzione di verosimiglianza, invece, è utilizzata per esprimere la plausibilità relativa di osservare i dati <span class="math notranslate nohighlight">\( y \)</span> al variare del parametro <span class="math notranslate nohighlight">\( \theta \)</span>. Matematicamente, questa è formulata come:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\theta \mid y) = \binom{n}{y} \theta^y (1 - \theta)^{n - y}.
\]</div>
<p>Poiché il coefficiente binomiale <span class="math notranslate nohighlight">\(\binom{n}{y}\)</span> è indipendente da <span class="math notranslate nohighlight">\( \theta \)</span>, possiamo semplificare la funzione di verosimiglianza a:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\theta \mid y) = \theta^y (1 - \theta)^{n - y}.
\]</div>
<p>Per illustrare con un esempio applicativo, facciamo riferimento allo studio di <span id="id1">Zetsche <em>et al.</em> [<a class="reference internal" href="../references/bibliography.html#id58" title="Ulrike Zetsche, Paul-Christian Buerkner, and Babette Renneberg. Future expectations in clinical depression: biased or realistic? Journal of Abnormal Psychology, 128(7):678, 2019.">ZBR19</a>]</span>. In questo studio, su un campione di 30 pazienti affetti da depressione clinica, 23 mostravano aspettative negativamente distorte riguardo al loro futuro stato d’animo. Pertanto, nel contesto di questi dati, la funzione di verosimiglianza è definita dalla distribuzione binomiale con un parametro <span class="math notranslate nohighlight">\( \theta \)</span> ignoto nell’intervallo [0, 1]. Avendo osservato <span class="math notranslate nohighlight">\( y = 23 \)</span> successi in <span class="math notranslate nohighlight">\( n = 30 \)</span> prove, la funzione di verosimiglianza assume la forma:</p>
<div class="math notranslate nohighlight" id="equation-eq-likebino23">
<span class="eqno">(53)<a class="headerlink" href="#equation-eq-likebino23" title="Permalink to this equation">#</a></span>\[
\begin{equation}
\mathcal{L}(\theta \mid y) = \frac{(23 + 7)!}{23!7!} \theta^{23} (1 - \theta)^7.
\end{equation}
\]</div>
<p>Per costruire il profilo di verosimiglianza, è necessario valutare l’Equazione <span class="math notranslate nohighlight">\((\text{eq-likebino23})\)</span> per un insieme di possibili valori di <span class="math notranslate nohighlight">\( \theta \)</span> nell’intervallo [0, 1]. Per esempio, potremmo considerare 100 valori di <span class="math notranslate nohighlight">\( \theta \)</span> equidistanti in questo intervallo. I dati <span class="math notranslate nohighlight">\( y \)</span> e <span class="math notranslate nohighlight">\( n \)</span> rimangono fissi durante questo processo. In questo modo, otteniamo una panoramica della plausibilità relativa dei vari valori di <span class="math notranslate nohighlight">\( \theta \)</span>, permettendoci di identificare il valore che massimizza la funzione di verosimiglianza e, quindi, di effettuare una stima puntuale del parametro <span class="math notranslate nohighlight">\( \theta \)</span> più congruente con i dati osservati.</p>
<p>Nella simulazione seguente, considereremo 100 valori possibili per <span class="math notranslate nohighlight">\(\theta\)</span> nell’intervallo [0, 1]. Iniziamo a definire i dati.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">23</span>
</pre></div>
</div>
</div>
</div>
<p>Creiamo ora i possibili valori del parametro <span class="math notranslate nohighlight">\(\theta\)</span> per i quali calcoleremo la verosimiglianza.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.         0.01010101 0.02020202 0.03030303 0.04040404 0.05050505
 0.06060606 0.07070707 0.08080808 0.09090909 0.1010101  0.11111111
 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616 0.17171717
 0.18181818 0.19191919 0.2020202  0.21212121 0.22222222 0.23232323
 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828 0.29292929
 0.3030303  0.31313131 0.32323232 0.33333333 0.34343434 0.35353535
 0.36363636 0.37373737 0.38383838 0.39393939 0.4040404  0.41414141
 0.42424242 0.43434343 0.44444444 0.45454545 0.46464646 0.47474747
 0.48484848 0.49494949 0.50505051 0.51515152 0.52525253 0.53535354
 0.54545455 0.55555556 0.56565657 0.57575758 0.58585859 0.5959596
 0.60606061 0.61616162 0.62626263 0.63636364 0.64646465 0.65656566
 0.66666667 0.67676768 0.68686869 0.6969697  0.70707071 0.71717172
 0.72727273 0.73737374 0.74747475 0.75757576 0.76767677 0.77777778
 0.78787879 0.7979798  0.80808081 0.81818182 0.82828283 0.83838384
 0.84848485 0.85858586 0.86868687 0.87878788 0.88888889 0.8989899
 0.90909091 0.91919192 0.92929293 0.93939394 0.94949495 0.95959596
 0.96969697 0.97979798 0.98989899 1.        ]
</pre></div>
</div>
</div>
</div>
<p>Per esempio, ponendo <span class="math notranslate nohighlight">\(\theta = 0.1\)</span> otteniamo il seguente valore dell’ordinata della funzione di verosimiglianza:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\theta \mid y) = \frac{(23 + 7)!}{23!7!} 0.1^{23} + (1-0.1)^7.
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>9.7371682902e-18
</pre></div>
</div>
</div>
</div>
<p>Ponendo <span class="math notranslate nohighlight">\(\theta = 0.2\)</span> otteniamo il seguente valore dell’ordinata della funzione di verosimiglianza:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\theta \mid y) = \frac{(23 + 7)!}{23!7!} 0.2^{23} + (1-0.2)^7.
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3.58141723492221e-11
</pre></div>
</div>
</div>
</div>
<p>Se ripetiamo questo processo 100 volte, una volta per ciascuno dei valori <span class="math notranslate nohighlight">\(\theta\)</span> che abbiamo elencato sopra, otteniamo 100 coppie di punti <span class="math notranslate nohighlight">\(\theta\)</span> e <span class="math notranslate nohighlight">\(f(\theta)\)</span>. A tale fine, definiamo la seguente funzione.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">like</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">comb</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span> <span class="o">*</span> <span class="n">theta</span><span class="o">**</span><span class="n">r</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">r</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>La curva che interpola i punti ottenuti è la funzione di verosimiglianza, come indicato dalla figura seguente.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">like</span><span class="p">(</span><span class="n">r</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="n">theta</span><span class="p">),</span> <span class="s2">&quot;-&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Funzione di verosimiglianza&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Valore della variabile casuale theta [0, 1]&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Verosimiglianza&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1d8e3f590917e8061fc2327511e98c167394a38ba1f847477aa8961fbbf6e6c3.png" src="../_images/1d8e3f590917e8061fc2327511e98c167394a38ba1f847477aa8961fbbf6e6c3.png" />
</div>
</div>
<section id="interpretazione-della-funzione-di-verosimiglianza">
<h3>Interpretazione della Funzione di Verosimiglianza<a class="headerlink" href="#interpretazione-della-funzione-di-verosimiglianza" title="Permalink to this heading">#</a></h3>
<p>La funzione di verosimiglianza, denotata come <span class="math notranslate nohighlight">\( L(\theta) \)</span> o alternativamente <span class="math notranslate nohighlight">\( p(Y \mid \theta) \)</span>, è una funzione che varia in base al parametro <span class="math notranslate nohighlight">\( \theta \)</span>. Ogni valore specifico di <span class="math notranslate nohighlight">\( \theta \)</span> determina un valore corrispondente della funzione di verosimiglianza, permettendoci di valutare quanto quel particolare <span class="math notranslate nohighlight">\( \theta \)</span> sia coerente o plausibile alla luce dei dati osservati. In altre parole, la funzione di verosimiglianza agisce come un termometro che misura l’adeguatezza dei diversi valori del parametro <span class="math notranslate nohighlight">\( \theta \)</span> rispetto al campione di dati disponibile.</p>
<p>Il valore di <span class="math notranslate nohighlight">\( \theta \)</span> che massimizza questa funzione è particolarmente significativo, poiché rappresenta la stima più plausibile del parametro dato il set di dati osservato. Ad esempio, nel nostro caso pratico, il valore di <span class="math notranslate nohighlight">\( \theta \)</span> che emerge come più plausibile è <span class="math notranslate nohighlight">\( \frac{23}{30} = 0.767 \)</span>, il quale corrisponde al massimo (o modalità) della funzione di verosimiglianza.</p>
<p>Per identificare numericamente questo valore ottimale di <span class="math notranslate nohighlight">\( \theta \)</span>, si può localizzare l’indice nel vettore dei valori di verosimiglianza dove questa raggiunge il suo picco. Metodi computazionali, come l’uso della funzione <code class="docutils literal notranslate"><span class="pre">argmax</span></code> in NumPy, possono automatizzare questo processo. Una volta individuato l’indice che massimizza la verosimiglianza, si può risalire al valore corrispondente di <span class="math notranslate nohighlight">\( \theta \)</span> nel vettore dei parametri, ottenendo così la stima di <span class="math notranslate nohighlight">\( \theta \)</span> che rende i dati osservati più probabili.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">l</span> <span class="o">=</span> <span class="n">like</span><span class="p">(</span><span class="n">r</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="n">theta</span><span class="p">)</span>
<span class="n">l</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>76
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">theta</span><span class="p">[</span><span class="mi">76</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.7676767676767677
</pre></div>
</div>
</div>
</div>
<p>È importante notare che, invece di utilizzare la funzione <code class="docutils literal notranslate"><span class="pre">like()</span></code> che abbiamo definito precedentemente per motivi didattici, è possibile ottenere lo stesso risultato utilizzando in modo equivalente la funzione <code class="docutils literal notranslate"><span class="pre">binom.pmf()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">theta</span><span class="p">),</span> <span class="s2">&quot;-&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Funzione di verosimiglianza&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Valore della variabile casuale theta [0, 1]&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Verosimiglianza&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1d8e3f590917e8061fc2327511e98c167394a38ba1f847477aa8961fbbf6e6c3.png" src="../_images/1d8e3f590917e8061fc2327511e98c167394a38ba1f847477aa8961fbbf6e6c3.png" />
</div>
</div>
</section>
<section id="la-funzione-di-log-verosimiglianza">
<h3>La Funzione di Log-Verosimiglianza<a class="headerlink" href="#la-funzione-di-log-verosimiglianza" title="Permalink to this heading">#</a></h3>
<p>Nelle applicazioni pratiche, si preferisce frequentemente lavorare con il logaritmo naturale della funzione di verosimiglianza, noto come funzione di log-verosimiglianza. Questa è definita come:</p>
<div class="math notranslate nohighlight" id="equation-eq-loglike-definition">
<span class="eqno">(54)<a class="headerlink" href="#equation-eq-loglike-definition" title="Permalink to this equation">#</a></span>\[
\begin{equation}
\ell(\theta) = \log \mathcal{L}(\theta).
\end{equation}
\]</div>
<p>Grazie alla monotonicità del logaritmo (specificamente, essendo una funzione strettamente crescente), i punti di massimo per <span class="math notranslate nohighlight">\( \mathcal{L}(\theta) \)</span> e <span class="math notranslate nohighlight">\( \ell(\theta) \)</span> coincidono, ovvero:</p>
<div class="math notranslate nohighlight">
\[
\hat{\theta} = \arg \max_{\theta \in \Theta} \ell(\theta) = \arg \max_{\theta \in \Theta} \mathcal{L}(\theta).
\]</div>
<p>La funzione di log-verosimiglianza per una distribuzione binomiale si articola come segue:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\ell(\theta \mid y) &amp;= \log \mathcal{L}(\theta \mid y) \\
&amp;= \log \left( \theta^y (1-\theta)^{n-y} \right) \\
&amp;= y \log \theta + (n-y) \log (1-\theta).
\end{aligned}
\end{split}\]</div>
<p>Utilizzare la log-verosimiglianza piuttosto che la verosimiglianza diretta è consigliato per diverse ragioni pratiche. Una delle principali è legata alla stabilità numerica: i valori della funzione di verosimiglianza possono diventare molto piccoli—al punto da raggiungere l’ordine di <span class="math notranslate nohighlight">\( 10^{-34} \)</span> o inferiori—portando a potenziali problemi di arrotondamento e precisione numerica durante i calcoli. La trasformazione logaritmica mitiga questo problema trasformando i prodotti in somme e rendendo i valori più maneggevoli dal punto di vista numerico.</p>
<p>Per illustrare questo concetto, possiamo riprendere l’esempio precedente e applicare la funzione di log-verosimiglianza per identificare il valore di <span class="math notranslate nohighlight">\( \theta \)</span> che massimizza questa funzione. In pratica, potremmo utilizzare metodi computazionali come la funzione <code class="docutils literal notranslate"><span class="pre">binom.logpmf()</span></code> per eseguire questa operazione.</p>
<p>La rappresentazione grafica della funzione di log-verosimiglianza fornisce ulteriori intuizioni sul comportamento di questa funzione.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">r</span> <span class="o">=</span> <span class="mi">23</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">logpmf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">theta</span><span class="p">),</span> <span class="s2">&quot;-&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Funzione di log-verosimiglianza&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Valore della variabile casuale theta [0, 1]&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Log-verosimiglianza&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ab403c74a5dfa647f1ed0a3780157406cf02cf9273c9bd3bd709f1c1df3a4216.png" src="../_images/ab403c74a5dfa647f1ed0a3780157406cf02cf9273c9bd3bd709f1c1df3a4216.png" />
</div>
</div>
<p>Il risultato replica quello trovato in precedenza con la funzione di verosimiglianza.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ll</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">logpmf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
<span class="n">ll</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>76
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">theta</span><span class="p">[</span><span class="mi">76</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.7676767676767677
</pre></div>
</div>
</div>
</div>
</section>
<section id="verosimiglianza-congiunta-di-n-campioni-iid-da-una-distribuzione-binomiale">
<h3>Verosimiglianza Congiunta di <span class="math notranslate nohighlight">\( n \)</span> Campioni IID da una Distribuzione Binomiale<a class="headerlink" href="#verosimiglianza-congiunta-di-n-campioni-iid-da-una-distribuzione-binomiale" title="Permalink to this heading">#</a></h3>
<p>Nella discussione precedente, abbiamo esaminato la verosimiglianza associata a una singola osservazione proveniente da una distribuzione binomiale. Ora estendiamo questo concetto a un contesto più generale, considerando <span class="math notranslate nohighlight">\( n \)</span> osservazioni indipendenti e identicamente distribuite (IID) da una distribuzione binomiale.</p>
<p>Denotiamo con <span class="math notranslate nohighlight">\( Y = [y_1, y_2, \ldots, y_n] \)</span> il vettore che contiene queste <span class="math notranslate nohighlight">\( n \)</span> osservazioni. In questo caso, la probabilità congiunta delle osservazioni <span class="math notranslate nohighlight">\( y_1, y_2, \ldots, y_n \)</span> dato il parametro <span class="math notranslate nohighlight">\( \theta \)</span> si scrive come <span class="math notranslate nohighlight">\( p(y_1, y_2, \ldots, y_n \mid \theta) \)</span>.</p>
<p>Grazie all’indipendenza delle osservazioni, la funzione di probabilità congiunta può essere fattorizzata nel prodotto delle funzioni di probabilità marginali, come segue:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
p(y_1, y_2, \ldots, y_n \mid \theta) &amp;= p(y_1 \mid \theta) \times p(y_2 \mid \theta) \times \cdots \times p(y_n \mid \theta) \\
&amp;= \text{Binomiale}(y_1 \mid \theta) \times \text{Binomiale}(y_2 \mid \theta) \times \cdots \times \text{Binomiale}(y_n \mid \theta).
\end{aligned}
\end{split}\]</div>
<p>In termini di verosimiglianza, quando inseriamo i valori osservati <span class="math notranslate nohighlight">\( Y[1], Y[2], \ldots, Y[n] \)</span> nella funzione di probabilità congiunta, otteniamo la funzione di verosimiglianza congiunta per le <span class="math notranslate nohighlight">\( n \)</span> osservazioni IID:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\theta \mid Y) = p(Y[1], Y[2], \ldots, Y[n] \mid \theta) = \prod_{i=1}^{n} p(Y[i] \mid \theta).
\]</div>
<p>Questa funzione di verosimiglianza congiunta fornisce una misura complessiva di quanto bene il modello binomiale con un dato <span class="math notranslate nohighlight">\( \theta \)</span> spiega l’insieme completo delle <span class="math notranslate nohighlight">\( n \)</span> osservazioni. Essa gioca un ruolo cruciale nell’inferenza statistica, specialmente quando si tratta di stimare il parametro <span class="math notranslate nohighlight">\( \theta \)</span> che massimizza la verosimiglianza congiunta a partire dai dati osservati.</p>
</section>
<section id="modello-gaussiano-e-verosimiglianza">
<h3>Modello Gaussiano e Verosimiglianza<a class="headerlink" href="#modello-gaussiano-e-verosimiglianza" title="Permalink to this heading">#</a></h3>
<p>Ampliamo ora la nostra analisi al caso della distribuzione gaussiana. Inizieremo con la verosimiglianza associata a una singola osservazione <span class="math notranslate nohighlight">\( Y \)</span>, per poi estendere la discussione a un insieme di osservazioni gaussiane indipendenti e identicamente distribuite (IID).</p>
<p>Per una distribuzione gaussiana con media <span class="math notranslate nohighlight">\( \mu \)</span> e varianza <span class="math notranslate nohighlight">\( \sigma^2 \)</span>, la funzione di verosimiglianza <span class="math notranslate nohighlight">\( L \)</span> si può esprimere come:</p>
<div class="math notranslate nohighlight" id="equation-eq-gaussian-likelihood">
<span class="eqno">(55)<a class="headerlink" href="#equation-eq-gaussian-likelihood" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
L(\theta \mid Y) &amp;= \prod_{i=1}^{n} f(y_i \mid \mu, \sigma^2) \\
&amp;= (2\pi\sigma^2)^{-\frac{n}{2}} \exp \left( -\frac{1}{2\sigma^2} \sum_{i=1}^{n} (y_i - \mu)^2 \right).
\end{aligned}
\end{split}\]</div>
</section>
<section id="caso-di-una-singola-osservazione">
<h3>Caso di una Singola Osservazione<a class="headerlink" href="#caso-di-una-singola-osservazione" title="Permalink to this heading">#</a></h3>
<p>Per una migliore comprensione dell’equazione di verosimiglianza (eq-gaussian-likelihood), consideriamo prima il caso in cui abbiamo una singola osservazione <span class="math notranslate nohighlight">\( y \)</span>.</p>
<p>Immaginiamo che la variabile casuale <span class="math notranslate nohighlight">\( y \)</span> rappresenti il Quoziente d’Intelligenza (QI) di un individuo. In questa circostanza, la funzione di verosimiglianza per una singola osservazione dalla distribuzione gaussiana è:</p>
<div class="math notranslate nohighlight">
\[
L(\mu, \sigma^2 \mid y) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{1}{2\sigma^2} (y - \mu)^2 \right).
\]</div>
<p>Questa funzione di verosimiglianza quantifica quanto è probabile osservare un determinato valore di QI <span class="math notranslate nohighlight">\( y \)</span> data una distribuzione gaussiana con parametri <span class="math notranslate nohighlight">\( \mu \)</span> e <span class="math notranslate nohighlight">\( \sigma^2 \)</span>. In altre parole, fornisce una misura di quanto i parametri <span class="math notranslate nohighlight">\( \mu \)</span> e <span class="math notranslate nohighlight">\( \sigma^2 \)</span> siano coerenti con l’osservazione <span class="math notranslate nohighlight">\( y \)</span> effettuata.</p>
<p>Poniamo</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="mi">114</span>
</pre></div>
</div>
</div>
</div>
<p>L’eq. <code class="xref eq docutils literal notranslate"><span class="pre">eq-gaussian-sim-like</span></code> dipende dai parametri <span class="math notranslate nohighlight">\(\mu\)</span> e <span class="math notranslate nohighlight">\(\sigma\)</span> e dai dati <span class="math notranslate nohighlight">\(y\)</span>. Per semplicità, ipotizziamo <span class="math notranslate nohighlight">\(\sigma\)</span> noto e uguale a 15. Nell’esercizio considereremo 1000 valori <span class="math notranslate nohighlight">\(\mu\)</span> compresi tra 70 e 160.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">70.0</span><span class="p">,</span> <span class="mf">160.0</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Nell’analisi che stiamo conducendo, esaminiamo 1000 diversi valori per il parametro <span class="math notranslate nohighlight">\( \mu \)</span>. Per costruire la funzione di verosimiglianza, applichiamo l’equazione (eq-gaussian-likelihood) per ciascuno di questi valori. In questo modo, otteniamo una valutazione della plausibilità relativa di ciascun valore di <span class="math notranslate nohighlight">\( \mu \)</span> alla luce dei dati osservati.</p>
<p>Per essere precisi, in ogni iterazione del calcolo applichiamo l’equazione (eq-gaussian-likelihood) utilizzando:</p>
<ul class="simple">
<li><p>Il valore osservato <span class="math notranslate nohighlight">\( y \)</span>, che è costante per tutte le iterazioni.</p></li>
<li><p>Un valore noto e costante per <span class="math notranslate nohighlight">\( \sigma \)</span>.</p></li>
<li><p>Un valore specifico per <span class="math notranslate nohighlight">\( \mu \)</span> tratto dall’insieme dei 1000 valori che stiamo esaminando.</p></li>
</ul>
<p>Pertanto, tra le variabili nell’equazione, <span class="math notranslate nohighlight">\( \mu \)</span> è l’unica che cambia in ciascuna delle 1000 iterazioni, mentre <span class="math notranslate nohighlight">\( y \)</span> e <span class="math notranslate nohighlight">\( \sigma \)</span> rimangono invariati.</p>
<p>In Python, la densità di probabilità della distribuzione gaussiana può essere calcolata utilizzando la funzione <code class="docutils literal notranslate"><span class="pre">norm.pdf()</span></code> dalla libreria <code class="docutils literal notranslate"><span class="pre">scipy.stats</span></code>. Questa funzione accetta come argomenti:</p>
<ul class="simple">
<li><p>Il valore osservato <span class="math notranslate nohighlight">\( y \)</span> o un array di tali valori.</p></li>
<li><p>La media <span class="math notranslate nohighlight">\( \mu \)</span> o un array di medie, nel caso si voglia valutare più di una distribuzione gaussiana.</p></li>
<li><p>La deviazione standard <span class="math notranslate nohighlight">\( \sigma \)</span> o un array di deviazioni standard, analogamente al caso delle medie.</p></li>
</ul>
<p>In pratica, utilizzando la funzione <code class="docutils literal notranslate"><span class="pre">norm.pdf()</span></code> con un valore fisso di <span class="math notranslate nohighlight">\( y \)</span> (ad esempio, <span class="math notranslate nohighlight">\( y = 114 \)</span>) e <span class="math notranslate nohighlight">\( \sigma \)</span> (ad esempio, <span class="math notranslate nohighlight">\( \sigma = 15 \)</span>), e variando <span class="math notranslate nohighlight">\( \mu \)</span> attraverso i 1000 valori predefiniti, otteniamo un array di 1000 valori di densità di probabilità. Questi rappresentano la verosimiglianza di ciascun valore specifico di <span class="math notranslate nohighlight">\( \mu \)</span> dato l’osservato <span class="math notranslate nohighlight">\( y \)</span>.</p>
<p>Applicando la funzione <code class="docutils literal notranslate"><span class="pre">norm.pdf()</span></code> 1000 volte, una volta per ciascuno dei valori <span class="math notranslate nohighlight">\(\mu\)</span> che abbiamo definito (e tenendo fissi <span class="math notranslate nohighlight">\(y = 114\)</span> e <span class="math notranslate nohighlight">\(\sigma = 15\)</span>), otteniamo 1000 valori <span class="math notranslate nohighlight">\(f(\mu)\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f_mu</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>La funzione di verosimiglianza è la curva che interpola i punti <span class="math notranslate nohighlight">\(\big(\mu, f(\mu)\big)\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">f_mu</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Funzione di verosimiglianza&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Valore della variabile casuale mu [70, 160]&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Verosimiglianza&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">70</span><span class="p">,</span> <span class="mi">160</span><span class="p">]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/d80735b973a737ec4f6ec1816fda78685029edf5f14099780a58f9b528790b17.png" src="../_images/d80735b973a737ec4f6ec1816fda78685029edf5f14099780a58f9b528790b17.png" />
</div>
</div>
<p>La funzione di verosimiglianza così trovata ha la forma della distribuzione Gaussiana. Nel caso di una singola osservazione, <em>ma solo in questo caso</em>, ha anche un’area unitaria. Per l’esempio presente, la moda della funzione di verosimiglianza è 114.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">l</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">mu</span><span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">argmax</span><span class="p">()]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>113.96396396396396
</pre></div>
</div>
</div>
</div>
</section>
<section id="campione-indipendente-di-osservazioni-da-una-distribuzione-gaussiana">
<h3>Campione indipendente di osservazioni da una distribuzione gaussiana<a class="headerlink" href="#campione-indipendente-di-osservazioni-da-una-distribuzione-gaussiana" title="Permalink to this heading">#</a></h3>
<p>Esaminiamo il caso più complesso di un campione di <span class="math notranslate nohighlight">\( n \)</span> osservazioni indipendenti estratte da una distribuzione gaussiana. Questo campione può essere concepito come una serie di <span class="math notranslate nohighlight">\( n \)</span> realizzazioni indipendenti e identicamente distribuite (i.i.d.) di una variabile casuale <span class="math notranslate nohighlight">\( Y \)</span> che segue una distribuzione normale <span class="math notranslate nohighlight">\( \mathcal{N}(\mu, \sigma^2) \)</span>. Gli elementi del campione sono quindi estratti da una popolazione che segue una distribuzione normale con parametri <span class="math notranslate nohighlight">\( \mu \)</span> e <span class="math notranslate nohighlight">\( \sigma \)</span>, entrambi sconosciuti, e che possiamo denotare con <span class="math notranslate nohighlight">\( \theta = \{\mu, \sigma\} \)</span>.</p>
<p>Nel caso in cui le osservazioni <span class="math notranslate nohighlight">\( y_1, y_2, \ldots, y_n \)</span> siano i.i.d., la densità di probabilità congiunta del campione è espressa come:</p>
<div class="math notranslate nohighlight">
\[
f(y \mid \theta) = \prod_{i=1}^{n} f(y_i \mid \theta),
\]</div>
<p>dove <span class="math notranslate nohighlight">\( f(\cdot) \)</span> rappresenta la funzione di densità di probabilità gaussiana parametrizzata da <span class="math notranslate nohighlight">\( \mu \)</span> e <span class="math notranslate nohighlight">\( \sigma \)</span>.</p>
<p>Mantenendo i dati osservati <span class="math notranslate nohighlight">\( y \)</span> costanti, la funzione di verosimiglianza per il campione diventa equivalente all’equazione (eq-gaussian-sim-like).</p>
<p>Per illustrare questo concetto concretamente, prendiamo in esame un esempio che utilizza i punteggi BDI-II (Beck Depression Inventory II) di trenta partecipanti in uno studio clinico, come descritto in <span id="id2">Zetsche <em>et al.</em> [<a class="reference internal" href="../references/bibliography.html#id58" title="Ulrike Zetsche, Paul-Christian Buerkner, and Babette Renneberg. Future expectations in clinical depression: biased or realistic? Journal of Abnormal Psychology, 128(7):678, 2019.">ZBR19</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span>
    <span class="mi">26</span><span class="p">,</span>
    <span class="mi">35</span><span class="p">,</span>
    <span class="mi">30</span><span class="p">,</span>
    <span class="mi">25</span><span class="p">,</span>
    <span class="mi">44</span><span class="p">,</span>
    <span class="mi">30</span><span class="p">,</span>
    <span class="mi">33</span><span class="p">,</span>
    <span class="mi">43</span><span class="p">,</span>
    <span class="mi">22</span><span class="p">,</span>
    <span class="mi">43</span><span class="p">,</span>
    <span class="mi">24</span><span class="p">,</span>
    <span class="mi">19</span><span class="p">,</span>
    <span class="mi">39</span><span class="p">,</span>
    <span class="mi">31</span><span class="p">,</span>
    <span class="mi">25</span><span class="p">,</span>
    <span class="mi">28</span><span class="p">,</span>
    <span class="mi">35</span><span class="p">,</span>
    <span class="mi">30</span><span class="p">,</span>
    <span class="mi">26</span><span class="p">,</span>
    <span class="mi">31</span><span class="p">,</span>
    <span class="mi">41</span><span class="p">,</span>
    <span class="mi">36</span><span class="p">,</span>
    <span class="mi">26</span><span class="p">,</span>
    <span class="mi">35</span><span class="p">,</span>
    <span class="mi">33</span><span class="p">,</span>
    <span class="mi">28</span><span class="p">,</span>
    <span class="mi">27</span><span class="p">,</span>
    <span class="mi">34</span><span class="p">,</span>
    <span class="mi">27</span><span class="p">,</span>
    <span class="mi">22</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Il nostro obiettivo è di costruire una funzione di verosimiglianza per i punteggi BDI-II di un campione di 30 partecipanti. Supponiamo, sulla base di ricerche precedenti, che questi punteggi siano distribuiti secondo una legge normale. Inoltre, assumiamo che la deviazione standard <span class="math notranslate nohighlight">\( \sigma \)</span> sia nota e pari alla deviazione standard del campione, che è 6.50.</p>
<p>Per la prima osservazione del campione, <span class="math notranslate nohighlight">\( y_1 = 26 \)</span>, la funzione di densità di probabilità è data da:</p>
<div class="math notranslate nohighlight">
\[
f(26 \mid \mu_0, \sigma = 6.50) = \frac{1}{6.50 \sqrt{2\pi}} \exp \left\{ -\frac{(26 - \mu_0)^2}{2 \times 6.50^2} \right\}.
\]</div>
<p>Per il campione completo, la densità di probabilità congiunta è il prodotto delle densità di tutte le singole osservazioni:</p>
<div class="math notranslate nohighlight">
\[
f(y \mid \mu, \sigma = 6.50) = \prod_{i=1}^{n} f(y_i \mid \mu, \sigma = 6.50).
\]</div>
<p>Quindi, la funzione di verosimiglianza, denotata come <span class="math notranslate nohighlight">\( \mathcal{L}(\mu_0, \sigma = 6.50 \mid y) \)</span>, è determinata dal prodotto delle densità di probabilità di tutte le osservazioni nel campione:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{L}(\mu_0, \sigma=6.50 \mid y) &amp;= \prod_{i=1}^{30} \frac{1}{6.50 \sqrt{2\pi}} \exp \left\{ -\frac{(y_i - \mu_0)^2}{2 \times 6.50^2} \right\} \\
&amp;= \frac{1}{6.50 \sqrt {2\pi}} \exp\left\{ -\frac{(26 - \mu_0)^2}{2 \times 6.50^2} \right\} \\
&amp;\quad \times \frac{1}{6.50 \sqrt {2\pi}} \exp\left\{ -\frac{(35 - \mu_0)^2}{2 \times 6.50^2} \right\} \\
&amp;\quad \times \vdots \\
&amp;\quad \times \frac{1}{6.50 \sqrt {2\pi}} \exp\left\{ -\frac{(22 - \mu_0)^2}{2 \times 6.50^2} \right\}.
\end{aligned}
\end{split}\]</div>
<p>In questo contesto, <span class="math notranslate nohighlight">\( \mu_0 \)</span> è uno dei molti possibili valori del parametro <span class="math notranslate nohighlight">\( \mu \)</span>. Se consideriamo, ad esempio, 1000 diversi valori possibili per <span class="math notranslate nohighlight">\( \mu \)</span>, dovremmo calcolare la funzione di verosimiglianza 1000 volte, una per ciascun valore di <span class="math notranslate nohighlight">\( \mu \)</span>.</p>
<p>Per rendere i calcoli più gestibili, è consigliabile utilizzare il logaritmo della funzione di verosimiglianza. In Python, possiamo definire una funzione <code class="docutils literal notranslate"><span class="pre">log_likelihood()</span></code> che accetta come argomenti <code class="docutils literal notranslate"><span class="pre">y</span></code>, <code class="docutils literal notranslate"><span class="pre">mu</span></code> e <code class="docutils literal notranslate"><span class="pre">sigma</span> <span class="pre">=</span> <span class="pre">true_sigma</span></code>. Per semplificare, impostiamo <code class="docutils literal notranslate"><span class="pre">true_sigma</span></code> uguale alla deviazione standard osservata nel campione.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">true_sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">true_sigma</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>6.495810615739622
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">true_sigma</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">true_sigma</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Consideriamo, ad esempio, il valore <span class="math notranslate nohighlight">\(\mu_0 = \bar{y}\)</span>, ovvero</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bar_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bar_y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>30.933333333333334
</pre></div>
</div>
</div>
</div>
<p>L’ordinata della funzione di log-verosimiglianza in corrispondenza di <span class="math notranslate nohighlight">\(\mu = 30.93\)</span> è</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">log_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mf">30.93</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">true_sigma</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-98.70288339960591
</pre></div>
</div>
</div>
</div>
<p>Troviamo ora i valori della log-verosimiglianza per ciascuno dei 1000 valori <span class="math notranslate nohighlight">\(\mu\)</span> nell’intervallo <span class="math notranslate nohighlight">\([\bar{y} - 2 \sigma, \bar{y} + 2 \sigma]\)</span>. Iniziamo a definire il vettore <code class="docutils literal notranslate"><span class="pre">mu</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">num</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Troviamo il valore dell’ordinata della funzione di log-verosimiglianza in corrispondenza di ciascuno dei 1000 valori <code class="docutils literal notranslate"><span class="pre">mu</span></code> che abbiamo definito.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ll</span> <span class="o">=</span> <span class="p">[</span><span class="n">log_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">mu_val</span><span class="p">,</span> <span class="n">true_sigma</span><span class="p">)</span> <span class="k">for</span> <span class="n">mu_val</span> <span class="ow">in</span> <span class="n">mu</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Nel caso di un solo parametro sconosciuto (nel caso presente, <span class="math notranslate nohighlight">\(\mu\)</span>) è possibile rappresentare la log-verosimiglianza con una curva che interpola i punti (<code class="docutils literal notranslate"><span class="pre">mu</span></code>, <code class="docutils literal notranslate"><span class="pre">ll</span></code>). Tale funzione descrive la <em>credibilità relativa</em> che può essere attribuita ai valori del parametro <span class="math notranslate nohighlight">\(\mu\)</span> alla luce dei dati osservati.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">ll</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Funzione di log-verosimiglianza&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Valore della variabile casuale mu&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Log-verosimiglianza&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/f92f8e0c178da5a7afc316c2e2892115657fdb50938db443482e955309bc04d6.png" src="../_images/f92f8e0c178da5a7afc316c2e2892115657fdb50938db443482e955309bc04d6.png" />
</div>
</div>
<p>Il valore <span class="math notranslate nohighlight">\(\mu\)</span> più credibile corrisponde al massimo della funzione di log-verosimiglinza e viene detto <em>stima di massima verosimiglianza</em>.</p>
<p>Il massimo della funzione di log-verosimiglianza, ovvero 30.93 per l’esempio in discussione, è identico alla media dei dati campionari.</p>
</section>
</section>
<section id="derivazione-formale-della-stima-di-massima-verosimiglianza-per-una-distribuzione-gaussiana">
<h2>Derivazione Formale della Stima di Massima Verosimiglianza per una Distribuzione Gaussiana<a class="headerlink" href="#derivazione-formale-della-stima-di-massima-verosimiglianza-per-una-distribuzione-gaussiana" title="Permalink to this heading">#</a></h2>
<p>Per ottenere le stime di massima verosimiglianza (MLE) dei parametri <span class="math notranslate nohighlight">\( \mu \)</span> e <span class="math notranslate nohighlight">\( \sigma^2 \)</span> in una distribuzione gaussiana, ricorriamo all’analisi matematica, in particolare al calcolo delle derivate. La funzione di log-verosimiglianza <span class="math notranslate nohighlight">\( \ell(\theta) \)</span> per una distribuzione gaussiana è data da:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\ell(\mu, \sigma^2 \mid Y) &amp;= \log \left( (2\pi\sigma^2)^{-n/2} \exp \left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \mu)^2 \right) \right) \\
&amp;= -\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \mu)^2.
\end{aligned}
\end{split}\]</div>
<p>L’obiettivo è massimizzare <span class="math notranslate nohighlight">\( \ell(\mu, \sigma^2 \mid Y) \)</span> rispetto a <span class="math notranslate nohighlight">\( \mu \)</span> e <span class="math notranslate nohighlight">\( \sigma^2 \)</span>, ovvero:</p>
<div class="math notranslate nohighlight">
\[
\max_{\mu, \sigma^2} \ell(\mu, \sigma^2 \mid Y).
\]</div>
<section id="stima-di-massima-verosimiglianza-per-mu">
<h3>Stima di Massima Verosimiglianza per <span class="math notranslate nohighlight">\( \mu \)</span><a class="headerlink" href="#stima-di-massima-verosimiglianza-per-mu" title="Permalink to this heading">#</a></h3>
<p>Per trovare la stima di <span class="math notranslate nohighlight">\( \mu \)</span> che massimizza la funzione di log-verosimiglianza, calcoliamo la derivata parziale di <span class="math notranslate nohighlight">\( \ell \)</span> rispetto a <span class="math notranslate nohighlight">\( \mu \)</span> e la poniamo uguale a zero:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial \ell}{\partial \mu} &amp;= \frac{\partial}{\partial \mu} \left( -\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \mu)^2 \right) \\
&amp;= \sum_{i=1}^n \frac{(y_i - \mu)}{\sigma^2} \\
&amp;= \frac{1}{\sigma^2} \sum_{i=1}^n (y_i - \mu) = 0.
\end{aligned}
\end{split}\]</div>
<p>Risolvendo questa equazione, otteniamo:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\hat{\mu}_{MLE} &amp;= \frac{1}{n} \sum_{i=1}^n y_i \\
&amp;= \bar{y}.
\end{aligned}
\end{split}\]</div>
<p>La stima di massima verosimiglianza per <span class="math notranslate nohighlight">\( \mu \)</span> corrisponde quindi alla media aritmetica delle osservazioni.</p>
</section>
<section id="stima-di-massima-verosimiglianza-per-sigma-2">
<h3>Stima di Massima Verosimiglianza per <span class="math notranslate nohighlight">\( \sigma^2 \)</span><a class="headerlink" href="#stima-di-massima-verosimiglianza-per-sigma-2" title="Permalink to this heading">#</a></h3>
<p>Una derivazione simile può essere effettuata per <span class="math notranslate nohighlight">\( \sigma^2 \)</span>, e si può dimostrare che la stima di massima verosimiglianza per <span class="math notranslate nohighlight">\( \sigma^2 \)</span> è la varianza campionaria.</p>
<p>In sintesi, le stime di massima verosimiglianza per <span class="math notranslate nohighlight">\( \mu \)</span> e <span class="math notranslate nohighlight">\( \sigma^2 \)</span> in una distribuzione gaussiana coincidono con la media e la varianza campionaria, rispettivamente.</p>
</section>
</section>
<section id="conclusione-e-riflessioni-finali">
<h2>Conclusione e Riflessioni Finali<a class="headerlink" href="#conclusione-e-riflessioni-finali" title="Permalink to this heading">#</a></h2>
<p>La funzione di verosimiglianza rappresenta un elemento cruciale che collega i dati osservati ai parametri di un modello statistico. Essa fornisce una misura della plausibilità dei dati in relazione a diversi valori possibili dei parametri del modello. La strutturazione di una funzione di verosimiglianza richiede la considerazione di tre componenti fondamentali: il modello statistico che si presume abbia generato i dati, l’insieme di valori possibili per i parametri di tale modello e le osservazioni empiriche che effettivamente abbiamo a disposizione.</p>
<p>La funzione di verosimiglianza è centrale nella pratica dell’inferenza statistica. Essa ci permette di quantificare quanto bene differenti set di parametri potrebbero aver generato i dati osservati. Questo è fondamentale sia per la selezione del modello che per la stima dei parametri, e pertanto è indispensabile per un’analisi dati rigorosa e per un’interpretazione accurata dei risultati.</p>
<p>Un’applicazione pratica e illustrativa dei principi esposti in questo capitolo è fornita nel capitolo <a class="reference internal" href="10_rescorla_wagner.html#notebook-rescorla-wagner"><span class="std std-ref">Apprendimento per rinforzo</span></a>, che è un esempio di come la teoria della verosimiglianza possa essere applicata per affrontare questioni empiriche complesse in psicologia.</p>
<p>In sintesi, la comprensione e l’applicazione appropriata della funzione di verosimiglianza sono passaggi essenziali nel processo di analisi dati. Essa costituisce uno strumento indispensabile per chi è impegnato nella ricerca empirica e nell’interpretazione di dati complessi.</p>
</section>
<section id="watermark">
<h2>Watermark<a class="headerlink" href="#watermark" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">load_ext</span> watermark
<span class="o">%</span><span class="k">watermark</span> -n -u -v -iv -w
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Last updated: Thu Nov 09 2023

Python implementation: CPython
Python version       : 3.11.6
IPython version      : 8.16.1

numpy     : 1.25.2
arviz     : 0.16.1
scipy     : 1.11.3
seaborn   : 0.13.0
matplotlib: 3.8.0
pandas    : 2.1.1

Watermark: 2.4.3
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_3"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="E_beta_distr.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">✏️ Esercizi</p>
      </div>
    </a>
    <a class="right-next"
       href="10_rescorla_wagner.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Apprendimento per rinforzo</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#concetto-di-verosimiglianza-e-sua-formalizzazione-matematica">Concetto di Verosimiglianza e Sua Formalizzazione Matematica</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#verosimiglianza-nel-contesto-del-modello-binomiale-un-esempio-approfondito">Verosimiglianza nel Contesto del Modello Binomiale: Un Esempio Approfondito</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretazione-della-funzione-di-verosimiglianza">Interpretazione della Funzione di Verosimiglianza</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#la-funzione-di-log-verosimiglianza">La Funzione di Log-Verosimiglianza</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#verosimiglianza-congiunta-di-n-campioni-iid-da-una-distribuzione-binomiale">Verosimiglianza Congiunta di <span class="math notranslate nohighlight">\( n \)</span> Campioni IID da una Distribuzione Binomiale</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modello-gaussiano-e-verosimiglianza">Modello Gaussiano e Verosimiglianza</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#caso-di-una-singola-osservazione">Caso di una Singola Osservazione</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#campione-indipendente-di-osservazioni-da-una-distribuzione-gaussiana">Campione indipendente di osservazioni da una distribuzione gaussiana</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derivazione-formale-della-stima-di-massima-verosimiglianza-per-una-distribuzione-gaussiana">Derivazione Formale della Stima di Massima Verosimiglianza per una Distribuzione Gaussiana</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stima-di-massima-verosimiglianza-per-mu">Stima di Massima Verosimiglianza per <span class="math notranslate nohighlight">\( \mu \)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stima-di-massima-verosimiglianza-per-sigma-2">Stima di Massima Verosimiglianza per <span class="math notranslate nohighlight">\( \sigma^2 \)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusione-e-riflessioni-finali">Conclusione e Riflessioni Finali</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#watermark">Watermark</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Corrado Caudek
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>