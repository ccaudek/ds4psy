

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Apprendimento per rinforzo &#8212; ds4p</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="../_static/a11y.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=365ca57ee442770a23c6"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/custom.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-VMXNE4BCDL"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-VMXNE4BCDL');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_3/10_rescorla_wagner';</script>
    <link rel="canonical" href="https://ccaudek.github.io/ds4psy/chapter_3/10_rescorla_wagner.html" />
    <link rel="shortcut icon" href="../_static/increasing.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="✏️ Esercizi" href="E_likelihood.html" />
    <link rel="prev" title="La verosimiglianza" href="09_likelihood.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="ds4p - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="ds4p - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Benvenuti
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_1/introduction_chapter_1.html">Python</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/01_python_1.html">Python (1)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/02_python_2.html">Python (2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_python.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/03_numpy.html">NumPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_numpy.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/04_pandas.html">Pandas (1)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/05_pandas_aggregate.html">Pandas (2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/06_pandas_functions.html">Pandas (3)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_pandas.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/07_matplotlib.html">Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/08_seaborn.html">Seaborn</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_matplotlib.html">✏️ Esercizi</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_2/introduction_chapter_2.html">Statistica descrittiva</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/01_key_notions.html">Concetti chiave</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_key_notions.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/02_measurement.html">La misurazione in psicologia</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_scales.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/03_freq_distr.html">Dati e frequenze</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_sums.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/04_loc_scale.html">Indici di posizione e di scala</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/05_correlation.html">Le relazioni tra variabili</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/06_causality.html">Correlazione e causazione</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/07_crisis.html">La crisi della generalizzabilità</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_eda.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_mehr_song_spelke.html">✏️ Esercizi</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="introduction_chapter_3.html">Probabilità</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01_intro_prob.html">Introduzione al calcolo delle probabilità</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_prob.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="02_conditional_prob.html">Probabilità condizionata</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_cond_prob.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_bayes_theorem.html">Il teorema di Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_bayes_theorem.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="04_expval_var.html">Variabili casuali</a></li>
<li class="toctree-l2"><a class="reference internal" href="04a_sampling_distr.html">Stime, stimatori e parametri</a></li>
<li class="toctree-l2"><a class="reference internal" href="04b_illusion.html">Incertezza inferenziale e variabilità dei risultati</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_rv_discrete.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_joint_prob.html">Probabilità congiunta</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_joint_prob.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="06_density_func.html">La funzione di densità di probabilità</a></li>
<li class="toctree-l2"><a class="reference internal" href="07_discr_rv_distr.html">Distribuzioni di v.c. discrete</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_binomial.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="08_cont_rv_distr.html">Distribuzioni di v.c. continue</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_gaussian.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_beta_distr.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="09_likelihood.html">La verosimiglianza</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Apprendimento per rinforzo</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_likelihood.html">✏️ Esercizi</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_4/introduction_part_4.html">Inferenza bayesiana</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/01_intro_bayes.html">Modellazione bayesiana</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/02_subj_prop.html">Pensare ad una proporzione in termini soggettivi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/03_conjugate_families_1.html">Distribuzioni coniugate (1)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/04_conjugate_families_2.html">Distribuzioni coniugate (2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/05_summary_posterior.html">Sintesi a posteriori</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/E_conjugate.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/06_balance-prior-post.html">L’influenza della distribuzione a priori</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/10_metropolis.html">Monte Carlo a Catena di Markov</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/11_beta_binomial_pymc.html">Inferenza bayesiana con PyMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/12_jax.html">Usare JAX per un campionamento più veloce</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/13_preliz.html">Scegliere le distribuzioni a priori</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/16_summary_posterior_pymc.html">Metodi di sintesi della distribuzione a posteriori</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/17_prediction.html">La predizione bayesiana</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/18_mcmc_diagnostics.html">Diagnostica delle catene markoviane</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/19_odds_ratio.html">Analisi bayesiana dell’odds-ratio</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/20_poisson_model.html">Modello di Poisson</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/21_poisson_sim.html">Modello di Poisson: derivazione analitica e MCMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/E_freq.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/22_normal_normal_model.html">Inferenza bayesiana su una media</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/E_one_mean.html">✏️ Esercizio</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/E_one_mean_2.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/23_two_groups.html">Confronto tra due gruppi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/24_multiple_groups.html">Gruppi multipli</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/30_entropy.html">Entropia</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/31_kl.html">La divergenza di Kullback-Leibler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/40_hier_beta_binom.html">Modello gerarchico beta-binomiale</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/41_hier_poisson.html">Modello gerarchico di Poisson</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/42_hier_gaussian.html">Modello gerarchico gaussiano</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_4/hssm.html">Drift Diffusion Model</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_5/introduction_part_5.html">Analisi della regressione</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_1.html">Il modello di regressione lineare</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_2.html">Analisi bayesiana del modello di regressione lineare</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_params_recovery.html">Analisi di simulazione per la stima dei parametri nel modello di regressione</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_3.html">Zucchero sintattico</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_4.html">Confronto tra le medie di due gruppi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_5.html">Il modello lineare gerarchico</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_multilevel_modeling.html">A Primer on Bayesian Methods for Multilevel Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_7.html">Regressione robusta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_simpson.html">Paradosso di Simpson</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_reglin_1.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_reglin_2.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_reglin_3.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_reglin_4.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_logistic_reg.html">Modello di regressione logistica</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_binomial_reg.html">Regressione binomiale</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_covid.html">Inferenza controfattuale: calcolo delle morti in eccesso dovute al COVID-19</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_counterfactual.html">Analisi causale con PyMC</a></li>

<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_stab.html">✏️ Esercizi</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_6/introduction_part_6.html">Inferenza frequentista</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_estimation.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/02_conf_interv.html">Intervallo di confidenza</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/03_test_ipotesi.html">Significatività statistica</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_interpretation_test.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_significato_test.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/04_two_ind_samples.html">Test t di Student per campioni indipendenti</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_test_media_pop.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_medie_pop_ampie.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_medie_pop_piccoli.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_campioni_appaiati.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_confronto_proporzioni.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/05_limiti_stat_frequentista.html">Limiti dell’inferenza frequentista</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/06_s_m_errors.html">Crisi della replicabilità</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../references/bibliography.html">Bibliografia</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_7/introduction_appendix.html">Appendici</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/00_installation.html">Ambiente di lavoro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a01_math_symbols.html">Simbologia di base</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a02_numbers.html">Numeri binari, interi, razionali, irrazionali e reali</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a04_summation_notation.html">Simbolo di somma (sommatorie)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a05_sets.html">Insiemi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a06_combinatorics.html">Calcolo combinatorio</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a07_calculus.html">Per liberarvi dai terrori preliminari</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a08_kde_plot.html">Kernel Density Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a09_prob_tutorial.html">Esercizi di probabilità discreta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a12_rng.html">Generazione di numeri casuali</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a14_predict_counts.html">La predizione delle frequenze</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a15_lin_fun.html">La funzione lineare</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a20_reglin_1.html">Regressione lineare bivariata</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a21_reglin_2.html">Regressione lineare con Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a23_reglin_4.html">Posterior Predictive Checks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a30_ttest_exercises.html">Esercizi sull’inferenza frequentista</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/ccaudek/ds4psy/blob/main/docs/chapter_3/10_rescorla_wagner.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapter_3/10_rescorla_wagner.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Apprendimento per rinforzo</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#teoria-il-modello-rescorla-wagner">Teoria: il modello Rescorla-Wagner</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simulare-l-apprendimento">Simulare l’apprendimento</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#la-regola-di-apprendimento-per-rinforzo-delta-rule">La regola di apprendimento per rinforzo (<span class="math notranslate nohighlight">\(\delta\)</span>-rule)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax">Softmax</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stima-dei-parametri-del-modello">Stima dei parametri del modello</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calcolo-del-logaritmo-negativo-della-verosimiglianza">Calcolo del logaritmo negativo della verosimiglianza</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#validazione">Validazione</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#commenti-e-considerazioni-conclusive">Commenti e considerazioni conclusive</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#watermark">Watermark</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <p><a target="_blank" rel="noopener noreferrer" href="https://colab.research.google.com/github/ccaudek/ds4psy_2023/blob/main/226_rescorla_wagner.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<section class="tex2jax_ignore mathjax_ignore" id="apprendimento-per-rinforzo">
<span id="notebook-rescorla-wagner"></span><h1>Apprendimento per rinforzo<a class="headerlink" href="#apprendimento-per-rinforzo" title="Permalink to this heading">#</a></h1>
<p>Per illustrare un esempio di stima dei parametri mediante massima verosimiglianza, prendiamo in considerazione uno dei modelli psicologici di maggior successo: il modello di apprendimento di Rescorla-Wagner. Questo modello offre una possibile spiegazione al processo di apprendimento associativo e dipende da due parametri fondamentali: <span class="math notranslate nohighlight">\(\alpha\)</span> e <span class="math notranslate nohighlight">\(\beta\)</span>. Vedremo come il metodo della massima verosimiglianza può essere impiegato per stimare tali parametri utilizzando i dati empirici ottenuti da un partecipante coinvolto in un compito di apprendimento associativo.</p>
<p>Il presente tutorial trae ispirazione dall’articolo di <a class="reference external" href="https://elifesciences.org/articles/49547">Wilson &amp; Collins (2019)</a> e utilizza il codice fornito da <a class="reference external" href="https://shawnrhoads.github.io/gu-psyc-347/index.html">Rhoads, S. A. &amp; Gan, L. (2022)</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>  <span class="c1"># matrix/array functions</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>  <span class="c1"># loading and manipulating data</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>  <span class="c1"># plotting</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>  <span class="c1"># finding optimal params in models</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;retina&#39;
<span class="n">RANDOM_SEED</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">RANDOM_SEED</span><span class="p">)</span>
<span class="n">az</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;arviz-darkgrid&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="teoria-il-modello-rescorla-wagner">
<h2>Teoria: il modello Rescorla-Wagner<a class="headerlink" href="#teoria-il-modello-rescorla-wagner" title="Permalink to this heading">#</a></h2>
<p>Lo scopo degli studi sull’apprendimento per rinforzo è quello di comprendere come le persone imparano a massimizzare le loro ricompense in situazioni in cui la scelta migliore è inizialmente sconosciuta. In modo più specifico, consideriamo il seguente problema di apprendimento. Un partecipante deve effettuare ripetutamente delle scelte tra diverse opzioni o azioni, e dopo ogni scelta riceve una ricompensa numerica estratta da una distribuzione di probabilità che dipende dall’azione selezionata. L’obiettivo del partecipante è massimizzare la ricompensa totale attesa durante un certo periodo di tempo, ad esempio, durante 100 scelte. Per descrivere questa situazione, viene spesso utilizzata la metafora di un giocatore che deve fare una serie di <span class="math notranslate nohighlight">\(T\)</span> scelte tra <span class="math notranslate nohighlight">\(K\)</span> slot machine (conosciute anche come “multi-armed bandits”) al fine di massimizzare le sue vincite. Se nella scelta <span class="math notranslate nohighlight">\(t\)</span> viene selezionata la slot machine <span class="math notranslate nohighlight">\(k\)</span>, viene ottenuta una ricompensa <span class="math notranslate nohighlight">\(r_t\)</span> che ha valore <code class="docutils literal notranslate"><span class="pre">1</span></code> con una probabilità di successo <span class="math notranslate nohighlight">\(\mu^k_t\)</span>, altrimenti ha valore <code class="docutils literal notranslate"><span class="pre">0</span></code>. Le probabilità di successo sono diverse per ogni slot machine e inizialmente sono sconosciute al partecipante. Nella versione più semplice di questo compito, le probabilità di successo rimangono costanti nel tempo.</p>
</section>
<section id="simulare-l-apprendimento">
<h2>Simulare l’apprendimento<a class="headerlink" href="#simulare-l-apprendimento" title="Permalink to this heading">#</a></h2>
<p>In questo problema, ogni scelta ha un valore associato, che rappresenta la ricompensa attesa quando quella specifica scelta viene selezionata. Chiamiamo questo valore “valore della scelta”. Se si conosce il valore di ogni scelta, risolvere il problema di apprendimento significa semplicemente selezionare la scelta con il valore più alto.</p>
<p>Abbiamo tre parametri per questo problema:</p>
<ol class="arabic simple">
<li><p>il numero di tentativi, <span class="math notranslate nohighlight">\(T\)</span>;</p></li>
<li><p>il numero di slot machine, <span class="math notranslate nohighlight">\(K\)</span>;</p></li>
<li><p>le probabilità di ricompensa delle diverse opzioni, <span class="math notranslate nohighlight">\(\mu^k_t\)</span>, che possono o meno variare nel tempo.</p></li>
</ol>
<p>In questo tutorial simuleremo il comportamento di due slot machine basato sul modello di apprendimento Rescorla-Wagner. Imposteremo <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">=</span> <span class="pre">100</span></code> (100 tentativi), <code class="docutils literal notranslate"><span class="pre">K</span> <span class="pre">=</span> <span class="pre">2</span></code> (due slot machine) e <code class="docutils literal notranslate"><span class="pre">mu</span> <span class="pre">=</span> <span class="pre">[.2,</span> <span class="pre">.8]</span></code> (la slot machine 1 fornisce una ricompensa con probabilità del 20%, la slot machine 2 fornisce una ricompensa con probabilità dell’80%).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">T</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">mu</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<section id="la-regola-di-apprendimento-per-rinforzo-delta-rule">
<h3>La regola di apprendimento per rinforzo (<span class="math notranslate nohighlight">\(\delta\)</span>-rule)<a class="headerlink" href="#la-regola-di-apprendimento-per-rinforzo-delta-rule" title="Permalink to this heading">#</a></h3>
<p>Rescorla e Wagner (1972) hanno formulato, in uno degli articoli più influenti del XX secolo, l’ipotesi che l’apprendimento avvenga tramite la rilevazione degli errori di previsione. Nel loro modello, i partecipanti acquisiscono inizialmente le aspettative di valore per ciascuna slot machine basandosi sulla storia dei risultati pregressi, e utilizzano tali valori per guidare le loro decisioni future.</p>
<p><strong>Equazione di Aggiornamento (Rescorla-Wagner)</strong></p>
<p>Il modello di Rescorla-Wagner rappresenta un modello di apprendimento fondato sull’errore di previsione, in cui gli stimoli acquisiscono valore quando emerge una discordanza tra la previsione e l’esito:</p>
<p>\begin{equation}
V_{s,t} = V_{s,t-1} + \alpha (r_{t-1} - V_{s,t-1}),
\end{equation}</p>
<p>dove</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(V_{s,t}\)</span> rappresenta il valore dello stimolo “s” al tempo “t”, il quale riflette l’aspettativa di una ricompensa.</p></li>
<li><p><span class="math notranslate nohighlight">\(r_{t-1}\)</span> denota la ricompensa ottenuta nell’istante precedente “t-1”.</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span>, che varia tra 0 e 1, corrisponde al tasso di apprendimento.</p></li>
</ul>
<p>In questo modo, il valore viene aggiornato secondo l’errore di previsione, ovvero la differenza tra la ricompensa ricevuta “r” e l’aspettativa “V”.</p>
<p>Il tasso di apprendimento definisce l’entità con cui viene ponderato tale errore di previsione; ad esempio, un valore <span class="math notranslate nohighlight">\(\alpha\)</span> maggiore attribuisce un peso più rilevante all’errore di previsione.</p>
</section>
<section id="softmax">
<h3>Softmax<a class="headerlink" href="#softmax" title="Permalink to this heading">#</a></h3>
<p>All’inizio di ogni prova, il partecipante è chiamato a decidere quale slot machine scegliere, basandosi sui valori dei due stimoli. Il partecipante può:</p>
<ul class="simple">
<li><p>Scegliere sempre lo stimolo con il valore più alto.</p></li>
<li><p>Oppure, a volte, esplorare se l’altra slot machine potrebbe essere migliore.</p></li>
</ul>
<p>Nonostante la prima opzione porterebbe al massimo numero di ricompense in questo compito, sia gli esseri umani che gli animali spesso evitano di adottare questa strategia di “massimizzazione della probabilità” (ovvero, scegliere sempre lo stimolo con la probabilità più alta di ricompensa). Invece, tendono a selezionare lo stimolo con la probabilità di successo più alta con maggiore frequenza, ma non costantemente. Tuttavia, vi sono differenze nella quantità di “esplorazione” che integrano nel loro comportamento. Per catturare queste varie strategie di scelta (<em>exploration vs exploitation</em>), si ricorre a un modello che possa rappresentarle. A tale scopo, viene utilizzata l’equazione softmax:</p>
<p>\begin{align}
p(s) = \frac{exp(\beta \cdot V_{s})}{\sum_i exp(\beta \cdot V_{i})},
\end{align}</p>
<p>dove il parametro <span class="math notranslate nohighlight">\(\beta\)</span>, chiamato “temperatura”, regola il grado di casualità nella scelta. Varia da <span class="math notranslate nohighlight">\(\beta = 0\)</span> per rappresentare scelte completamente casuali fino a <span class="math notranslate nohighlight">\(\beta = \infty\)</span> per rappresentare la scelta deterministica dell’opzione con il valore più alto. In altre parole, un individuo con un alto valore di <span class="math notranslate nohighlight">\(\beta\)</span> sceglierà quasi sempre l’opzione con il valore più elevato (<span class="math notranslate nohighlight">\(V^k_t\)</span>), mentre uno con un basso valore di <span class="math notranslate nohighlight">\(\beta\)</span> esplorerà più frequentemente altre opzioni.</p>
<p>Esaminiamo l’impatto di <span class="math notranslate nohighlight">\(\beta\)</span> sulla probabilità di scelta. Supponiamo di avere 2 stimoli, A e B, dove il valore di B è sempre [1 - valore di A]. Nel grafico sottostante, abbiamo assunto un valore di <span class="math notranslate nohighlight">\(\beta\)</span> pari a 3 e abbiamo illustrato la probabilità di scegliere A in relazione al suo valore.</p>
<p>Iniziamo definendo la funzione softmax.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">va</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.02</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">)</span>
    <span class="n">vb</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">va</span>
    <span class="n">x</span> <span class="o">=</span> <span class="mf">0.02</span>

    <span class="n">pa</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="n">va</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="n">va</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="n">vb</span><span class="p">))</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">51</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">pa</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">tmp</span>
    <span class="n">x_axis</span> <span class="o">=</span> <span class="n">va</span> <span class="o">-</span> <span class="n">vb</span>
    <span class="k">return</span> <span class="n">pa</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">x_axis</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pa</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">x_axis</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">()</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">pa</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Beta=&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">beta</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;*&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;None&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Softmax function&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Choice(P=A)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Value(A) - Value(B)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/21e87db82b603007b712f0017ea3cbf4412da4d84d06a9b69972f8dff78be4dc.png" src="../_images/21e87db82b603007b712f0017ea3cbf4412da4d84d06a9b69972f8dff78be4dc.png" />
</div>
</div>
<ul class="simple">
<li><p>La probabilità di scegliere lo stimolo A cresce in modo monotonico con la differenza di valore tra A e B.</p></li>
<li><p>Gli asterischi indicano le scelte effettuate dal soggetto in base a queste probabilità di scelta; è evidente che il soggetto preferisce lo stimolo A nella maggior parte dei casi quando VA &gt; vBB, ma non sempre.</p></li>
<li><p>Da qui proviene il termine ‘softmax’: il soggetto sceglie lo stimolo con il valore massimo nella maggior parte dei casi, ma non sempre; pertanto, si parla di una funzione di massimizzazione ‘soft’.</p></li>
<li><p>Un valore di <span class="math notranslate nohighlight">\(\beta\)</span> più elevato rende le scelte più ‘deterministiche’, cioè è più probabile che il partecipante scelga l’opzione con il valore più alto.</p></li>
<li><p>Con un <span class="math notranslate nohighlight">\(\beta\)</span> più basso, il partecipante diventa più ‘esplorativo’, ovvero è più propenso a scegliere anche l’altra opzione di tanto in tanto.</p></li>
</ul>
<p>La funzione softmax svolge il ruolo di convertire i valori di <span class="math notranslate nohighlight">\(V\)</span> e <span class="math notranslate nohighlight">\(\beta\)</span> in una distribuzione di probabilità. Per illustrare meglio questo concetto, consideriamo di mantenere costanti i valori di <span class="math notranslate nohighlight">\(V\)</span> (ad esempio, 0.1 per A e 0.75 per B) e di variare il parametro <span class="math notranslate nohighlight">\(\beta\)</span>. In altre parole, stiamo esaminando come cambia la probabilità di scelta quando la “temperatura” <span class="math notranslate nohighlight">\(\beta\)</span> varia.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Ridefiniamo la funzione softmax in modo tale che dipenda solo da V e beta.</span>
<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="n">V</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="n">V</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">p</span>
</pre></div>
</div>
</div>
</div>
<p>Definiamo un insieme di 100 valori beta compresi tra 0 e 5.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">beta_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Finalmente, calcoliamo la probabilità.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">probability_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>  <span class="c1"># Initialize an array to store probabilities</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">beta</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">beta_values</span><span class="p">):</span>
    <span class="n">probability_array</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Esploriamo passo dopo passo il codice precedente:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">probability_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>  
</pre></div>
</div>
<p>Qui viene creato un array NumPy chiamato <code class="docutils literal notranslate"><span class="pre">probability_array</span></code> con dimensioni <code class="docutils literal notranslate"><span class="pre">(2,</span> <span class="pre">100)</span></code>. Questo array sarà utilizzato per immagazzinare le probabilità calcolate dalla funzione <code class="docutils literal notranslate"><span class="pre">softmax</span></code> per ogni valore di <code class="docutils literal notranslate"><span class="pre">beta</span></code>. La prima dimensione rappresenta il numero di opzioni (2 opzioni nel nostro caso), mentre la seconda dimensione rappresenta il numero di diversi valori di <code class="docutils literal notranslate"><span class="pre">beta</span></code> (100 nel nostro caso).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">beta</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">beta_values</span><span class="p">):</span>
    <span class="n">probability_array</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
</pre></div>
</div>
<p>Qui, viene usato un ciclo <code class="docutils literal notranslate"><span class="pre">for</span></code> per iterare su ciascun valore di <code class="docutils literal notranslate"><span class="pre">beta</span></code> nell’array <code class="docutils literal notranslate"><span class="pre">beta_values</span></code>. La funzione <code class="docutils literal notranslate"><span class="pre">enumerate</span></code> fornisce sia l’indice <code class="docutils literal notranslate"><span class="pre">i</span></code> che il valore <code class="docutils literal notranslate"><span class="pre">beta</span></code> dall’array <code class="docutils literal notranslate"><span class="pre">beta_values</span></code>. Ad ogni iterazione, vengono calcolate le probabilità usando la funzione <code class="docutils literal notranslate"><span class="pre">softmax</span></code> per le opzioni specificate nell’array <code class="docutils literal notranslate"><span class="pre">v</span></code> e il valore corrente di <code class="docutils literal notranslate"><span class="pre">beta</span></code>.</p>
<p>La riga <code class="docutils literal notranslate"><span class="pre">probability_array[:,</span> <span class="pre">i]</span></code> assegna le probabilità calcolate alla colonna corrispondente nell’array <code class="docutils literal notranslate"><span class="pre">probability_array</span></code> per il valore corrente di <code class="docutils literal notranslate"><span class="pre">beta</span></code>. La notazione di indicizzazione <code class="docutils literal notranslate"><span class="pre">[:,</span> <span class="pre">i]</span></code> seleziona tutte le righe (opzioni) nella colonna corrente <code class="docutils literal notranslate"><span class="pre">i</span></code> dell’array <code class="docutils literal notranslate"><span class="pre">probability_array</span></code>.</p>
<p>Alla fine di questo ciclo, l’array <code class="docutils literal notranslate"><span class="pre">probability_array</span></code> conterrà i valori delle probabilità calcolate usando la funzione <code class="docutils literal notranslate"><span class="pre">softmax</span></code> per diverse combinazioni di valori di <code class="docutils literal notranslate"><span class="pre">beta</span></code> e opzioni.</p>
<p>In sintesi, il codice inizializza un array per contenere le probabilità e poi utilizza un ciclo per calcolare e memorizzare le probabilità per vari valori di <code class="docutils literal notranslate"><span class="pre">beta</span></code> e opzioni utilizzando la funzione <code class="docutils literal notranslate"><span class="pre">softmax</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">probability_array</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(2, 100)
</pre></div>
</div>
</div>
</div>
<p>Passiamo ora a creare un grafico che esprime la probabilità di scelta in funzione del valore beta.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">option_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Option 1&#39;</span><span class="p">,</span> <span class="s1">&#39;Option 2&#39;</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">option_labels</span><span class="p">)):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">beta_values</span><span class="p">,</span> <span class="n">probability_array</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">option_labels</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Beta&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Probabilità&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Funzione Softmax - Modello Rescorla-Wagner&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2d61e324cd8402acb3391a611f34b590d2c80c71012af0e833b9ef121f0b6ef0.png" src="../_images/2d61e324cd8402acb3391a611f34b590d2c80c71012af0e833b9ef121f0b6ef0.png" />
</div>
</div>
<p>Il grafico evidenzia come le probabilità di scelta variano al variare del parametro β, offrendo un’illustrazione dell’effetto di tale parametro sulla distribuzione di probabilità nel modello di Rescorla-Wagner. Quando β è prossimo a zero, la funzione softmax genera una distribuzione di probabilità uniforme, dando luogo a una scelta completamente casuale (0.5 per ciascuna delle due opzioni). Quando β tende all’infinito, la funzione softmax privilegia sempre l’opzione con il valore più elevato (l’opzione 2 ha un valore di 0.75, mentre l’opzione 1 ha un valore di 0.1), rendendo la scelta deterministica. In altre parole, un valore maggiore di β implica che l’opzione con il valore stimato più elevato (nel nostro caso, l’opzione 2) avrà una probabilità di scelta più elevata, mentre valori inferiori di β permettono una maggiore esplorazione delle opzioni con valori stimati più bassi.</p>
<p>Combinando la regola di apprendimento e la regola decisionale, otteniamo un modello del processo decisionale con due parametri liberi: il tasso di apprendimento α e la temperatura inversa β.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">T</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">mu</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]</span> <span class="c1"># probabilità di ricompensa per la scelta delle due opzioni</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">simulate_RescorlaWagner</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">mu</span><span class="p">):</span>
    <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="n">params</span>

    <span class="c1"># Un array di zeri di lunghezza T; c = &quot;choice&quot; in [0, 1]; r = &quot;reward&quot;.</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">T</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">T</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

    <span class="c1"># Un array multidimensionale di zeri di dimensione 2xT</span>
    <span class="n">V_stored</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="n">T</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>

    <span class="c1"># Inizializza V per t == 0</span>
    <span class="n">V</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="c1"># Salva i valori V per V_{t+1}</span>
        <span class="n">V_stored</span><span class="p">[:,</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">V</span>

        <span class="c1"># Calcola le probabilità di scelta</span>
        <span class="n">p0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">p1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">p0</span>

        <span class="c1"># Viene simulato un comportamento di scelta rumoroso in cui l&#39;opzione 0</span>
        <span class="c1"># è scelta con probabilità p0, mentre l&#39;opzione 1 è scelta con</span>
        <span class="c1"># probabilità 1-p0. </span>
        <span class="c1"># `np.random.random_sample(1)` genera un valore casuale tra 0 e 1. `p0` </span>
        <span class="c1"># rappresenta la probabilità di scegliere l&#39;opzione 0. L&#39;istruzione `if` </span>
        <span class="c1"># verifica se il valore casuale generato è minore di p0. Se lo è, l&#39;agente </span>
        <span class="c1"># sceglie l&#39;opzione 0 (`c[t] = 0`). Se il valore casuale non è minore di p0, </span>
        <span class="c1"># l&#39;agente sceglie l&#39;opzione 1 (`c[t] = 1`).</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random_sample</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">p0</span><span class="p">:</span>
            <span class="n">c</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">c</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
       
        <span class="c1"># Genera la ricompensa sulla base delle probabilità di ricompensa.</span>
        <span class="c1"># Il confronto `np.random.rand() &lt; mu[c[t]]` determina se viene ricevuta una </span>
        <span class="c1"># ricompensa in base alla probabilità definita per l&#39;opzione scelta. Se il valore </span>
        <span class="c1"># casuale generato è minore della probabilità di ricompensa (`mu`), allora viene </span>
        <span class="c1"># ricevuta una ricompensa (`r[t]` viene impostato su `True`); in caso contrario, </span>
        <span class="c1"># non viene ricevuta alcuna ricompensa (`r[t]` viene impostato su `False`).</span>
        <span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">mu</span><span class="p">[</span><span class="n">c</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span>

        <span class="c1"># Aggiorna le aspettative di valore</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">-</span> <span class="n">V</span><span class="p">[</span><span class="n">c</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span>
        <span class="n">V</span><span class="p">[</span><span class="n">c</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span> <span class="o">=</span> <span class="n">V</span><span class="p">[</span><span class="n">c</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">delta</span>

    <span class="k">return</span> <span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">V_stored</span>
</pre></div>
</div>
</div>
</div>
<p>Simuliamo <code class="docutils literal notranslate"><span class="pre">T</span></code> = 100 prove utilizzando il modello generativo dei dati definito in precedenza.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">c3</span><span class="p">,</span> <span class="n">r3</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">simulate_RescorlaWagner</span><span class="p">([</span><span class="mf">.1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">],</span> <span class="n">T</span><span class="o">=</span><span class="n">T</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Rappresentiamo graficamente i risultati ottenuti dalla simulazione.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">),</span> <span class="n">r3</span><span class="p">,</span> <span class="s2">&quot;C2--&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">),</span> <span class="n">c3</span><span class="p">,</span> <span class="s2">&quot;+&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;scelta&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Prove&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feedback</span><span class="se">\n</span><span class="s2"> (1=Ricompensa,</span><span class="se">\n</span><span class="s2"> 0=Nessuna ricompensa)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Apprendimento di Rescorla-Wagner&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/fcc4d935883ac8364430a4f0ce2a9e885f0f66af48a942dbe73ea4aaa0948c6e.png" src="../_images/fcc4d935883ac8364430a4f0ce2a9e885f0f66af48a942dbe73ea4aaa0948c6e.png" />
</div>
</div>
<p>Come possiamo osservare, le scelte per la slot machine che produce meno ricompense diventano meno frequenti nel corso delle prove.</p>
<p>Possiamo anche rappresentare graficamente le aspettative di valore <span class="math notranslate nohighlight">\(V\)</span> delle due slot machine nel corso delle prove.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">),</span> <span class="n">V</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="s2">&quot;C0--&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;80</span><span class="si">% s</span><span class="s2">lot machine&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">),</span> <span class="n">V</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="s2">&quot;C3-&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;20</span><span class="si">% s</span><span class="s2">lot machine&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">),</span> <span class="n">c3</span><span class="p">,</span> <span class="s2">&quot;C0+&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;choice&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;trials&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;value&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Rescorla-Wagner Learning&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/a54bc20a2e749d7990b9aa760c6f36c43a0f4c7ba8cb453a8e583fd870ab628c.png" src="../_images/a54bc20a2e749d7990b9aa760c6f36c43a0f4c7ba8cb453a8e583fd870ab628c.png" />
</div>
</div>
<p>Si noti come nel corso delle prove i valori delle slot macchine convergano lentamente verso le probabilità di ricompensa (20% e 80%).</p>
</section>
</section>
<section id="stima-dei-parametri-del-modello">
<h2>Stima dei parametri del modello<a class="headerlink" href="#stima-dei-parametri-del-modello" title="Permalink to this heading">#</a></h2>
<p>Dopo aver esaminato un’implementazione possibile del modello generativo dei dati di Rescorla-Wagner, affrontiamo ora la sfida di stimare i parametri del modello a partire dai dati osservati. Nella modellazione computazionale, una componente fondamentale è la stima dei valori dei parametri che meglio descrivono i dati comportamentali. Sebbene esistano diversi metodi di stima dei parametri, ci concentreremo sull’approccio della <em>Massima Verosimiglianza</em> (si veda l’Appendice di <a class="reference external" href="https://elifesciences.org/articles/49547">Wilson &amp; Collins (2019)</a>).</p>
<p>Nell’approccio della massima verosimiglianza, l’obiettivo è trovare i valori dei parametri di un modello <span class="math notranslate nohighlight">\(m\)</span> che massimizzano la probabilità dei dati <span class="math notranslate nohighlight">\(d_{1:T}\)</span>. Nel caso del modello di Rescorla-Wagner, vogliamo massimizzare la probabilità dei dati <span class="math notranslate nohighlight">\(d_{1:T}\)</span> alla luce dei valori dei parametri <span class="math notranslate nohighlight">\((\alpha, \beta)_m\)</span> del modello <span class="math notranslate nohighlight">\(m\)</span>.</p>
<section id="calcolo-del-logaritmo-negativo-della-verosimiglianza">
<h3>Calcolo del logaritmo negativo della verosimiglianza<a class="headerlink" href="#calcolo-del-logaritmo-negativo-della-verosimiglianza" title="Permalink to this heading">#</a></h3>
<p>Massimizzare la verosimiglianza è equivalente a massimizzare il logaritmo della verosimiglianza, <span class="math notranslate nohighlight">\(\log \mathcal{L} = \log p( d_{1:t-1} | (\alpha, \beta)_m, m)\)</span>, che è più conveniente da gestire numericamente. Il logaritmo della verosimiglianza può essere espresso in termini delle probabilità di ogni singola scelta come</p>
<div class="math notranslate nohighlight">
\[
\log \mathcal{L} = \log p(d_{1:T} | (\alpha, \beta)_m, m) = \sum_{t=1}^T \log p(c_t | d_{1:t-1}, s_t, (\alpha, \beta)_m, m)
\]</div>
<p>dove <span class="math notranslate nohighlight">\(p(c_t | d_{1:t-1}, s_t, (\alpha, \beta)_m, m)\)</span> rappresenta la <em>probabilità di ogni singola scelta</em> dati i parametri del modello e le informazioni disponibili fino a quella scelta (nella notazione precedente, <span class="math notranslate nohighlight">\(d\)</span> sono i feedback e <span class="math notranslate nohighlight">\(s\)</span> è lo stimolo, cioè la slot machine).</p>
<p>Massimizzare il logaritmo della verosimiglianza è equivalente a minimizzare il logaritmo negativo della verosimiglianza. Pertanto, possiamo riscrivere l’equazione precedente come:</p>
<div class="math notranslate nohighlight">
\[
-\log \mathcal{L} = -\sum_{t=1}^T \log p(c_t | d_{1:t-1}, s_t, (\alpha, \beta)_m, m)
\]</div>
<p>Nella pratica, la verosimiglianza è semplicemente una funzione dei dati e dei parametri del modello. Nel caso del modello di Rescorla-Wagner, possiamo definire la funzione log-verosimiglianza negativa come:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">negll_RescorlaWagner</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>
    <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="n">params</span>

    <span class="n">V</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
    <span class="n">T</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
    <span class="n">choiceProb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">T</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="c1"># Calcola le probabilità di scelta per k = 2</span>
        <span class="n">p0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="c1"># &quot;p&quot; è una lista di probabilità di scelta per le due opzioni disponibili, in</span>
        <span class="c1"># cui p[0] rappresenta la probabilità di scegliere l&#39;opzione 1 e p[1] rappresenta</span>
        <span class="c1"># la probabilità di scegliere l&#39;opzione 2.</span>
        <span class="n">p</span> <span class="o">=</span> <span class="p">[</span><span class="n">p0</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">p0</span><span class="p">]</span>

        <span class="c1"># La variabile &quot;c[t]&quot; indica l&#39;opzione effettivamente scelta al tempo &quot;t&quot;, che può</span>
        <span class="c1"># essere 0 o 1. Quindi &quot;p[c[t]]&quot; seleziona l&#39;elemento corrispondente nell&#39;elenco</span>
        <span class="c1"># delle probabilità di scelta. Ad esempio, se &quot;c[t]&quot; è uguale a 0, &quot;p[c[t]]&quot;</span>
        <span class="c1"># restituirà p[0], ovvero la probabilità di scegliere l&#39;opzione 1. Allo stesso</span>
        <span class="c1"># modo, se &quot;c[t]&quot; è uguale a 1, &quot;p[c[t]]&quot; restituirà p[1], ovvero la probabilità</span>
        <span class="c1"># di scegliere l&#39;opzione 2.</span>
        <span class="n">choiceProb</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">p</span><span class="p">[</span><span class="n">c</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span>

        <span class="c1"># Aggiorniamo le aspettative di valore secondo la regola di Rescorla-Wagner.</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">-</span> <span class="n">V</span><span class="p">[</span><span class="n">c</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span>
        <span class="n">V</span><span class="p">[</span><span class="n">c</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span> <span class="o">=</span> <span class="n">V</span><span class="p">[</span><span class="n">c</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">delta</span>

    <span class="c1"># Una volta trovate le probabilità associate a ciascuna scelta, otteniamo il negativo</span>
    <span class="c1"># della log-verosimiglianza</span>
    <span class="n">negLL</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">choiceProb</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">negLL</span>
</pre></div>
</div>
</div>
</div>
<p>Simuliamo ora un set di dati.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># simulate choices from RW Model</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mf">1.5</span>
<span class="n">c4</span><span class="p">,</span> <span class="n">r4</span><span class="p">,</span> <span class="n">V2</span> <span class="o">=</span> <span class="n">simulate_RescorlaWagner</span><span class="p">([</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">],</span> <span class="n">T</span><span class="o">=</span><span class="n">T</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Per fare un esempio, valutiamo la log-verosimiglianza negativa per i dati simulati in corrispondenza dei valori <code class="docutils literal notranslate"><span class="pre">alpha</span></code> e <code class="docutils literal notranslate"><span class="pre">theta</span></code> indicati di seguito.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">alpha_hat</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">beta_hat</span> <span class="o">=</span> <span class="mf">2.5</span>
<span class="n">negLL</span> <span class="o">=</span> <span class="n">negll_RescorlaWagner</span><span class="p">([</span><span class="n">alpha_hat</span><span class="p">,</span> <span class="n">beta_hat</span><span class="p">],</span> <span class="n">c4</span><span class="p">,</span> <span class="n">r4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">alpha_hat</span><span class="p">,</span> <span class="n">beta_hat</span><span class="p">,</span> <span class="n">negLL</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.3 2.5 68.52600058524918
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">alpha_hat</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">beta_hat</span> <span class="o">=</span> <span class="mf">1.5</span>
<span class="n">negLL</span> <span class="o">=</span> <span class="n">negll_RescorlaWagner</span><span class="p">([</span><span class="n">alpha_hat</span><span class="p">,</span> <span class="n">beta_hat</span><span class="p">],</span> <span class="n">c4</span><span class="p">,</span> <span class="n">r4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">alpha_hat</span><span class="p">,</span> <span class="n">beta_hat</span><span class="p">,</span> <span class="n">negLL</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.2 1.5 62.90586040224763
</pre></div>
</div>
</div>
</div>
<p>Un metodo semplice per determinare i parametri attraverso la massima verosimiglianza prevede una ricerca esaustiva all’interno dell’intero spazio dei parametri. Questo significa selezionare i valori di <code class="docutils literal notranslate"><span class="pre">alpha</span></code> e <code class="docutils literal notranslate"><span class="pre">beta</span></code> che minimizzano il valore della funzione <code class="docutils literal notranslate"><span class="pre">negLL</span></code>.</p>
<p>Applichiamo questo approccio al set di dati simulato. Per semplicità, supponiamo di conoscere il valore di <span class="math notranslate nohighlight">\(\beta\)</span>, ma di non avere informazioni sul valore di <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nLL</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">alpha_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="k">for</span> <span class="n">alpha_val</span> <span class="ow">in</span> <span class="n">alpha_vals</span><span class="p">:</span>
    <span class="n">nLL</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">negll_RescorlaWagner</span><span class="p">([</span><span class="n">alpha_val</span><span class="p">,</span> <span class="n">beta</span><span class="p">],</span> <span class="n">c4</span><span class="p">,</span> <span class="n">r4</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alpha_vals</span><span class="p">,</span> <span class="n">nLL</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">alpha_vals</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">nLL</span><span class="p">)],</span> <span class="n">nLL</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">nLL</span><span class="p">)],</span> <span class="s2">&quot;X&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;optimal $\hat \alpha$&quot;</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;negative log likelihood&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">rf</span><span class="s2">&quot;learning rate, $\hat \alpha$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Rescorla-Wagner Learning&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/558c5aaa3bdba35618c0f6f318c18bf6b347af72e4c6fb4193f154f317d0af1c.png" src="../_images/558c5aaa3bdba35618c0f6f318c18bf6b347af72e4c6fb4193f154f317d0af1c.png" />
</div>
</div>
</section>
</section>
<section id="validazione">
<h2>Validazione<a class="headerlink" href="#validazione" title="Permalink to this heading">#</a></h2>
<p>Una volta trovato un metodo per stimare i parametri del modello a partire dai dati ci dobbiamo chiedere quale sia il tipo di corrispondenza che ci possiamo aspettare tra le stime ottenute e i veri valori dei parametri del modello. Per rispondere a questa domanda è possibile svolgere uno studio di simulazione.</p>
<p>I parametri della simulazione sono i seguenti.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">T</span> <span class="o">=</span> <span class="mi">250</span>
<span class="n">mu</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]</span>
<span class="n">NSUBJ</span> <span class="o">=</span> <span class="mi">500</span>
</pre></div>
</div>
</div>
</div>
<p>Riscriviamo qui la funzione per il calcolo della log-verosimiglianza.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">negll_RescorlaWagner</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">choices</span><span class="p">,</span> <span class="n">outcomes</span><span class="p">):</span>
    <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="n">params</span>

    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">beta</span><span class="p">):</span>  <span class="c1"># check inputs</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">c</span><span class="p">,</span> <span class="n">r</span> <span class="o">=</span> <span class="n">choices</span><span class="p">,</span> <span class="n">outcomes</span>

        <span class="n">T</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>  <span class="c1"># Q at trial 0</span>
        <span class="n">V_stored</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="n">T</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
        <span class="n">choiceProb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">T</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>  <span class="c1"># add comment</span>
            <span class="c1"># compute choice probabilities for k=2</span>
            <span class="c1"># use the softmax rule</span>
            <span class="n">ev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">V</span><span class="p">))</span>
            <span class="n">sum_ev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ev</span><span class="p">)</span>
            <span class="n">p</span> <span class="o">=</span> <span class="n">ev</span> <span class="o">/</span> <span class="n">sum_ev</span>

            <span class="c1"># compute choice probability for actual choice</span>
            <span class="n">choiceProb</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">p</span><span class="p">[</span><span class="n">c</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span>

            <span class="c1"># update values</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">-</span> <span class="n">V</span><span class="p">[</span><span class="n">c</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span>
            <span class="n">V</span><span class="p">[</span><span class="n">c</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span> <span class="o">=</span> <span class="n">V</span><span class="p">[</span><span class="n">c</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">delta</span>

            <span class="c1"># store Q_t+1</span>
            <span class="n">V_stored</span><span class="p">[:,</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">V</span>

        <span class="n">negLL</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">choiceProb</span><span class="p">))</span>  <span class="c1"># add comment</span>

        <span class="k">return</span> <span class="n">negLL</span>
</pre></div>
</div>
</div>
</div>
<p>Calcolimo i valori di massima verosimiglianza dei parametri <code class="docutils literal notranslate"><span class="pre">alpha</span></code> e <code class="docutils literal notranslate"><span class="pre">theta</span></code> usando la funzione <code class="docutils literal notranslate"><span class="pre">minimize</span></code> per minimizzare la funzione di log-verosimiglianza. Simuliamo i dati di un soggetto.</p>
<p>Specifichiamo poi le stime iniziali per i valori dei parametri e i valori margine delle possibili soluzioni. I risultati saranno salvati nell’oggetto <code class="docutils literal notranslate"><span class="pre">result</span></code>. Le stime dei due parametri si estraggono con <code class="docutils literal notranslate"><span class="pre">result.x</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">simulate_RescorlaWagner</span><span class="p">([</span><span class="mf">0.15</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">T</span><span class="o">=</span><span class="n">T</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">)</span>

<span class="n">init_guess</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># minimize neg LL</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span>
    <span class="n">negll_RescorlaWagner</span><span class="p">,</span>
    <span class="n">init_guess</span><span class="p">,</span>
    <span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">),</span>
    <span class="n">bounds</span><span class="o">=</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)),</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.05155418 1.5622571 ]
</pre></div>
</div>
</div>
</div>
<p>Simuliamo i dati per 500 soggetti, con 250 osservazioni ciascuno, utilizzando valori casuali di <code class="docutils literal notranslate"><span class="pre">alpha</span></code> e <code class="docutils literal notranslate"><span class="pre">theta</span></code>. Successivamente, eseguiamo la stima di massima verosimiglianza per i dati di ogni soggetto, inizializzando casualmente i parametri per ciascuno di essi. Infine, salviamo i risultati ottenuti nel DataFrame <code class="docutils literal notranslate"><span class="pre">df</span></code>. Ecco il codice corrispondente:</p>
<div class="cell tag_output-hide docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">index</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">NSUBJ</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;true_alpha&quot;</span><span class="p">,</span> <span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="s2">&quot;true_beta&quot;</span><span class="p">,</span> <span class="s2">&quot;beta&quot;</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># loop through subjects</span>
<span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NSUBJ</span><span class="p">):</span>
    <span class="n">true_alpha</span> <span class="o">=</span> <span class="mf">0.95</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
    <span class="n">true_beta</span> <span class="o">=</span> <span class="mf">4.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>

    <span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">simulate_RescorlaWagner</span><span class="p">([</span><span class="n">true_alpha</span><span class="p">,</span> <span class="n">true_beta</span><span class="p">],</span> <span class="n">T</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">)</span>

    <span class="n">init_guess</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(),</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">())</span>
    <span class="c1"># minimize neg LL</span>
    <span class="n">param_fits</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span>
        <span class="n">negll_RescorlaWagner</span><span class="p">,</span>
        <span class="n">init_guess</span><span class="p">,</span>
        <span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">),</span>
        <span class="n">bounds</span><span class="o">=</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)),</span>
    <span class="p">)</span>

    <span class="c1"># store in dataframe</span>
    <span class="n">df</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">index</span><span class="p">,</span> <span class="s2">&quot;true_alpha&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">true_alpha</span>
    <span class="n">df</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">index</span><span class="p">,</span> <span class="s2">&quot;true_beta&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">true_beta</span>
    <span class="n">df</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">index</span><span class="p">,</span> <span class="s2">&quot;alpha&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_fits</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">df</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">index</span><span class="p">,</span> <span class="s2">&quot;beta&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_fits</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>La figura successiva mostra una corrispondenza tra i valori stimati di alpha e i valori veri. Tuttavia, è importante notare che la corrispondenza non è perfetta a causa della presenza di una componente di casualità nei dati. Inoltre, in alcuni casi si possono osservare valori stimati di alpha pari a 0 o 1, che corrispondono a risultati spurii dell’algoritmo. Il numero di risultati spurii aumenta con il diminuire del numero di osservazioni per ciascun soggetto.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">true_alpha</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span> <span class="s2">&quot;C0o&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;True alpha&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Estimated alpha&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ML estimation&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dfaf0f30d2bb8656eb8f00a78a8cab33d64ad25a2f265d4484f663423ffe6811.png" src="../_images/dfaf0f30d2bb8656eb8f00a78a8cab33d64ad25a2f265d4484f663423ffe6811.png" />
</div>
</div>
<p>Un discorso analogo si può fare per theta, anche se in questo caso vi è una migliore corrispondenza tra i valori stimati e i valori veri.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">true_beta</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span> <span class="s2">&quot;C0o&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;True beta&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Estimated beta&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ML estimation&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/cb8a32d729b62d4ee23c9106168f6e9fa58f66cb59eb01caaecf09aa1d5de688.png" src="../_images/cb8a32d729b62d4ee23c9106168f6e9fa58f66cb59eb01caaecf09aa1d5de688.png" />
</div>
</div>
<p>In sintesi, possiamo affermare che il metodo di massima verosimiglianza è in grado di recuperare i valori simulati dei parametri <code class="docutils literal notranslate"><span class="pre">alpha</span></code> e <code class="docutils literal notranslate"><span class="pre">beta</span></code> del modello Rescorla-Wagner, ma solo quando il numero di osservazioni per soggetto è considerevole. Tuttavia, è importante sottolineare che questo metodo può produrre risultati erronei in specifiche circostanze.</p>
<p>Esistono altri metodi di stima che offrono risultati migliori anche con un numero inferiore di osservazioni per soggetto. Tra questi, il metodo gerarchico bayesiano è ampiamente utilizzato nella pratica. Va però precisato che l’obiettivo di questo tutorial era principalmente illustrare in modo semplice come sia possibile ottenere con buona accuratezza i parametri del modello Rescorla-Wagner dai dati generati da una simulazione, considerando condizioni ottimali in cui i valori dei parametri del modello sono noti.</p>
</section>
<section id="commenti-e-considerazioni-conclusive">
<h2>Commenti e considerazioni conclusive<a class="headerlink" href="#commenti-e-considerazioni-conclusive" title="Permalink to this heading">#</a></h2>
<p>È fondamentale sottolineare che, nella pratica, la stima dei parametri è un processo complesso e che l’accuratezza delle stime dipende da vari fattori, come la dimensione del campione e la natura dei dati osservati. Pertanto, è sempre consigliabile valutare attentamente i risultati e considerare l’utilizzo di approcci più sofisticati, come il metodo gerarchico bayesiano, per ottenere stime dei parametri del modello più affidabili.</p>
</section>
<section id="watermark">
<h2>Watermark<a class="headerlink" href="#watermark" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">load_ext</span> watermark
<span class="o">%</span><span class="k">watermark</span> -n -u -v -iv -w
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Last updated: Thu Nov 09 2023

Python implementation: CPython
Python version       : 3.11.6
IPython version      : 8.16.1

pandas    : 2.1.1
matplotlib: 3.8.0
arviz     : 0.16.1
numpy     : 1.25.2
seaborn   : 0.13.0

Watermark: 2.4.3
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_3"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="09_likelihood.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">La verosimiglianza</p>
      </div>
    </a>
    <a class="right-next"
       href="E_likelihood.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">✏️ Esercizi</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#teoria-il-modello-rescorla-wagner">Teoria: il modello Rescorla-Wagner</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simulare-l-apprendimento">Simulare l’apprendimento</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#la-regola-di-apprendimento-per-rinforzo-delta-rule">La regola di apprendimento per rinforzo (<span class="math notranslate nohighlight">\(\delta\)</span>-rule)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax">Softmax</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stima-dei-parametri-del-modello">Stima dei parametri del modello</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calcolo-del-logaritmo-negativo-della-verosimiglianza">Calcolo del logaritmo negativo della verosimiglianza</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#validazione">Validazione</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#commenti-e-considerazioni-conclusive">Commenti e considerazioni conclusive</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#watermark">Watermark</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Corrado Caudek
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>