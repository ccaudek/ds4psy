

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Modellazione bayesiana &#8212; ds4p</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="../_static/a11y.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/custom.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-VMXNE4BCDL"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-VMXNE4BCDL');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_4/01_intro_bayes';</script>
    <link rel="canonical" href="https://ccaudek.github.io/ds4psy/chapter_4/01_intro_bayes.html" />
    <link rel="shortcut icon" href="../_static/increasing.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Pensare ad una proporzione in termini soggettivi" href="02_subj_prop.html" />
    <link rel="prev" title="Inferenza bayesiana" href="introduction_part_4.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="ds4p - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="ds4p - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Benvenuti
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_1/introduction_chapter_1.html">Python</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/01_python_1.html">Python (1)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/02_python_2.html">Python (2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_python.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/03_numpy.html">NumPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_numpy.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/04_pandas.html">Pandas (1)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/05_pandas_aggregate.html">Pandas (2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/06_pandas_functions.html">Pandas (3)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_pandas.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/07_matplotlib.html">Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/08_seaborn.html">Seaborn</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_matplotlib.html">✏️ Esercizi</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_2/introduction_chapter_2.html">Statistica descrittiva</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/01_key_notions.html">Concetti chiave</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_key_notions.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/02_measurement.html">La misurazione in psicologia</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_scales.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/03_freq_distr.html">Dati e frequenze</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_sums.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/04_loc_scale.html">Indici di posizione e di scala</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/05_correlation.html">Le relazioni tra variabili</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/06_causality.html">Correlazione e causazione</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/07_crisis.html">La crisi della generalizzabilità</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_eda.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_mehr_song_spelke.html">✏️ Esercizi</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_3/introduction_chapter_3.html">Probabilità</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/01_intro_prob.html">Introduzione al calcolo delle probabilità</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_prob.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/02_conditional_prob.html">Probabilità condizionata</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_cond_prob.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/03_bayes_theorem.html">Il teorema di Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_bayes_theorem.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/04_expval_var.html">Variabili casuali</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/04a_sampling_distr.html">Stime, stimatori e parametri</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/04b_illusion.html">Incertezza inferenziale e variabilità dei risultati</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_rv_discrete.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/05_joint_prob.html">Probabilità congiunta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_joint_prob.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/06_density_func.html">La funzione di densità di probabilità</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/07_discr_rv_distr.html">Distribuzioni di v.c. discrete</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_binomial.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/08_cont_rv_distr.html">Distribuzioni di v.c. continue</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_gaussian.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_beta_distr.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/09_likelihood.html">La verosimiglianza</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/10_rescorla_wagner.html">Apprendimento per rinforzo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_likelihood.html">✏️ Esercizi</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="introduction_part_4.html">Inferenza bayesiana</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Modellazione bayesiana</a></li>
<li class="toctree-l2"><a class="reference internal" href="02_subj_prop.html">Pensare ad una proporzione in termini soggettivi</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_conjugate_families_1.html">Distribuzioni coniugate (1)</a></li>
<li class="toctree-l2"><a class="reference internal" href="04_conjugate_families_2.html">Distribuzioni coniugate (2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_summary_posterior.html">Sintesi a posteriori</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_conjugate.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="06_balance-prior-post.html">L’influenza della distribuzione a priori</a></li>
<li class="toctree-l2"><a class="reference internal" href="10_metropolis.html">Monte Carlo a Catena di Markov</a></li>
<li class="toctree-l2"><a class="reference internal" href="11_beta_binomial_pymc.html">Inferenza bayesiana con PyMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="12_jax.html">Usare JAX per un campionamento più veloce</a></li>
<li class="toctree-l2"><a class="reference internal" href="13_preliz.html">Scegliere le distribuzioni a priori</a></li>
<li class="toctree-l2"><a class="reference internal" href="16_summary_posterior_pymc.html">Metodi di sintesi della distribuzione a posteriori</a></li>
<li class="toctree-l2"><a class="reference internal" href="17_prediction.html">La predizione bayesiana</a></li>
<li class="toctree-l2"><a class="reference internal" href="18_mcmc_diagnostics.html">Diagnostica delle catene markoviane</a></li>
<li class="toctree-l2"><a class="reference internal" href="19_odds_ratio.html">Analisi bayesiana dell’odds-ratio</a></li>
<li class="toctree-l2"><a class="reference internal" href="20_poisson_model.html">Modello di Poisson</a></li>
<li class="toctree-l2"><a class="reference internal" href="21_poisson_sim.html">Modello di Poisson: derivazione analitica e MCMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_freq.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="22_normal_normal_model.html">Inferenza bayesiana su una media</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_one_mean.html">✏️ Esercizio</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_one_mean_2.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="23_two_groups.html">Confronto tra due gruppi</a></li>
<li class="toctree-l2"><a class="reference internal" href="24_multiple_groups.html">Gruppi multipli</a></li>
<li class="toctree-l2"><a class="reference internal" href="30_entropy.html">Entropia</a></li>
<li class="toctree-l2"><a class="reference internal" href="31_kl.html">La divergenza di Kullback-Leibler</a></li>
<li class="toctree-l2"><a class="reference internal" href="40_hier_beta_binom.html">Modello gerarchico beta-binomiale</a></li>
<li class="toctree-l2"><a class="reference internal" href="41_hier_poisson.html">Modello gerarchico di Poisson</a></li>
<li class="toctree-l2"><a class="reference internal" href="42_hier_gaussian.html">Modello gerarchico gaussiano</a></li>
<li class="toctree-l2"><a class="reference internal" href="hssm.html">Drift Diffusion Model</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_5/introduction_part_5.html">Analisi della regressione</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_1.html">Il modello di regressione lineare</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_2.html">Analisi bayesiana del modello di regressione lineare</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_params_recovery.html">Analisi di simulazione per la stima dei parametri nel modello di regressione</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_3.html">Zucchero sintattico</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_4.html">Confronto tra le medie di due gruppi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_5.html">Il modello lineare gerarchico</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_multilevel_modeling.html">A Primer on Bayesian Methods for Multilevel Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_7.html">Regressione robusta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_simpson.html">Paradosso di Simpson</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_reglin_1.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_reglin_2.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_reglin_3.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_reglin_4.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_logistic_reg.html">Modello di regressione logistica</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_binomial_reg.html">Regressione binomiale</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_covid.html">Inferenza controfattuale: calcolo delle morti in eccesso dovute al COVID-19</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_counterfactual.html">Analisi causale con PyMC</a></li>

<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_stab.html">✏️ Esercizi</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_6/introduction_part_6.html">Inferenza frequentista</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_estimation.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/02_conf_interv.html">Intervallo di confidenza</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/03_test_ipotesi.html">Significatività statistica</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_interpretation_test.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_significato_test.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/04_two_ind_samples.html">Test t di Student per campioni indipendenti</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_test_media_pop.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_medie_pop_ampie.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_medie_pop_piccoli.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_campioni_appaiati.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_confronto_proporzioni.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/05_limiti_stat_frequentista.html">Limiti dell’inferenza frequentista</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/06_s_m_errors.html">Crisi della replicabilità</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../references/bibliography.html">Bibliografia</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_7/introduction_appendix.html">Appendici</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/00_installation.html">Ambiente di lavoro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a01_math_symbols.html">Simbologia di base</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a02_numbers.html">Numeri binari, interi, razionali, irrazionali e reali</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a04_summation_notation.html">Simbolo di somma (sommatorie)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a05_sets.html">Insiemi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a06_combinatorics.html">Calcolo combinatorio</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a07_calculus.html">Per liberarvi dai terrori preliminari</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a08_kde_plot.html">Kernel Density Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a09_prob_tutorial.html">Esercizi di probabilità discreta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a12_rng.html">Generazione di numeri casuali</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a14_predict_counts.html">La predizione delle frequenze</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a15_lin_fun.html">La funzione lineare</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a20_reglin_1.html">Regressione lineare bivariata</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a21_reglin_2.html">Regressione lineare con Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a23_reglin_4.html">Posterior Predictive Checks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a30_ttest_exercises.html">Esercizi sull’inferenza frequentista</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/ccaudek/ds4psy/blob/main/docs/chapter_4/01_intro_bayes.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapter_4/01_intro_bayes.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Modellazione bayesiana</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#esplorazione-dei-fondamenti-dei-metodi-bayesiani-in-psicologia">Esplorazione dei Fondamenti dei Metodi Bayesiani in Psicologia</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#il-ciclo-continuo-dell-apprendimento-bayesiano">Il Ciclo Continuo dell’Apprendimento Bayesiano</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dalla-generazione-dei-dati-alla-modellizzazione-bayesiana">Dalla Generazione dei Dati alla Modellizzazione Bayesiana</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#riallocazione-della-credibilita-e-aggiornamento-sequenziale-delle-informazioni">Riallocazione della Credibilità e Aggiornamento Sequenziale delle Informazioni</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#la-modellizzazione-statistica-un-ponte-tra-teoria-e-dati">La Modellizzazione Statistica: Un Ponte tra Teoria e Dati</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#caratteristiche-fondamentali-dei-modelli-bayesiani">Caratteristiche Fondamentali dei Modelli Bayesiani</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#il-processo-di-sviluppo-di-un-modello-bayesiano">Il Processo di Sviluppo di un Modello Bayesiano</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#riesame-del-teorema-di-bayes">Riesame del Teorema di Bayes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#l-aggiornamento-bayesiano">L’aggiornamento bayesiano</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linguaggi-di-programmazione-probabilistici">Linguaggi di programmazione probabilistici</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notazione">Notazione</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#la-distribuzione-a-priori">La Distribuzione a Priori</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distribuzioni-a-priori-non-informativa">Distribuzioni a Priori Non Informativa</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distribuzioni-a-priori-debolmente-informativa">Distribuzioni a Priori Debolmente Informativa</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distribuzioni-a-priori-informativa">Distribuzioni a Priori Informativa</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#la-verosimiglianza-marginale">La verosimiglianza marginale</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metodi-di-stima-della-distribuzione-a-posteriori">Metodi di stima della distribuzione a posteriori</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#il-flusso-di-lavoro-bayesiano">Il flusso di lavoro bayesiano</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fasi-del-flusso-di-lavoro">Fasi del flusso di lavoro</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#commenti-e-considerazioni-finali">Commenti e considerazioni finali</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="modellazione-bayesiana">
<span id="bayes-workflow-notebook"></span><h1>Modellazione bayesiana<a class="headerlink" href="#modellazione-bayesiana" title="Permalink to this heading">#</a></h1>
<p>L’obiettivo di questo Capitolo è di introdurre il quadro concettuale dela modellizzazione bayesiana.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">uniform</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;retina&#39;
<span class="n">RANDOM_SEED</span> <span class="o">=</span> <span class="mi">8927</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">RANDOM_SEED</span><span class="p">)</span>
<span class="n">az</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;arviz-darkgrid&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<section id="esplorazione-dei-fondamenti-dei-metodi-bayesiani-in-psicologia">
<h2>Esplorazione dei Fondamenti dei Metodi Bayesiani in Psicologia<a class="headerlink" href="#esplorazione-dei-fondamenti-dei-metodi-bayesiani-in-psicologia" title="Permalink to this heading">#</a></h2>
<p>Spesso ci affidiamo alle probabilità in modo informale per esprimere le nostre conoscenze e convinzioni riguardo a quantità sconosciute. Tuttavia, è possibile formalizzare l’uso delle probabilità per rappresentare queste informazioni: dal punto di vista matematico, si può dimostrare che le probabilità possono rappresentare numericamente un insieme di convinzioni razionali. Inoltre, esiste una connessione tra probabilità e informazione, e la regola di Bayes fornisce un metodo logico per aggiornare le nostre convinzioni in base a nuove informazioni. Questo processo di apprendimento induttivo attraverso la regola di Bayes è noto come <em>inferenza bayesiana</em>.</p>
<p>Negli ultimi anni, il campo della psicologia ha assistito a un crescente interesse e impiego dei metodi bayesiani. Questo cambiamento paradigmatico ha facilitato l’integrazione più ampia dei modelli bayesiani nell’analisi e interpretazione dei dati psicologici. La transizione è stata supportata da una vasta gamma di risorse educative, tra cui pubblicazioni e manuali specificatamente orientati agli psicologi, , come dimostrato dalle opere di <span id="id1">Brooks [<a class="reference internal" href="../references/bibliography.html#id27" title="Stephen P Brooks. Bayesian computation: a statistical revolution. Philosophical Transactions of the Royal Society of London. Series A: Mathematical, Physical and Engineering Sciences, 361(1813):2681–2697, 2003.">Bro03</a>]</span>, <span id="id2">Van De Schoot <em>et al.</em> [<a class="reference internal" href="../references/bibliography.html#id26" title="Rens Van De Schoot, Sonja D Winter, Oisín Ryan, Mariëlle Zondervan-Zwijnenburg, and Sarah Depaoli. A systematic review of bayesian articles in psychology: the last 25 years. Psychological Methods, 22(2):217–239, 2017.">VDSWR+17</a>]</span>, <span id="id3">Kruschke [<a class="reference internal" href="../references/bibliography.html#id134" title="John Kruschke. Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press, 2014.">Kru14</a>]</span>, <span id="id4">McElreath [<a class="reference internal" href="../references/bibliography.html#id107" title="Richard McElreath. Statistical rethinking: A Bayesian course with examples in R and Stan. CRC Press, Boca Raton, Florida, 2nd edition edition, 2020.">McE20</a>]</span>, <span id="id5">Johnson <em>et al.</em> [<a class="reference internal" href="../references/bibliography.html#id86" title="Alicia A. Johnson, Miles Ott, and Mine Dogucu. Bayes Rules! An Introduction to Bayesian Modeling with R. CRC Press, 2022.">JOD22</a>]</span>), e altri autori rilevanti nel settore.</p>
<section id="il-ciclo-continuo-dell-apprendimento-bayesiano">
<h3>Il Ciclo Continuo dell’Apprendimento Bayesiano<a class="headerlink" href="#il-ciclo-continuo-dell-apprendimento-bayesiano" title="Permalink to this heading">#</a></h3>
<p>Il framework bayesiano può essere visto come un modello matematico del processo di apprendimento continuo, un fenomeno intrinseco a tutti gli esseri viventi. Questo ciclo iterativo di aggiornamento delle conoscenze si basa sul teorema di Bayes. In questo contesto, ogni nuovo set di dati osservati serve a raffinare la nostra comprensione o le nostre ipotesi su un determinato fenomeno. La distribuzione a posteriori ottenuta da un ciclo di apprendimento diventa la distribuzione a priori per il ciclo successivo, consentendo un aggiornamento sequenziale e indefinito delle informazioni, come enfatizzato da <span id="id6">[<a class="reference internal" href="../references/bibliography.html#id16" title="Eric-Jan Wagenmakers, Gilles Dutilh, and Alexandra Sarafoglou. The creativity-verification cycle in psychological science: new methods to combat old idols. Perspectives on Psychological Science, 13(4):418–427, 2018.">WDS18</a>]</span>.</p>
</section>
<section id="dalla-generazione-dei-dati-alla-modellizzazione-bayesiana">
<h3>Dalla Generazione dei Dati alla Modellizzazione Bayesiana<a class="headerlink" href="#dalla-generazione-dei-dati-alla-modellizzazione-bayesiana" title="Permalink to this heading">#</a></h3>
<p>Un aspetto fondamentale dell’approccio bayesiano è la sua capacità di svelare i processi generativi che stanno alla base dei dati osservat, noti come Data-Generating Processes (DGP). In sostanza, l’inferenza statistica bayesiana si propone di identificare i meccanismi sottostanti, non direttamente osservabili, che generano i dati raccolti. Questi processi possono essere rappresentati attraverso modelli statistici che incarnano specifiche ipotesi sulla realtà. Per esempio, se l’ipotesi è che “le donne siano migliori degli uomini a scacchi”, un modello statistico pertinente potrebbe prevedere che la media dei punteggi Elo delle donne sia superiore a quella degli uomini, una volta controllate le differenze di base nella frequenza di partecipazione.</p>
</section>
<section id="riallocazione-della-credibilita-e-aggiornamento-sequenziale-delle-informazioni">
<h3>Riallocazione della Credibilità e Aggiornamento Sequenziale delle Informazioni<a class="headerlink" href="#riallocazione-della-credibilita-e-aggiornamento-sequenziale-delle-informazioni" title="Permalink to this heading">#</a></h3>
<p>Un esempio intuitivo per illustrare la riallocazione della credibilità e l’aggiornamento sequenziale delle informazioni nel contesto bayesiano è l’approccio investigativo di Sherlock Holmes. <span id="id7">Kruschke [<a class="reference internal" href="../references/bibliography.html#id134" title="John Kruschke. Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press, 2014.">Kru14</a>]</span> utilizza l’iconico detective per mostrare come, in un’indagine su un crimine con quattro possibili cause (A, B, C e D), l’acquisizione di nuove prove possa portare all’esclusione di una di esse, ridistribuendo così la probabilità tra le rimanenti. In pratica, se Holmes raccoglie dati sufficienti per escludere la causa A, la probabilità associata a essa viene annullata e riallocata tra le cause B, C e D, rendendole tutte egualmente probabili come cause reali del crimine. Tuttavia, nella ricerca psicologica, dove i dati sono spesso meno definitivi, raramente un’ipotesi viene completamente esclusa; piuttosto, la sua credibilità può essere ridimensionata, aumentando la plausibilità delle alternative.</p>
<p>Nell’ambito statistico, le ipotesi in esame sono rappresentate da valori parametrici nei modelli matematici. Il framework bayesiano permette di esplorare un ampio spettro di scenari plausibili, tra i quali la credibilità viene costantemente riallocata. Questo aggiornamento continuo delle informazioni, come enfatizzato da <span id="id8">Kruschke [<a class="reference internal" href="../references/bibliography.html#id134" title="John Kruschke. Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press, 2014.">Kru14</a>]</span>, sottolinea il principio che “il posterior di oggi diventa il prior di domani” (Dennis Lindley).</p>
</section>
<section id="la-modellizzazione-statistica-un-ponte-tra-teoria-e-dati">
<h3>La Modellizzazione Statistica: Un Ponte tra Teoria e Dati<a class="headerlink" href="#la-modellizzazione-statistica-un-ponte-tra-teoria-e-dati" title="Permalink to this heading">#</a></h3>
<p>La modellizzazione statistica bayesiana può essere vista come un collegamento cruciale tra la teoria psicologica e i dati empirici. Essa agisce come uno strumento fondamentale per scomporre la complessità dei fenomeni psicologici in elementi più semplici e maneggevoli. Questo processo di scomposizione non solo facilita una migliore comprensione, ma permette anche la simulazione dei comportamenti o processi in esame. Per realizzare con successo questa modellizzazione, è necessario un’attenta sinergia tra la conoscenza specifica del settore e le competenze statistiche, al fine di convertire efficacemente teorie e dati empirici in modelli computazionali funzionali.</p>
<p>Nel corso di questo e dei capitoli successivi, esploreremo come la modellizzazione statistica si avvalga di due elementi chiave:</p>
<ul class="simple">
<li><p><strong>Variabili Casuali</strong>: Queste sono impiegate per rappresentare le quantità incognite e per modellare le relazioni tra le diverse variabili che costituiscono il fenomeno di interesse.</p></li>
<li><p><strong>Distribuzioni Statistiche</strong>: Servono come strumenti matematici per strutturare e quantificare l’incertezza associata ai parametri del modello, fornendo così una base solida per l’inferenza.</p></li>
</ul>
</section>
<section id="caratteristiche-fondamentali-dei-modelli-bayesiani">
<h3>Caratteristiche Fondamentali dei Modelli Bayesiani<a class="headerlink" href="#caratteristiche-fondamentali-dei-modelli-bayesiani" title="Permalink to this heading">#</a></h3>
<p>La modellizzazione statistica bayesiana si distingue per due aspetti cruciali:</p>
<ul class="simple">
<li><p><strong>Distribuzioni Priori</strong>: Queste rappresentano le credenze iniziali riguardanti le quantità non note e sono utilizzate per definire i parametri del modello. Le distribuzioni priori riflettono le informazioni o le convinzioni preesistenti prima dell’analisi dei dati.</p></li>
<li><p><strong>Aggiornamento Bayesiano</strong>: Questo è il processo attraverso cui le distribuzioni priori vengono aggiornate utilizzando il teorema di Bayes in risposta ai dati osservati. L’aggiornamento bayesiano permette di affinare e ridurre l’incertezza sui parametri del modello.</p></li>
</ul>
</section>
<section id="il-processo-di-sviluppo-di-un-modello-bayesiano">
<h3>Il Processo di Sviluppo di un Modello Bayesiano<a class="headerlink" href="#il-processo-di-sviluppo-di-un-modello-bayesiano" title="Permalink to this heading">#</a></h3>
<p>La costruzione di un modello bayesiano si articola in tre fasi principali:</p>
<ol class="arabic simple">
<li><p><strong>Progettazione del Modello</strong>: In questa fase iniziale, il modello viene progettato sulla base delle conoscenze preliminari e delle ipotesi relative al meccanismo generativo dei dati.</p></li>
<li><p><strong>Esecuzione dell’Inferenza Bayesiana</strong>: In questo stadio, si applica il teorema di Bayes per rivedere e aggiornare le credenze priori alla luce dei dati osservati, portando alla formulazione di una distribuzione a posteriori.</p></li>
<li><p><strong>Analisi e Confronto dei Modelli</strong>: In questa fase finale, si valuta l’adeguatezza del modello, esaminando la sua coerenza e affidabilità attraverso diversi criteri. Se necessario, si confronta il modello con altre alternative per determinare quale sia il più appropriato.</p></li>
</ol>
<p>Nei successivi capitoli, approfondiremo ciascuna di queste fasi, delineando il processo standard adottato nel flusso di lavoro bayesiano <span id="id9">[<a class="reference internal" href="../references/bibliography.html#id30" title="Beth Baribault and Anne GE Collins. Troubleshooting bayesian cognitive models. Psychological Methods, 2023.">BC23</a>]</span>.</p>
</section>
</section>
<section id="riesame-del-teorema-di-bayes">
<h2>Riesame del Teorema di Bayes<a class="headerlink" href="#riesame-del-teorema-di-bayes" title="Permalink to this heading">#</a></h2>
<p>Per cominciare, denotiamo con <span class="math notranslate nohighlight">\( Y \)</span> una variabile casuale che assume un valore realizzato <span class="math notranslate nohighlight">\( y \)</span>. Ad esempio, il punteggio di uno studente in un esame di Psicometria potrebbe essere considerato una variabile casuale <span class="math notranslate nohighlight">\( Y \)</span> che assume un ampio insieme di valori possibili. Una volta che lo studente riceve un voto nell’esame di Psicometria, la variabile casuale <span class="math notranslate nohighlight">\( Y \)</span> è ora realizzata come <span class="math notranslate nohighlight">\( y \)</span>. Poiché <span class="math notranslate nohighlight">\( Y \)</span> è una variabile casuale non osservata, dobbiamo specificare un modello di probabilità per spiegare come abbiamo ottenuto i valori effettivi dei dati <span class="math notranslate nohighlight">\( y \)</span>. Ci riferiamo a questo modello come il processo generatore di dati (o <em>Data Generating Process</em>, DGP).</p>
<p>Successivamente, denotiamo con <span class="math notranslate nohighlight">\( \theta \)</span> un parametro che riteniamo caratterizzi il modello di probabilità di interesse. Il parametro <span class="math notranslate nohighlight">\( \theta \)</span> può essere uno scalare, come la media o la varianza di una distribuzione, o può essere un vettore, come un insieme di coefficienti di regressione nell’analisi di regressione. Per evitare troppa confusione notazionale, per ora useremo <span class="math notranslate nohighlight">\( \theta \)</span> per rappresentare sia parametri scalari che vettoriali dove la differenza sarà rivelata dal contesto.</p>
<p>Nell’inferenza statistica, l’obiettivo è ottenere stime dei parametri sconosciuti dati i dati. La differenza chiave tra l’inferenza statistica bayesiana e l’inferenza statistica frequentista riguarda la natura dei parametri sconosciuti <span class="math notranslate nohighlight">\( \theta \)</span>. Nella tradizione frequentista, si assume che <span class="math notranslate nohighlight">\( \theta \)</span> sia sconosciuto, ma abbia un valore fisso che desideriamo stimare. Nell’inferenza statistica bayesiana, <span class="math notranslate nohighlight">\( \theta \)</span> è considerato anch’esso sconosciuto, ma invece di essere fisso, si suppone, come <span class="math notranslate nohighlight">\( Y \)</span>, di essere una variabile casuale che possiede una distribuzione di probabilità a priori che riflette la nostra incertezza sul vero valore di <span class="math notranslate nohighlight">\( \theta \)</span> prima di aver visto i dati. Poiché sia i dati osservati <span class="math notranslate nohighlight">\( y \)</span> che i parametri <span class="math notranslate nohighlight">\( \theta \)</span> sono considerati variabili casuali, il calcolo delle probabilità ci consente di modellare la probabilità congiunta dei parametri e dei dati come una funzione della distribuzione condizionale dei dati dati i parametri, e la distribuzione a priori dei parametri. Più formalmente,</p>
<div class="math notranslate nohighlight" id="equation-eq-prob-congiunta-bayes">
<span class="eqno">(56)<a class="headerlink" href="#equation-eq-prob-congiunta-bayes" title="Permalink to this equation">#</a></span>\[ 
\begin{equation}
p(\theta, y) = p(y \mid \theta)p(\theta), 
\end{equation}
\]</div>
<p>dove <span class="math notranslate nohighlight">\( p(\theta, y) \)</span> è la distribuzione congiunta dei parametri e dei dati. Usando il teorema di Bayes dell’Eq. <a class="reference internal" href="../chapter_3/03_bayes_theorem.html#equation-eq-bayes-cont">(16)</a>, otteniamo il seguente:</p>
<div class="math notranslate nohighlight" id="equation-eq-bayes-revisited">
<span class="eqno">(57)<a class="headerlink" href="#equation-eq-bayes-revisited" title="Permalink to this equation">#</a></span>\[ 
\begin{equation}
p(\theta \mid y) = \frac{p(\theta, y)}{p(y)} = \frac{p(y \mid \theta)p(\theta)}{p(y)},
\end{equation}
\]</div>
<p>dove <span class="math notranslate nohighlight">\( p(\theta \mid y) \)</span> rappresenta la distribuzione a posteriori dei parametri <span class="math notranslate nohighlight">\( \theta \)</span> dati i dati osservati <span class="math notranslate nohighlight">\( y \)</span>. Quindi, dall’Eq. <a class="reference internal" href="#equation-eq-bayes-revisited">(57)</a>, la distribuzione a posteriori di <span class="math notranslate nohighlight">\( \theta \)</span> dato <span class="math notranslate nohighlight">\( y \)</span> è uguale alla distribuzione dei dati <span class="math notranslate nohighlight">\( p(y \mid \theta) \)</span> moltiplicata per la distribuzione a priori dei parametri <span class="math notranslate nohighlight">\( p(\theta) \)</span> normalizzata per <span class="math notranslate nohighlight">\( p(y) \)</span> in modo che la distribuzione a posteriori  integri a 1. Per variabili discrete,</p>
<div class="math notranslate nohighlight">
\[ 
p(y) = \sum_\theta p(y \mid \theta)p(\theta)  
\]</div>
<p>e per variabili continue,</p>
<div class="math notranslate nohighlight">
\[ 
p(y) = \int p(y \mid \theta)p(\theta)d\theta.
\]</div>
<p>Come osservazione a margine, per modelli complessi con molti parametri, l’Eq. <a class="reference internal" href="#equation-eq-bayes-revisited">(57)</a> sarà molto difficile da valutare, ed è per questa ragione che abbiamo bisogno dei metodi computazionali che verranno discussi in seguito.</p>
<p>Il denominatore dell’Eq. <a class="reference internal" href="#equation-eq-bayes-revisited">(57)</a> non coinvolge i parametri del modello, quindi possiamo omettere il termine <span class="math notranslate nohighlight">\(p(y)\)</span> e ottenere la distribuzione a posteriori non normalizzata:</p>
<div class="math notranslate nohighlight" id="equation-eq-not-normalized-posterior">
<span class="eqno">(58)<a class="headerlink" href="#equation-eq-not-normalized-posterior" title="Permalink to this equation">#</a></span>\[ 
\begin{equation}
p(\theta \mid y) \propto p(y \mid \theta)p(\theta).
\end{equation}
\]</div>
<p>Consideriamo la densità dei dati <span class="math notranslate nohighlight">\( p(y \mid \theta) \)</span> sul lato destro dell’Eq. <a class="reference internal" href="#equation-eq-not-normalized-posterior">(58)</a>. Quando è espresso in termini dei parametri sconosciuti <span class="math notranslate nohighlight">\( \theta \)</span> per valori fissi di <span class="math notranslate nohighlight">\( y \)</span>, questo termine è la verosimiglianza <span class="math notranslate nohighlight">\( L(\theta \mid y) \)</span>, che abbiamo discusso in dettaglio nel capitolo <a class="reference internal" href="../chapter_3/09_likelihood.html#notebook-likelihood"><span class="std std-ref">La verosimiglianza</span></a>. Quindi, l’Eq. <a class="reference internal" href="#equation-eq-not-normalized-posterior">(58)</a> può essere riscritta come</p>
<div class="math notranslate nohighlight" id="equation-eq-not-normalized-posterior-like">
<span class="eqno">(59)<a class="headerlink" href="#equation-eq-not-normalized-posterior-like" title="Permalink to this equation">#</a></span>\[ 
\begin{equation}
p(\theta \mid y) \propto L(\theta \mid y)p(\theta).
\end{equation}
\]</div>
<p>Le Eq. <a class="reference internal" href="#equation-eq-not-normalized-posterior">(58)</a> o <a class="reference internal" href="#equation-eq-not-normalized-posterior-like">(59)</a> rappresentano il nucleo dell’inferenza statistica bayesiana ed è ciò che separa la statistica bayesiana dalla statistica frequentista.</p>
<p>La validità dell’approccio bayesiano è supportata dai lavori di Cox e Savage, che dimostrano come, se p(θ) e p(y|θ) rappresentano le convinzioni razionali di una persona, la regola di Bayes sia il metodo ottimale per aggiornarle alla luce di nuove informazioni. Questo fornisce una base teorica solida per il suo utilizzo nell’apprendimento quantitativo. Tuttavia, nella pratica, può essere difficile esprimere matematicamente le convinzioni a priori con precisione, portando a una scelta di p(θ) talvolta ad hoc o dettata dalla convenienza computazionale.</p>
<p>Secondo Box e Draper, “tutti i modelli sono sbagliati, ma alcuni sono utili”. Allo stesso modo, una distribuzione a priori p(θ) potrebbe non rappresentare perfettamente le nostre convinzioni iniziali, ma ciò non ne sminuisce l’utilità. Se p(θ) approssima le nostre convinzioni, allora la distribuzione a posteriori p(θ|y) sarà un’utile approssimazione di quelle che dovrebbero essere le nostre convinzioni aggiornate. Inoltre, la regola di Bayes può essere usata per esplorare come i dati influenzerebbero le convinzioni di persone con opinioni a priori diverse. Di particolare interesse potrebbero essere le convinzioni a posteriori di qualcuno con informazioni a priori deboli. Ciò ha motivato l’uso di distribuzioni a priori “diffuse”, che assegnano la probabilità più o meno uniformemente su ampie regioni dello spazio dei parametri.</p>
<p>Infine, in molti problemi statistici complessi non esistono metodi non bayesiani di stima o inferenza. In queste situazioni,  la regola di Bayes offre un mezzo per generare procedure di stima, la cui efficacia può essere valutata secondo criteri non bayesiani. Spesso, si è constatato che le procedure bayesiane o quasi-bayesiane sono efficaci anche per scopi non bayesiani.</p>
</section>
<section id="l-aggiornamento-bayesiano">
<h2>L’aggiornamento bayesiano<a class="headerlink" href="#l-aggiornamento-bayesiano" title="Permalink to this heading">#</a></h2>
<p>Per offrire una spiegazione quantitativa del procedimento di aggiornamento bayesiano, prendiamo in considerazione un esempio pratico: la localizzazione di un aeromobile disperso. In questo capitolo, ci concentriamo esclusivamente sulla comprensione della struttura logica del problema e sulla sua formalizzazione tramite l’impiego di distribuzioni di probabilità. In questa fase, tralasciamo i dettagli implementativi e mettiamo invece l’accento sul significato delle diverse fasi e degli obiettivi del processo di aggiornamento bayesiano. Successivamente, esamineremo le metodologie per conseguire tali obiettivi, approfondendo gli aspetti di natura computazionale.</p>
<p>L’illustrazione coinvolge un ipotetico scenario in cui un aereo è disperso nell’Oceano Pacifico. Ci troviamo in un contesto in cui la latitudine è determinata, ma la longitudine rimane ignota (disponiamo unicamente dell’indicazione della direzione del viaggio, senza conoscere la distanza percorsa). L’obiettivo principale è rifinire la stima della posizione approssimata (<span class="math notranslate nohighlight">\(\theta\)</span>) dell’aereo. Per raggiungere questo fine, gli operatori di soccorso raccolgono dati dai frammenti dei detriti che sono stati individuati.</p>
<p>Iniziamo il processo con una stima iniziale, definita come <em>distribuzione a priori</em>. Questa distribuzione di probabilità rappresenta il nostro grado di conoscenza sulla posizione dell’aereo prima di ricevere ulteriori dati o informazioni.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">dnorm_trunc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ll</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ul</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">sd</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">ul</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">sd</span><span class="p">)</span> <span class="o">-</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">ll</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">sd</span><span class="p">))</span>
    <span class="n">out</span><span class="p">[(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="n">ul</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">x</span> <span class="o">&lt;</span> <span class="n">ll</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="c1"># Data points for x-axis</span>
<span class="n">x_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="c1"># Parameters for the dnorm_trunc function</span>
<span class="n">mean_val</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">sd_val</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="c1"># Calculate the y values using dnorm_trunc function</span>
<span class="n">y_vals</span> <span class="o">=</span> <span class="n">dnorm_trunc</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">mean_val</span><span class="p">,</span> <span class="n">sd_val</span><span class="p">)</span>

<span class="c1"># Plot the curve</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\theta$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;West&quot;</span><span class="p">,</span> <span class="s2">&quot;East&quot;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/7c847ae2620c36eb07e8000518ab46c04d16bb76c051cdcdf6219d4d5edef81d.png" src="../_images/7c847ae2620c36eb07e8000518ab46c04d16bb76c051cdcdf6219d4d5edef81d.png" />
</div>
</div>
<p>Nel contesto bayesiano, una distribuzione di probabilità assume il compito di rappresentare l’incertezza o le convinzioni che nutriamo riguardo ai molteplici valori possibili che un parametro può assumere. Nel caso attuale, il parametro <span class="math notranslate nohighlight">\(\theta\)</span> indica la longitudine associata alla posizione dell’aereo disperso nell’oceano Pacifico. La posizione esatta dell’aereo rimane sconosciuta. Tuttavia, formuliamo delle ipotesi iniziali sulle possibili localizzazioni. Per ogni valore possibile di <span class="math notranslate nohighlight">\(\theta\)</span>, la distribuzione di probabilità attribuisce un livello di fiducia che rispecchia quanto riteniamo probabile che quel valore specifico rappresenti il vero valore del parametro. I valori di <span class="math notranslate nohighlight">\(\theta\)</span> associati a ordinate più elevate nella funzione indicano un grado superiore di fiducia, in quanto riteniamo che tali valori siano più propensi a rappresentare il vero valore del parametro. In contrasto, i valori di <span class="math notranslate nohighlight">\(\theta\)</span> associati a ordinate più basse denotano convinzioni più deboli.</p>
<p>Sull’asse <span class="math notranslate nohighlight">\(x\)</span> del grafico sopra riportato sono indicati i valori di <span class="math notranslate nohighlight">\(\theta\)</span>, cioè i diversi possibili valori della longitudine dell’aereo. Le estremità dell’asse delle ascisse sono etichettate come “West” (Ovest) e “East” (Est), per indicare che <span class="math notranslate nohighlight">\(\theta\)</span> spazia da ovest a est. Lungo l’asse delle ordinate (<span class="math notranslate nohighlight">\(y\)</span>), sono tracciati i valori delle densità di probabilità associati a ciascun valore di <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Nel nostro caso specifico, la distribuzione a priori è centrata intorno a <span class="math notranslate nohighlight">\(\theta = 0.8\)</span>, suggerendo una previsione iniziale che l’aereo sia più verosimilmente situato ad est. Tuttavia, permangono incertezze considerevoli riguardo alla posizione esatta. Possiamo affermare, comunque, che inizialmente riteniamo che sia due volte più probabile che l’aereo si trovi ad est rispetto a ovest.</p>
<p>La scelta della distribuzione a priori rispecchia le convinzioni del ricercatore riguardo al problema in questione. Di conseguenza, diversi ricercatori potrebbero formulare diverse distribuzioni a priori per lo stesso problema, e tale diversità è accettabile, a patto che tali distribuzioni siano ragionevolmente giustificate. Nel prosieguo, scopriremo che nelle analisi Bayesiane, anche con campioni di dimensioni moderate, le varie distribuzioni a priori generano solitamente differenze trascurabili.</p>
<p>Ora, supponiamo di aver raccolto dei detriti nelle posizioni mostrate nel grafico seguente.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">qnorm_trunc</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ll</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ul</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">cdf_ll</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">ll</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sd</span><span class="p">)</span>
    <span class="n">cdf_ul</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">ul</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sd</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">cdf_ll</span> <span class="o">+</span> <span class="n">p</span> <span class="o">*</span> <span class="p">(</span><span class="n">cdf_ul</span> <span class="o">-</span> <span class="n">cdf_ll</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sd</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">rnorm_trunc</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ll</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ul</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">qnorm_trunc</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">sd</span><span class="p">,</span> <span class="n">ll</span><span class="o">=</span><span class="n">ll</span><span class="p">,</span> <span class="n">ul</span><span class="o">=</span><span class="n">ul</span><span class="p">)</span>


<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">101</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">compute_lik</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pts</span><span class="o">=</span><span class="n">grid</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">binwidth</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="n">lik_vals</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="n">pts</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sd</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span>
        <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">pts</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sd</span><span class="p">)</span> <span class="o">-</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">pts</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sd</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">lik</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">lik_vals</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">lik</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">lik</span><span class="p">)</span> <span class="o">/</span> <span class="n">binwidth</span>


<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">dat_x</span> <span class="o">=</span> <span class="n">rnorm_trunc</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">lik_x</span> <span class="o">=</span> <span class="n">compute_lik</span><span class="p">(</span><span class="n">dat_x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">lik_x</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">dat_x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">dat_x</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\theta$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Likelihood (Scaled)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;West&quot;</span><span class="p">,</span> <span class="s2">&quot;East&quot;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/2deb23f512ccfef7be2a8ab85cd94575508cfe621c7d98b66b41abb67afec577.png" src="../_images/2deb23f512ccfef7be2a8ab85cd94575508cfe621c7d98b66b41abb67afec577.png" />
</div>
</div>
<p>Dal Teorema di Bayes, è possibile derivare la distribuzione a posteriori attraverso il seguente procedimento:</p>
<div class="math notranslate nohighlight">
\[
\text{Probabilità a posteriori} \propto \text{Probabilità a priori} \times
                                       \text{Verosimiglianza}.
\]</div>
<p>In altre parole, è sufficiente moltiplicare le probabilità a priori e la verosimiglianza al fine di ottenere la probabilità posteriore per ciascuna posizione. È importante garantire che l’area sotto la curva sia normalizzata a 1. Questo processo è conosciuto come <em>aggiornamento bayesiano</em>.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">update_probs</span><span class="p">(</span><span class="n">prior_probs</span><span class="p">,</span> <span class="n">lik</span><span class="p">,</span> <span class="n">binwidth</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="n">post_probs</span> <span class="o">=</span> <span class="n">prior_probs</span> <span class="o">*</span> <span class="n">lik</span>
    <span class="k">return</span> <span class="n">post_probs</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">post_probs</span><span class="p">)</span> <span class="o">/</span> <span class="n">binwidth</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">101</span><span class="p">)</span>

<span class="c1"># Prior probabilities</span>
<span class="n">prior_probs</span> <span class="o">=</span> <span class="n">dnorm_trunc</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Likelihood values</span>
<span class="n">lik_x</span> <span class="o">=</span> <span class="n">compute_lik</span><span class="p">(</span><span class="n">dat_x</span><span class="p">)</span>

<span class="c1"># Posterior probabilities</span>
<span class="n">posterior_probs</span> <span class="o">=</span> <span class="n">update_probs</span><span class="p">(</span><span class="n">prior_probs</span><span class="p">,</span> <span class="n">lik_x</span><span class="p">)</span>

<span class="c1"># Plotting</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">dnorm_trunc</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Prior&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">lik_x</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;solid&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Likelihood&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">posterior_probs</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Posterior&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">dat_x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">dat_x</span><span class="p">)</span><span class="o">+</span><span class="mf">.1</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.15</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="o">-</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;West&quot;</span><span class="p">,</span> <span class="s2">&quot;East&quot;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/d3f10a8110b7f2c8da50ffce0104b11a4e5c38be3fe5f3e175c811ec497a17e6.png" src="../_images/d3f10a8110b7f2c8da50ffce0104b11a4e5c38be3fe5f3e175c811ec497a17e6.png" />
</div>
</div>
<p>In questa situazione, è evidente che una distribuzione a priori come quella descritta in precedenza, che definiremo “debolmente informativa”, ha un impatto trascurabile e la distribuzione a posteriori risulta quasi indistinguibile dalla verosimiglianza (che è stata normalizzata).</p>
<p>Ora, esamineremo come si comporta una distribuzione a priori maggiormente informativa.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">101</span><span class="p">)</span>

<span class="c1"># Prior probabilities</span>
<span class="n">prior_probs</span> <span class="o">=</span> <span class="n">dnorm_trunc</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Likelihood values</span>
<span class="n">lik_x</span> <span class="o">=</span> <span class="n">compute_lik</span><span class="p">(</span><span class="n">dat_x</span><span class="p">)</span>

<span class="c1"># Posterior probabilities</span>
<span class="n">posterior_probs</span> <span class="o">=</span> <span class="n">update_probs</span><span class="p">(</span><span class="n">prior_probs</span><span class="p">,</span> <span class="n">lik_x</span><span class="p">)</span>

<span class="c1"># Plotting</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">dnorm_trunc</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Prior&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">lik_x</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;solid&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Likelihood&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">posterior_probs</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Posterior&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">dat_x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">dat_x</span><span class="p">),</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.15</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;West&quot;</span><span class="p">,</span> <span class="s2">&quot;East&quot;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/f316c79753b285c7e4b8caba7343bf668fd5d96e45160db5270106b7679898dd.png" src="../_images/f316c79753b285c7e4b8caba7343bf668fd5d96e45160db5270106b7679898dd.png" />
</div>
</div>
<p>Il grafico illustra le tre distribuzioni coinvolte nell’aggiornamento bayesiano.</p>
<ol class="arabic simple">
<li><p>La curva tratteggiata rossa rappresenta la distribuzione a priori di <span class="math notranslate nohighlight">\(\theta\)</span>. Questa curva riflette le nostre credenze iniziali o le aspettative riguardo ai possibili valori di <span class="math notranslate nohighlight">\(\theta\)</span> prima di effettuare qualsiasi osservazione. Nel nostro esempio, la distribuzione a priori è una distribuzione normale con una media di 0.8 e una deviazione standard di 0.1. Questo implica che, prima di raccogliere dati, prevediamo che il valore di <span class="math notranslate nohighlight">\(\theta\)</span> sia prossimo a 0.8, con una limitata variazione intorno a questa media.</p></li>
<li><p>La curva continua blu rappresenta la verosimiglianza dei dati osservati dato un valore specifico di <span class="math notranslate nohighlight">\(\theta\)</span>. In altre parole, essa descrive quanto i dati osservati supportano ogni possibile valore di <span class="math notranslate nohighlight">\(\theta\)</span>. La forma della curva indica quale valore di <span class="math notranslate nohighlight">\(\theta\)</span> risulta più plausibile in base a ciò che è stato osservato. Maggiore è l’altezza della curva in un determinato punto, maggiore è il supporto fornito dai dati a quel valore di <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
<li><p>La curva tratteggiata nera rappresenta la distribuzione a posteriori di <span class="math notranslate nohighlight">\(\theta\)</span>. Questa curva rappresenta la nostra stima aggiornata di <span class="math notranslate nohighlight">\(\theta\)</span> dopo aver incorporato i dati osservati e le credenze iniziali attraverso il calcolo bayesiano. La distribuzione a posteriori combina la distribuzione a priori con la verosimiglianza dei dati, fornendo una stima più precisa e informativa di <span class="math notranslate nohighlight">\(\theta\)</span>. In sostanza, essa riflette la nostra comprensione aggiornata del valore di <span class="math notranslate nohighlight">\(\theta\)</span> più probabile, tenendo conto sia delle informazioni iniziali che dei dati osservati.</p>
<ul class="simple">
<li><p>Il valore di <span class="math notranslate nohighlight">\(\theta\)</span> più probabile nella distribuzione a posteriori corrisponde al punto in cui la curva raggiunge l’apice, ovvero il valore di <span class="math notranslate nohighlight">\(\theta\)</span> in cui la densità è massima. Questo punto rappresenta la stima del parametro <span class="math notranslate nohighlight">\(\theta\)</span> che appare più plausibile alla luce dei dati osservati e delle credenze iniziali.</p></li>
<li><p>Inoltre, la distribuzione a posteriori fornisce indicazioni sulla nostra incertezza riguardo al valore di <span class="math notranslate nohighlight">\(\theta\)</span>. Se la distribuzione a posteriori è concentrata attorno a un valore specifico di <span class="math notranslate nohighlight">\(\theta\)</span> e presenta un picco netto, ciò suggerisce che siamo più sicuri nella stima di <span class="math notranslate nohighlight">\(\theta\)</span> e l’incertezza è ridotta. In altre parole, i dati osservati sono informativi e hanno ridotto l’incertezza sul valore di <span class="math notranslate nohighlight">\(\theta\)</span>. Invece, se la distribuzione a posteriori è ampia e ha una forma meno definita, implica maggiore incertezza nella stima di <span class="math notranslate nohighlight">\(\theta\)</span>. Questo può accadere quando i dati osservati sono scarsi o poco informativi, oppure se la distribuzione a priori era ampia, consentendo una vasta gamma di valori di <span class="math notranslate nohighlight">\(\theta\)</span>. In sintesi, la forma della distribuzione a posteriori riflette quanto i dati raccolti ci abbiano aiutato a restringere le possibili valutazioni di <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
</ul>
</li>
</ol>
</section>
<section id="linguaggi-di-programmazione-probabilistici">
<h2>Linguaggi di programmazione probabilistici<a class="headerlink" href="#linguaggi-di-programmazione-probabilistici" title="Permalink to this heading">#</a></h2>
<p>L’esempio precedente illustra il problema cui la modellazione bayesiana intende rispondere e la soluzione che essa propone per risolverlo. Finora abbiamo illustrato la logica dell’aggiornamento bayesiano, senza ancora entrare nei dettagli computazionali che verranno approfonditi nei prossimi capitoli.</p>
<p>Per ora, è importante sottolineare che l’attuale statistica bayesiana fa un ampio utilizzo di un linguaggio di programmazione probabilistico (<em>Probabilistic Programming Language</em>, PPL) implementato su computer per eseguire l’aggiornamento bayesiano. Questo approccio ha rivoluzionato il modo in cui si svolgevano le analisi statistiche bayesiane anche solo pochi decenni fa. L’adozione di tali metodi computazionali ha semplificato la formulazione di modelli statistici complessi, abbassando la soglia delle competenze matematiche e computazionali richieste, e agevolando il processo di modellazione bayesiana. Inoltre, questi strumenti hanno aperto nuove opportunità per affrontare problemi di analisi dei dati che, in passato, sarebbero stati notevolmente complessi da trattare.</p>
</section>
<section id="notazione">
<h2>Notazione<a class="headerlink" href="#notazione" title="Permalink to this heading">#</a></h2>
<p>Per chiarire la notazione, nel seguito useremo <span class="math notranslate nohighlight">\(y\)</span> per rappresentare i dati e <span class="math notranslate nohighlight">\(\theta\)</span> per indicare i parametri incogniti di un modello statistico. Entrambi, <span class="math notranslate nohighlight">\(y\)</span> e <span class="math notranslate nohighlight">\(\theta\)</span>, saranno trattati come variabili casuali. Utilizzeremo invece <span class="math notranslate nohighlight">\(x\)</span> per denotare le quantità note, come ad esempio i predittori di un modello lineare.</p>
<p>Per rappresentare in modo conciso i modelli probabilistici, adotteremo una notazione particolare. Ad esempio, invece di scrivere <span class="math notranslate nohighlight">\(p(\theta) = Beta(1, 1)\)</span>, scriveremo semplicemente <span class="math notranslate nohighlight">\(\theta \sim Beta(1, 1)\)</span>. Il simbolo “<span class="math notranslate nohighlight">\(\sim\)</span>” viene comunemente letto come “è distribuito come”. Possiamo anche interpretarlo nel senso che <span class="math notranslate nohighlight">\(\theta\)</span> è un campione casuale estratto dalla distribuzione Beta(1, 1). Allo stesso modo, la verosimiglianza di un modello binomiale sarà scritta come <span class="math notranslate nohighlight">\(y \sim \text{Bin}(n, \theta)\)</span>.</p>
</section>
<section id="la-distribuzione-a-priori">
<h2>La Distribuzione a Priori<a class="headerlink" href="#la-distribuzione-a-priori" title="Permalink to this heading">#</a></h2>
<p>Nessuno studio viene condotto in assenza totale di conoscenze derivate da ricerche precedenti. L’inferenza statistica bayesiana richiede semplicemente che queste conoscenze pregresse vengano esplicitate, per poi essere messe alla prova dai dati effettivi a disposizione. Le distribuzioni a priori codificano direttamente le nostre assunzioni riguardo ai valori ragionevoli dei parametri del modello.</p>
<section id="distribuzioni-a-priori-non-informativa">
<h3>Distribuzioni a Priori Non Informativa<a class="headerlink" href="#distribuzioni-a-priori-non-informativa" title="Permalink to this heading">#</a></h3>
<p>Le distribuzioni a priori possono differire in base al grado di certezza con cui spingono il ricercatore a considerare credibile un determinato intervallo di valori dei parametri. Un caso estremo è rappresentato dalle distribuzioni a priori <em>non informative</em>, che indicano una totale mancanza di conoscenza pregressa e assegnano lo stesso livello di credibilità a tutti i valori dei parametri. Questi tipi di priori vengono anche definiti come priori vaghi o diffusi. Una delle distribuzioni a priori non informative più ovvie da utilizzare è la distribuzione di probabilità uniforme su un intervallo sensato di valori dei parametri. L’applicazione della distribuzione uniforme si basa sul Principio della Ragione Insufficiente, enunciato per la prima volta da Laplace (1774/1951), che afferma che in assenza di qualsiasi evidenza rilevante (precedente), si dovrebbero assegnare le proprie gradi di credibilità in modo uguale tra tutti gli esiti possibili.</p>
</section>
<section id="distribuzioni-a-priori-debolmente-informativa">
<h3>Distribuzioni a Priori Debolmente Informativa<a class="headerlink" href="#distribuzioni-a-priori-debolmente-informativa" title="Permalink to this heading">#</a></h3>
<p>Le distribuzioni a priori debolmente informative sono distribuzioni di probabilità che forniscono un metodo per incorporare meno informazioni di quante se ne abbiano effettivamente in una particolare situazione. Una distribuzione a priori debolmente informativa che contiene un intervallo di valori ragionevoli dei parametri per riflettere le assunzioni ragionevoli sui parametri ma riconoscendo anche le incertezze in un’analisi data, risulta utile nel stabilizzare le stime di un modello. L’inferenza bayesiana può richiedere un impegno computazionale notevole, specialmente per modelli gerarchici. Pertanto, fornire informazioni a priori debolmente informative può aiutare a stabilizzare l’analisi senza influenzare le inferenze.</p>
</section>
<section id="distribuzioni-a-priori-informativa">
<h3>Distribuzioni a Priori Informativa<a class="headerlink" href="#distribuzioni-a-priori-informativa" title="Permalink to this heading">#</a></h3>
<p>Ricerche precedenti, opinioni esperte, o entrambe, possono essere utilizzate per affrontare un problema e essere incorporate sistematicamente nelle distribuzioni a priori. Questi tipi di priori sono denominati come distribuzioni a priori informative. Queste distribuzioni riflettono informazioni concrete e pertinenti che possono avere un impatto significativo sull’analisi, fornendo una base di conoscenza su cui costruire l’inferenza bayesiana. Le distribuzioni a priori informative possono provenire da una varietà di fonti e offrono un modo strutturato per integrare la conoscenza pregressa nel processo di analisi statistica, migliorando così la robustezza e l’accuratezza delle conclusioni estratte dai dati.</p>
</section>
</section>
<section id="la-verosimiglianza-marginale">
<h2>La verosimiglianza marginale<a class="headerlink" href="#la-verosimiglianza-marginale" title="Permalink to this heading">#</a></h2>
<p>La formula completa della distribuzione a posteriori si esprime come segue:</p>
<div class="math notranslate nohighlight">
\[
p(\theta \mid y) = \frac{p(y \mid \theta) p(\theta)}{\int_{\Theta}p(y \mid \theta) p(\theta) \,d\theta} \quad \text{per} \quad \theta \in \Theta, 
\]</div>
<p>dove <span class="math notranslate nohighlight">\(\Theta\)</span> rappresenta lo spazio dei possibili valori del parametro <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Per calcolare <span class="math notranslate nohighlight">\(p(\theta \mid y)\)</span>, è necessario dividere il prodotto tra la distribuzione a priori e la verosimiglianza per una costante di normalizzazione. Questa costante, chiamata <em>verosimiglianza marginale</em>, viene introdotta per garantire che <span class="math notranslate nohighlight">\(p(\theta \mid y)\)</span> abbia un’area unitaria sotto la curva. Tuttavia, l’integrale al denominatore della formula <code class="xref eq docutils literal notranslate"><span class="pre">eq-bayes-intro</span></code> spesso risulta difficile da risolvere analiticamente. Di conseguenza, l’inferenza bayesiana procede generalmente mediante metodi di approssimazione numerica.</p>
</section>
<section id="metodi-di-stima-della-distribuzione-a-posteriori">
<h2>Metodi di stima della distribuzione a posteriori<a class="headerlink" href="#metodi-di-stima-della-distribuzione-a-posteriori" title="Permalink to this heading">#</a></h2>
<p>Ci sono due principali strategie per calcolare la distribuzione a posteriori:</p>
<ol class="arabic simple">
<li><p><strong>Metodo esatto</strong>: Questo approccio è applicabile quando la distribuzione a priori e la funzione di verosimiglianza appartengono alla stessa classe di distribuzioni, note come <em>distribuzioni a priori coniugate</em>. Quando ciò accade, la distribuzione a posteriori può essere determinata analiticamente, senza necessità di approssimazioni. Questo metodo è elegante e computazionalmente efficiente, ma ha una portata limitata, in quanto le distribuzioni a priori coniugate esistono solo per certe combinazioni specifiche di distribuzioni a priori e verosimiglianze.</p></li>
<li><p><strong>Metodo approssimato</strong>: Quando il metodo esatto non può essere utilizzato, ad esempio, quando le distribuzioni a priori e le verosimiglianze non sono coniugate, si può ricorrere al metodo approssimato. Questo implica l’uso di algoritmi computazionalmente intensivi, come le Catene di Markov Monte Carlo (MCMC), per stimare la distribuzione a posteriori. Sebbene questo approccio sia più flessibile e applicabile a una vasta gamma di scenari, richiede più risorse computazionali e può essere più lento rispetto al metodo esatto.</p></li>
</ol>
</section>
<section id="il-flusso-di-lavoro-bayesiano">
<h2>Il flusso di lavoro bayesiano<a class="headerlink" href="#il-flusso-di-lavoro-bayesiano" title="Permalink to this heading">#</a></h2>
<p>Dopo aver analizzato l’esempio introduttivo delineato nelle sezioni precedenti, esploriamo ora con maggiore attenzione il cosiddetto flusso di lavoro bayesiano. Metaforicamente definito come il processo di “girare la manovella Bayesiana”, il flusso di lavoro bayesiano enfatizza un approccio iterativo e adattabile nel contesto della ricerca scientifica. Questo procedimento, ben delineato nella figura tratta dall’articolo di <span id="id10">Baribault and Collins [<a class="reference internal" href="../references/bibliography.html#id30" title="Beth Baribault and Anne GE Collins. Troubleshooting bayesian cognitive models. Psychological Methods, 2023.">BC23</a>]</span>, è caratterizzato da diverse fasi cruciali che vanno dalla definizione delle priori all’inferenza basata sui dati a posteriori.</p>
<figure class="align-default" id="bayes-workflow-fig">
<a class="reference internal image-reference" href="../_images/bayesian_workflow.png"><img alt="../_images/bayesian_workflow.png" src="../_images/bayesian_workflow.png" style="height: 550px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">Una rappresentazione abbreviata del flusso di lavoro bayesiano. L’output del modello che non supera il filtro (che rappresenta i necessari controlli computazionali e di coerenza) deve essere respinto. È necessario migliorare la specifica del modello in modo che l’output possa  superare tutti i controlli. Solo allora il modello bayesiano può essere utilizzato come base per l’inferenza. (Figura tratta da <span id="id11">Baribault and Collins [<a class="reference internal" href="../references/bibliography.html#id30" title="Beth Baribault and Anne GE Collins. Troubleshooting bayesian cognitive models. Psychological Methods, 2023.">BC23</a>]</span>).</span><a class="headerlink" href="#bayes-workflow-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<section id="fasi-del-flusso-di-lavoro">
<h3>Fasi del flusso di lavoro<a class="headerlink" href="#fasi-del-flusso-di-lavoro" title="Permalink to this heading">#</a></h3>
<p>Di seguito, delineiamo le fasi essenziali nel flusso di lavoro Bayesiano:</p>
<ol class="arabic simple">
<li><p><strong>Studio di simulazione</strong>: Qui, l’obiettivo è generare dati sintetici che replicano fedelmente il contesto della ricerca, verificando la robustezza del disegno sperimentale e assicurando che le specificazioni del modello siano appropriate.</p></li>
<li><p><strong>Raccolta e identificazione dei dati</strong>: Questa fase comporta l’acquisizione e l’esplorazione preliminare dei dati reali, garantendo che siano adeguati e preparati correttamente per le analisi successive.</p></li>
<li><p><strong>Selezione del modello statistico</strong>: In questa fase, si formula un modello statistico che rappresenta accuratamente le teorie e le ipotesi sottostanti alla ricerca, basandosi su una solida comprensione del fenomeno in esame e su principi statistici ben fondati.</p></li>
<li><p><strong>Definizione delle distribuzioni a priori</strong>: Qui, si definiscono le distribuzioni a priori dei parametri del modello, basandosi su conoscenze preesistenti e su un ragionamento teorico valido.</p></li>
<li><p><strong>Calcolo delle distribuzioni a posteriori</strong>: Utilizzando metodi analitici o, più comunemente, tecniche di campionamento come Monte Carlo a Catene di Markov (MCMC), si derivano le distribuzioni a posteriori dei parametri.</p></li>
<li><p><strong>Troubleshooting e diagnostica</strong>: In questa fase, si effettuano controlli rigorosi sulla convergenza del modello e sulla validità delle inferenze, attraverso metriche e diagnosi specializzate.</p></li>
<li><p><strong>Controlli di coerenza</strong>: Oltre alla diagnostica tecnica, è fondamentale valutare la coerenza e la plausibilità del modello in rapporto ai dati e al contesto teorico, compresa una valutazione predittiva a posteriori.</p></li>
<li><p><strong>Interpretazione e comunicazione dei risultati</strong>: Infine, si interpretano i risultati nel contesto della teoria sottostante e si comunicano in modo chiaro ed efficace, integrandoli nel quadro più ampio della comprensione del fenomeno studiato.</p></li>
</ol>
<p>Attraverso questo processo iterativo e rigoroso, il flusso di lavoro Bayesiano mira a costruire modelli statistici robusti e inferenze valide, fornendo un quadro solido per la ricerca scientifica in vari campi, inclusa la psicologia.</p>
</section>
</section>
<section id="commenti-e-considerazioni-finali">
<h2>Commenti e considerazioni finali<a class="headerlink" href="#commenti-e-considerazioni-finali" title="Permalink to this heading">#</a></h2>
<p>L’approccio bayesiano offre una modalità distintiva per gestire l’incertezza associata ai parametri di interesse, distinguendosi nettamente dalla metodologia classica. Contrariamente all’idea che i parametri siano valori fissi e ignoti, l’approccio bayesiano li considera come quantità probabilistiche, attribuendo loro una distribuzione a priori che rappresenta le nostre credenze e intuizioni iniziali prima di condurre l’esperimento. Mediante l’applicazione del teorema di Bayes, queste credenze vengono modificate e affinate sulla base dei dati raccolti, portando alla definizione della distribuzione a posteriori. Quest’ultima esprime una visione aggiornata dell’incertezza, incorporando sia l’evidenza empirica che le informazioni preesistenti.</p>
<p>La forza dell’approccio bayesiano sta nella sua capacità di integrare le conoscenze pregresse con le nuove osservazioni, fornendo stime del parametro di interesse sia più accurate che ricche di significato. Questa metodologia va oltre la semplice analisi, guidando il processo decisionale e permettendo di affrontare l’incertezza con una comprensione profonda, coniugando conoscenze teoriche ed evidenze empiriche. In ultima analisi, l’approccio bayesiano non è solo un metodo statistico, ma uno strumento decisionale che valorizza l’interazione dinamica tra teoria ed esperienza.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_4"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="introduction_part_4.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Inferenza bayesiana</p>
      </div>
    </a>
    <a class="right-next"
       href="02_subj_prop.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Pensare ad una proporzione in termini soggettivi</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#esplorazione-dei-fondamenti-dei-metodi-bayesiani-in-psicologia">Esplorazione dei Fondamenti dei Metodi Bayesiani in Psicologia</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#il-ciclo-continuo-dell-apprendimento-bayesiano">Il Ciclo Continuo dell’Apprendimento Bayesiano</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dalla-generazione-dei-dati-alla-modellizzazione-bayesiana">Dalla Generazione dei Dati alla Modellizzazione Bayesiana</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#riallocazione-della-credibilita-e-aggiornamento-sequenziale-delle-informazioni">Riallocazione della Credibilità e Aggiornamento Sequenziale delle Informazioni</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#la-modellizzazione-statistica-un-ponte-tra-teoria-e-dati">La Modellizzazione Statistica: Un Ponte tra Teoria e Dati</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#caratteristiche-fondamentali-dei-modelli-bayesiani">Caratteristiche Fondamentali dei Modelli Bayesiani</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#il-processo-di-sviluppo-di-un-modello-bayesiano">Il Processo di Sviluppo di un Modello Bayesiano</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#riesame-del-teorema-di-bayes">Riesame del Teorema di Bayes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#l-aggiornamento-bayesiano">L’aggiornamento bayesiano</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linguaggi-di-programmazione-probabilistici">Linguaggi di programmazione probabilistici</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notazione">Notazione</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#la-distribuzione-a-priori">La Distribuzione a Priori</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distribuzioni-a-priori-non-informativa">Distribuzioni a Priori Non Informativa</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distribuzioni-a-priori-debolmente-informativa">Distribuzioni a Priori Debolmente Informativa</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distribuzioni-a-priori-informativa">Distribuzioni a Priori Informativa</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#la-verosimiglianza-marginale">La verosimiglianza marginale</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metodi-di-stima-della-distribuzione-a-posteriori">Metodi di stima della distribuzione a posteriori</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#il-flusso-di-lavoro-bayesiano">Il flusso di lavoro bayesiano</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fasi-del-flusso-di-lavoro">Fasi del flusso di lavoro</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#commenti-e-considerazioni-finali">Commenti e considerazioni finali</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Corrado Caudek
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>