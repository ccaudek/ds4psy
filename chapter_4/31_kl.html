

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>La divergenza di Kullback-Leibler &#8212; ds4p</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="../_static/a11y.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=365ca57ee442770a23c6"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/custom.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-VMXNE4BCDL"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-VMXNE4BCDL');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_4/31_kl';</script>
    <link rel="canonical" href="https://ccaudek.github.io/ds4psy/chapter_4/31_kl.html" />
    <link rel="shortcut icon" href="../_static/increasing.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Modello gerarchico beta-binomiale" href="40_hier_beta_binom.html" />
    <link rel="prev" title="Entropia" href="30_entropy.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="ds4p - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="ds4p - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Benvenuti
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_1/introduction_chapter_1.html">Python</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/01_python_1.html">Python (1)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/02_python_2.html">Python (2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_python.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/03_numpy.html">NumPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_numpy.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/04_pandas.html">Pandas (1)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/05_pandas_aggregate.html">Pandas (2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/06_pandas_functions.html">Pandas (3)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_pandas.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/07_matplotlib.html">Matplotlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/08_seaborn.html">Seaborn</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/ex_matplotlib.html">✏️ Esercizi</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_2/introduction_chapter_2.html">Statistica descrittiva</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/01_key_notions.html">Concetti chiave</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_key_notions.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/02_measurement.html">La misurazione in psicologia</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_scales.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/03_freq_distr.html">Dati e frequenze</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_sums.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/04_loc_scale.html">Indici di posizione e di scala</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/05_correlation.html">Le relazioni tra variabili</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/06_causality.html">Correlazione e causazione</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/07_crisis.html">La crisi della generalizzabilità</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_eda.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_2/E_mehr_song_spelke.html">✏️ Esercizi</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_3/introduction_chapter_3.html">Probabilità</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/01_intro_prob.html">Introduzione al calcolo delle probabilità</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_prob.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/02_conditional_prob.html">Probabilità condizionata</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_cond_prob.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/03_bayes_theorem.html">Il teorema di Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_bayes_theorem.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/04_expval_var.html">Variabili casuali</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/04a_sampling_distr.html">Stime, stimatori e parametri</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/04b_illusion.html">Incertezza inferenziale e variabilità dei risultati</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_rv_discrete.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/05_joint_prob.html">Probabilità congiunta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_joint_prob.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/06_density_func.html">La funzione di densità di probabilità</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/07_discr_rv_distr.html">Distribuzioni di v.c. discrete</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_binomial.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/08_cont_rv_distr.html">Distribuzioni di v.c. continue</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_gaussian.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_beta_distr.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/09_likelihood.html">La verosimiglianza</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/10_rescorla_wagner.html">Apprendimento per rinforzo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_3/E_likelihood.html">✏️ Esercizi</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="introduction_part_4.html">Inferenza bayesiana</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01_intro_bayes.html">Modellazione bayesiana</a></li>
<li class="toctree-l2"><a class="reference internal" href="02_subj_prop.html">Pensare ad una proporzione in termini soggettivi</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_conjugate_families_1.html">Distribuzioni coniugate (1)</a></li>
<li class="toctree-l2"><a class="reference internal" href="04_conjugate_families_2.html">Distribuzioni coniugate (2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_summary_posterior.html">Sintesi a posteriori</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_conjugate.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="06_balance-prior-post.html">L’influenza della distribuzione a priori</a></li>
<li class="toctree-l2"><a class="reference internal" href="10_metropolis.html">Monte Carlo a Catena di Markov</a></li>
<li class="toctree-l2"><a class="reference internal" href="11_beta_binomial_pymc.html">Inferenza bayesiana con PyMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="12_jax.html">Usare JAX per un campionamento più veloce</a></li>
<li class="toctree-l2"><a class="reference internal" href="13_preliz.html">Scegliere le distribuzioni a priori</a></li>
<li class="toctree-l2"><a class="reference internal" href="16_summary_posterior_pymc.html">Metodi di sintesi della distribuzione a posteriori</a></li>
<li class="toctree-l2"><a class="reference internal" href="17_prediction.html">La predizione bayesiana</a></li>
<li class="toctree-l2"><a class="reference internal" href="18_mcmc_diagnostics.html">Diagnostica delle catene markoviane</a></li>
<li class="toctree-l2"><a class="reference internal" href="19_odds_ratio.html">Analisi bayesiana dell’odds-ratio</a></li>
<li class="toctree-l2"><a class="reference internal" href="20_poisson_model.html">Modello di Poisson</a></li>
<li class="toctree-l2"><a class="reference internal" href="21_poisson_sim.html">Modello di Poisson: derivazione analitica e MCMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_freq.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="22_normal_normal_model.html">Inferenza bayesiana su una media</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_one_mean.html">✏️ Esercizio</a></li>
<li class="toctree-l2"><a class="reference internal" href="E_one_mean_2.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="23_two_groups.html">Confronto tra due gruppi</a></li>
<li class="toctree-l2"><a class="reference internal" href="24_multiple_groups.html">Gruppi multipli</a></li>
<li class="toctree-l2"><a class="reference internal" href="30_entropy.html">Entropia</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">La divergenza di Kullback-Leibler</a></li>
<li class="toctree-l2"><a class="reference internal" href="40_hier_beta_binom.html">Modello gerarchico beta-binomiale</a></li>
<li class="toctree-l2"><a class="reference internal" href="41_hier_poisson.html">Modello gerarchico di Poisson</a></li>
<li class="toctree-l2"><a class="reference internal" href="42_hier_gaussian.html">Modello gerarchico gaussiano</a></li>
<li class="toctree-l2"><a class="reference internal" href="hssm.html">Drift Diffusion Model</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_5/introduction_part_5.html">Analisi della regressione</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_1.html">Il modello di regressione lineare</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_2.html">Analisi bayesiana del modello di regressione lineare</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_params_recovery.html">Analisi di simulazione per la stima dei parametri nel modello di regressione</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_3.html">Zucchero sintattico</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_4.html">Confronto tra le medie di due gruppi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_5.html">Il modello lineare gerarchico</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_multilevel_modeling.html">A Primer on Bayesian Methods for Multilevel Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_reglin_7.html">Regressione robusta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_simpson.html">Paradosso di Simpson</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_reglin_1.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_reglin_2.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_reglin_3.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_reglin_4.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_logistic_reg.html">Modello di regressione logistica</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_binomial_reg.html">Regressione binomiale</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_covid.html">Inferenza controfattuale: calcolo delle morti in eccesso dovute al COVID-19</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_5/05_counterfactual.html">Analisi causale con PyMC</a></li>

<li class="toctree-l2"><a class="reference internal" href="../chapter_5/E_stab.html">✏️ Esercizi</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_6/introduction_part_6.html">Inferenza frequentista</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_estimation.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/02_conf_interv.html">Intervallo di confidenza</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/03_test_ipotesi.html">Significatività statistica</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_interpretation_test.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_significato_test.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/04_two_ind_samples.html">Test t di Student per campioni indipendenti</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_test_media_pop.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_medie_pop_ampie.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_medie_pop_piccoli.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_campioni_appaiati.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/E_confronto_proporzioni.html">✏️ Esercizi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/05_limiti_stat_frequentista.html">Limiti dell’inferenza frequentista</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_6/06_s_m_errors.html">Crisi della replicabilità</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../references/bibliography.html">Bibliografia</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_7/introduction_appendix.html">Appendici</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_1/00_installation.html">Ambiente di lavoro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a01_math_symbols.html">Simbologia di base</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a02_numbers.html">Numeri binari, interi, razionali, irrazionali e reali</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a04_summation_notation.html">Simbolo di somma (sommatorie)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a05_sets.html">Insiemi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a06_combinatorics.html">Calcolo combinatorio</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a07_calculus.html">Per liberarvi dai terrori preliminari</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a08_kde_plot.html">Kernel Density Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a09_prob_tutorial.html">Esercizi di probabilità discreta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a12_rng.html">Generazione di numeri casuali</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a14_predict_counts.html">La predizione delle frequenze</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a15_lin_fun.html">La funzione lineare</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a20_reglin_1.html">Regressione lineare bivariata</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a21_reglin_2.html">Regressione lineare con Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a23_reglin_4.html">Posterior Predictive Checks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_7/a30_ttest_exercises.html">Esercizi sull’inferenza frequentista</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/ccaudek/ds4psy/blob/main/docs/chapter_4/31_kl.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapter_4/31_kl.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>La divergenza di Kullback-Leibler</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#perdita-di-informazione-e-divergenza-di-kullback-leibler">Perdita di Informazione e Divergenza di Kullback-Leibler</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#un-esempio-empirico">Un Esempio Empirico</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#la-divergenza-dipende-dalla-direzione">La Divergenza Dipende dalla Direzione</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confronto-tra-modelli-tramite-la-divergenza-mathbb-kl">Confronto tra Modelli Tramite la Divergenza <span class="math notranslate nohighlight">\(\mathbb{KL}\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distribuzione-predittiva-a-posteriori">Distribuzione Predittiva a Posteriori</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#valutazione-della-somiglianza-tra-distribuzioni">Valutazione della Somiglianza tra Distribuzioni</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#confronto-tra-modelli-multipli">Confronto tra Modelli Multipli</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expected-log-predictive-density">Expected Log Predictive Density</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approssimazione-e-stima">Approssimazione e Stima</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Un Esempio Empirico</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#un-secondo-esempio-empirico">Un Secondo Esempio Empirico</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#una-singola-osservazione">Una Singola Osservazione</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tutte-le-osservazioni-del-campione">Tutte le Osservazioni del Campione</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#leave-one-out-cross-validation-loo-cv">Leave One Out Cross-Validation (LOO-CV)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#valori-diagnostici-pareto-k">Valori Diagnostici Pareto <span class="math notranslate nohighlight">\(k\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ruolo-dell-elpd-nella-valutazione-comparativa-dei-modelli">Ruolo dell’ELPD nella Valutazione Comparativa dei Modelli</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simulazione">Simulazione</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#commenti-e-considerazioni-finali">Commenti e Considerazioni Finali</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#watermark">Watermark</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <p><a target="_blank" rel="noopener noreferrer" href="https://colab.research.google.com/github/ccaudek/ds4psy_2023/blob/main/kl.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<section class="tex2jax_ignore mathjax_ignore" id="la-divergenza-di-kullback-leibler">
<span id="kl-notebook"></span><h1>La divergenza di Kullback-Leibler<a class="headerlink" href="#la-divergenza-di-kullback-leibler" title="Permalink to this heading">#</a></h1>
<p>Nel campo della statistica e del machine learning, ci si trova spesso di fronte alla necessità di utilizzare una distribuzione di probabilità, che indichiamo con <span class="math notranslate nohighlight">\( q \)</span>, come approssimazione di un’altra distribuzione più complessa o sconosciuta, indicata con <span class="math notranslate nohighlight">\( p \)</span>. Questa necessità emerge in modo particolare quando trattare direttamente con la distribuzione <span class="math notranslate nohighlight">\( p \)</span> risulta impraticabile a causa della sua complessità o dell’incertezza sulle sue caratteristiche. Di conseguenza, emerge una domanda fondamentale: quanta informazione perdiamo utilizzando la distribuzione <span class="math notranslate nohighlight">\( q \)</span> in luogo di <span class="math notranslate nohighlight">\( p \)</span>, e come questa scelta influisce sull’incertezza nei nostri risultati analitici?</p>
<p>Per rispondere a queste domande, ci affidiamo a un indicatore chiave noto come <em>Divergenza di Kullback-Leibler</em> (KL), introdotto nel capitolo precedente. Questa metrica, denotata come <span class="math notranslate nohighlight">\( \mathbb{KL}(p \mid\mid q) \)</span>, misura la discrepanza tra le due distribuzioni di probabilità. Un valore di zero indica che le distribuzioni <span class="math notranslate nohighlight">\( q \)</span> e <span class="math notranslate nohighlight">\( p \)</span> sono identiche, mentre valori maggiori di zero riflettono discrepanze crescenti tra le due. In questo senso, la Divergenza KL funge da misura quantitativa per valutare la “vicinanza” o somiglianza tra distribuzioni.</p>
<p>Parallelamente, l’<em>Expected Log Predictive Density</em> (elpd) rappresenta un altro strumento fondamentale nell’ambito della valutazione dei modelli statistici. Questa misura valuta l’adeguatezza di un modello statistico rispetto ai dati osservati, fornendo un equilibrio tra la capacità del modello di adattarsi ai dati e la sua complessità. L’elpd si rivela particolarmente efficace nel confrontare diversi modelli statistici, offrendo uno strumento per bilanciare adattamento e complessità.</p>
<p>Il focus di questo capitolo sarà dunque su queste due metriche fondamentali: la Divergenza di Kullback-Leibler e l’elpd. Approfondiremo la loro rilevanza nella valutazione e nel confronto di modelli statistici, con un’attenzione particolare al contesto bayesiano. Con una conoscenza approfondita di questi strumenti, il lettore sarà in grado di applicare efficacemente queste misure nella selezione e nell’analisi di modelli statistici, adeguandoli a specifici set di dati.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">stats</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">from</span> <span class="nn">statsmodels.graphics</span> <span class="kn">import</span> <span class="n">tsaplots</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">pymc.sampling_jax</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>
<span class="kn">from</span> <span class="nn">scipy.integrate</span> <span class="kn">import</span> <span class="n">quad</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/corrado/opt/anaconda3/envs/pymc_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;retina&#39;
<span class="n">RANDOM_SEED</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">RANDOM_SEED</span><span class="p">)</span>
<span class="n">az</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;arviz-darkgrid&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="perdita-di-informazione-e-divergenza-di-kullback-leibler">
<h2>Perdita di Informazione e Divergenza di Kullback-Leibler<a class="headerlink" href="#perdita-di-informazione-e-divergenza-di-kullback-leibler" title="Permalink to this heading">#</a></h2>
<p>La <em>Divergenza di Kullback-Leibler</em> (KL), conosciuta anche come entropia relativa, è uno strumento statistico fondamentale per quantificare la quantità di informazione che si “perde” quando una distribuzione di probabilità vera, indicata con <span class="math notranslate nohighlight">\( p \)</span>, viene sostituita da una distribuzione approssimata, <span class="math notranslate nohighlight">\( q \)</span>. Matematicamente, la Divergenza KL si definisce come l’aspettativa del logaritmo del rapporto tra le distribuzioni <span class="math notranslate nohighlight">\( p \)</span> e <span class="math notranslate nohighlight">\( q \)</span>, ponderata secondo la distribuzione vera <span class="math notranslate nohighlight">\( p \)</span>. La sua formula è la seguente:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{KL} (p \mid\mid q) = \sum_i^n p_i (\log p_i - \log q_i),
\]</div>
<p>dove l’indice <span class="math notranslate nohighlight">\(i\)</span> si estende a tutti gli elementi possibili nelle distribuzioni. Questa misura ci offre una valutazione della discrepanza media nelle probabilità logaritmiche quando <span class="math notranslate nohighlight">\( q \)</span> viene usata per approssimare <span class="math notranslate nohighlight">\( p \)</span>.</p>
<p>La Divergenza di Kullback-Leibler può essere anche vista in termini di entropia, ossia come la differenza tra l’entropia incrociata, denotata con <span class="math notranslate nohighlight">\( h(p, q) \)</span>, e l’entropia della distribuzione vera, <span class="math notranslate nohighlight">\( h(p) \)</span>. Le formule per queste due misure di entropia sono:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
h(p) &amp; = -\sum_i^n p_i \log p_i, \\
h(p, q) &amp; = -\sum_i^n p_i \log q_i.
\end{align*}
\end{split}\]</div>
<p>In questo contesto, <span class="math notranslate nohighlight">\( h(p) \)</span> rappresenta l’incertezza intrinseca associata alla distribuzione vera <span class="math notranslate nohighlight">\( p \)</span>, mentre <span class="math notranslate nohighlight">\( h(p, q) \)</span> indica l’incertezza che deriva dall’utilizzo delle probabilità logaritmiche di <span class="math notranslate nohighlight">\( q \)</span>, calcolate però usando le probabilità di <span class="math notranslate nohighlight">\( p \)</span>.</p>
<p>Di conseguenza, la formula della Divergenza KL può essere riformulata come:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{KL} (p \mid\mid q) = h(p, q) - h(p).
\]</div>
<p>In sintesi, la Divergenza KL ci permette di misurare la differenza di incertezza tra l’uso di <span class="math notranslate nohighlight">\( q \)</span> come approssimazione di <span class="math notranslate nohighlight">\( p \)</span> e l’incertezza originale di <span class="math notranslate nohighlight">\( p \)</span>. È un indicatore cruciale di quanto accuratamente <span class="math notranslate nohighlight">\( q \)</span> riesca a approssimare <span class="math notranslate nohighlight">\( p \)</span>, quantificando la perdita di informazione associata a questa approssimazione.</p>
<section id="un-esempio-empirico">
<h3>Un Esempio Empirico<a class="headerlink" href="#un-esempio-empirico" title="Permalink to this heading">#</a></h3>
<p>Per comprendere meglio questi concetti, esaminiamo ora un esempio pratico in Python, dove definiremo due distribuzioni di probabilità discrete, <span class="math notranslate nohighlight">\(p\)</span> e <span class="math notranslate nohighlight">\(q\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Calcoliamo l’entropia di <span class="math notranslate nohighlight">\(p\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">h_p</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Entropia di p: &quot;</span><span class="p">,</span> <span class="n">h_p</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Entropia di p:  1.4854752972273344
</pre></div>
</div>
</div>
</div>
<p>Calcoliamo l’entropia incrociata tra <span class="math notranslate nohighlight">\(p\)</span> e <span class="math notranslate nohighlight">\(q\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">h_pq</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">q</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Entropia incrociata tra p e q: &quot;</span><span class="p">,</span> <span class="n">h_pq</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Entropia incrociata tra p e q:  1.5539580943104374
</pre></div>
</div>
</div>
</div>
<p>Calcoliamo la divergenza di Kullback-Leibler da <span class="math notranslate nohighlight">\(p\)</span> a <span class="math notranslate nohighlight">\(q\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kl_pq</span> <span class="o">=</span> <span class="n">h_pq</span> <span class="o">-</span> <span class="n">h_p</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Divergenza KL da p a q: &quot;</span><span class="p">,</span> <span class="n">kl_pq</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Divergenza KL da p a q:  0.06848279708310301
</pre></div>
</div>
</div>
</div>
<p>Consideriamo un secondo esempio. Sia <span class="math notranslate nohighlight">\(p\)</span> una distribuzione binomiale di parametri <span class="math notranslate nohighlight">\(\theta = 0.2\)</span> e <span class="math notranslate nohighlight">\(n = 5\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the parameters</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="c1"># Compute the probability mass function</span>
<span class="n">true_py</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">true_py</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.4096 0.4096 0.1536 0.0256 0.0016]
</pre></div>
</div>
</div>
</div>
<p>Sia <span class="math notranslate nohighlight">\(q_1\)</span> una approssimazione a <span class="math notranslate nohighlight">\(p\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">q1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.46</span><span class="p">,</span> <span class="mf">0.42</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">q1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.46 0.42 0.1  0.01 0.01]
</pre></div>
</div>
</div>
</div>
<p>Sia <span class="math notranslate nohighlight">\(q_2\)</span> una distribuzione uniforme:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">q2</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">]</span> <span class="o">*</span> <span class="mi">5</span>
<span class="nb">print</span><span class="p">(</span><span class="n">q2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.2, 0.2, 0.2, 0.2, 0.2]
</pre></div>
</div>
</div>
</div>
<p>La divergenza <span class="math notranslate nohighlight">\(\mathbb{KL}\)</span> di <span class="math notranslate nohighlight">\(q_1\)</span> da <span class="math notranslate nohighlight">\(p\)</span> è</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kl_pq1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">true_py</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">true_py</span> <span class="o">/</span> <span class="n">q1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Divergenza KL da p a q1: &quot;</span><span class="p">,</span> <span class="n">kl_pq1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Divergenza KL da p a q1:  0.029251990333458974
</pre></div>
</div>
</div>
</div>
<p>La divergenza <span class="math notranslate nohighlight">\(\mathbb{KL}\)</span> di <span class="math notranslate nohighlight">\(q_2\)</span> da <span class="math notranslate nohighlight">\(p\)</span> è:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kl_pq2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">true_py</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">true_py</span> <span class="o">/</span> <span class="n">q2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Divergenza KL da p a q2: &quot;</span><span class="p">,</span> <span class="n">kl_pq2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Divergenza KL da p a q2:  0.4863577787141543
</pre></div>
</div>
</div>
</div>
<p>È chiaro che perdiamo una quantità maggiore di informazioni se, per descrivere la distribuzione binomiale <span class="math notranslate nohighlight">\(p\)</span>, usiamo la distribuzione uniforme <span class="math notranslate nohighlight">\(q_2\)</span> anziché <span class="math notranslate nohighlight">\(q_1\)</span>.</p>
</section>
<section id="la-divergenza-dipende-dalla-direzione">
<h3>La Divergenza Dipende dalla Direzione<a class="headerlink" href="#la-divergenza-dipende-dalla-direzione" title="Permalink to this heading">#</a></h3>
<p>La Divergenza <span class="math notranslate nohighlight">\( \mathbb{KL} \)</span> può essere vista come una misura di “distanza” tra due distribuzioni di probabilità, ma è importante sottolineare che non è una vera e propria distanza in senso matematico poiché non è simmetrica. Questa asimmetria riflette il fatto che sostituire <span class="math notranslate nohighlight">\(p\)</span> con <span class="math notranslate nohighlight">\(q\)</span> non è equivalente a sostituire <span class="math notranslate nohighlight">\(q\)</span> con <span class="math notranslate nohighlight">\(p\)</span> in termini di perdita di informazione.</p>
<p>La relazione tra la Divergenza <span class="math notranslate nohighlight">\( \mathbb{KL} \)</span>, l’entropia <span class="math notranslate nohighlight">\(h(p)\)</span> e l’entropia incrociata <span class="math notranslate nohighlight">\(h(p, q)\)</span> è cruciale per comprendere come la <span class="math notranslate nohighlight">\( \mathbb{KL} \)</span> quantifichi la perdita di informazione. L’entropia di <span class="math notranslate nohighlight">\(p\)</span> rappresenta l’incertezza intrinseca nella distribuzione vera, mentre l’entropia incrociata tra <span class="math notranslate nohighlight">\(p\)</span> e <span class="math notranslate nohighlight">\(q\)</span> rappresenta l’incertezza quando si utilizza la distribuzione approssimata <span class="math notranslate nohighlight">\(q\)</span> per rappresentare <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p>Illustriamo numericamente questi concetti per due distribuzioni <span class="math notranslate nohighlight">\(p = \{0.01, 0.99\}\)</span> e <span class="math notranslate nohighlight">\(q = \{0.7, 0.3\}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Definire le distribuzioni p e q</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">])</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span>

<span class="c1"># Calcolo dell&#39;entropia di p</span>
<span class="n">h_p</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>

<span class="c1"># Calcolo dell&#39;entropia incrociata da p a q</span>
<span class="n">h_pq</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">q</span><span class="p">))</span>

<span class="c1"># Calcolo della divergenza KL da p a q</span>
<span class="n">kl_pq</span> <span class="o">=</span> <span class="n">h_pq</span> <span class="o">-</span> <span class="n">h_p</span>

<span class="c1"># Calcolo dell&#39;entropia di q</span>
<span class="n">h_q</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">q</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">q</span><span class="p">))</span>

<span class="c1"># Calcolo dell&#39;entropia incrociata da q a p</span>
<span class="n">h_qp</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">q</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>

<span class="c1"># Calcolo della divergenza KL da q a p</span>
<span class="n">kl_qp</span> <span class="o">=</span> <span class="n">h_qp</span> <span class="o">-</span> <span class="n">h_q</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Entropia di p: </span><span class="si">{</span><span class="n">h_p</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Entropia incrociata da p a q: </span><span class="si">{</span><span class="n">h_pq</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Divergenza KL da p a q: </span><span class="si">{</span><span class="n">kl_pq</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Entropia di q: </span><span class="si">{</span><span class="n">h_q</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Entropia incrociata da q a p: </span><span class="si">{</span><span class="n">h_qp</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Divergenza KL da q a p: </span><span class="si">{</span><span class="n">kl_qp</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Entropia di p: 0.056001534354847345
Entropia incrociata da p a q: 1.1954998257220641
Divergenza KL da p a q: 1.1394982913672167

Entropia di q: 0.6108643020548935
Entropia incrociata da q a p: 3.226634230947714
Divergenza KL da q a p: 2.6157699288928207
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="confronto-tra-modelli-tramite-la-divergenza-mathbb-kl">
<h2>Confronto tra Modelli Tramite la Divergenza <span class="math notranslate nohighlight">\(\mathbb{KL}\)</span><a class="headerlink" href="#confronto-tra-modelli-tramite-la-divergenza-mathbb-kl" title="Permalink to this heading">#</a></h2>
<p>La divergenza <span class="math notranslate nohighlight">\(\mathbb{KL}\)</span> è uno strumento utilizzato per confrontare diversi modelli probabilistici. Essa quantifica la discrepanza tra la distribuzione di probabilità generata da un modello ipotetico <span class="math notranslate nohighlight">\(p_{\mathcal{M}}\)</span> e quella del modello che effettivamente ha generato i dati, <span class="math notranslate nohighlight">\(p_t\)</span>.</p>
<section id="distribuzione-predittiva-a-posteriori">
<h3>Distribuzione Predittiva a Posteriori<a class="headerlink" href="#distribuzione-predittiva-a-posteriori" title="Permalink to this heading">#</a></h3>
<p>Abbiamo precedentemente introdotto il concetto di distribuzione predittiva a posteriori, formulata come segue:</p>
<div class="math notranslate nohighlight">
\[
p(\tilde{y} \mid y) = \int_\Theta p(\tilde{y} \mid \theta) p(\theta \mid y) \, \mathrm{d}\theta .
\]</div>
<p>Questa distribuzione riflette il tipo di dati che ci aspettiamo siano generati dal modello <span class="math notranslate nohighlight">\(\mathcal{M}\)</span>, considerando sia le nostre credenze a priori <span class="math notranslate nohighlight">\(p(\theta)\)</span> sia i dati effettivamente osservati <span class="math notranslate nohighlight">\(y\)</span>.</p>
</section>
<section id="valutazione-della-somiglianza-tra-distribuzioni">
<h3>Valutazione della Somiglianza tra Distribuzioni<a class="headerlink" href="#valutazione-della-somiglianza-tra-distribuzioni" title="Permalink to this heading">#</a></h3>
<p>L’obiettivo è di valutare quanto bene la distribuzione <span class="math notranslate nohighlight">\(q_{\mathcal{M}} = p(\tilde{y} \mid y)\)</span> approssimi la distribuzione del modello generatore reale <span class="math notranslate nohighlight">\(p_t(\tilde{y})\)</span>. In altre parole, vogliamo quantificare quanto siano “simili” i dati generati dal modello ipotetico <span class="math notranslate nohighlight">\(q_{\mathcal{M}}\)</span> rispetto a quelli generati dal modello reale <span class="math notranslate nohighlight">\(p_t\)</span>. La misura di questa somiglianza è fornita dalla divergenza di Kullback-Leibler:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{KL}(p_t \mid\mid q_{\mathcal{M}}).
\]</div>
</section>
<section id="confronto-tra-modelli-multipli">
<h3>Confronto tra Modelli Multipli<a class="headerlink" href="#confronto-tra-modelli-multipli" title="Permalink to this heading">#</a></h3>
<p>Immaginiamo di avere <span class="math notranslate nohighlight">\(k\)</span> modelli distinti <span class="math notranslate nohighlight">\( \{q_{\mathcal{M}_1}, q_{\mathcal{M}_2}, \ldots, q_{\mathcal{M}_k}\} \)</span>. Se conoscessimo <span class="math notranslate nohighlight">\(p_t\)</span>, potremmo calcolare la divergenza di Kullback-Leibler per ogni modello come segue:</p>
<div class="math notranslate nohighlight" id="equation-eq-kl-mod-comp">
<span class="eqno">(66)<a class="headerlink" href="#equation-eq-kl-mod-comp" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align*}
\mathbb{KL} (p_t \mid\mid q_{\mathcal{M}_1}) &amp;= \mathbb{E} (\log p_{t}) - \mathbb{E} (\log q_{\mathcal{M}_1}) \\
\mathbb{KL} (p_t \mid\mid q_{\mathcal{M}_2}) &amp;= \mathbb{E} (\log p_{t}) - \mathbb{E} (\log q_{\mathcal{M}_2}) \\
&amp;\vdots \\
\mathbb{KL} (p_t \mid\mid q_{\mathcal{M}_k}) &amp;= \mathbb{E} (\log p_{t}) - \mathbb{E} (\log q_{\mathcal{M}_k}) .
\end{align*}
\end{split}\]</div>
<p>Anche se nella pratica <span class="math notranslate nohighlight">\(p_t\)</span> è sconosciuta, notiamo che essa è costante in tutti i termini della divergenza. Di conseguenza, possiamo confrontare i modelli focalizzandoci sul secondo termine della divergenza di Kullback-Leibler, che è indipendente da <span class="math notranslate nohighlight">\(p_t\)</span>. Per un modello generico <span class="math notranslate nohighlight">\(\mathcal{M}\)</span>, questo termine può essere espresso come:</p>
<div class="math notranslate nohighlight" id="equation-eq-kl-div-cont-t2">
<span class="eqno">(67)<a class="headerlink" href="#equation-eq-kl-div-cont-t2" title="Permalink to this equation">#</a></span>\[
\mathbb{E} \log p_{\mathcal{M}}(y) = \int_{-\infty}^{+\infty} p_{t}(y) \log p_{\mathcal{M}}(y) \, \mathrm{d}y .
\]</div>
<p>In questo modo, siamo in grado di costruire un criterio di confronto tra modelli che non richiede la conoscenza del modello generatore reale dei dati.</p>
<p>In conclusione, l’eq. <a class="reference internal" href="#equation-eq-kl-div-cont-t2">(67)</a> esprime la densità logaritmica media della distribuzione <span class="math notranslate nohighlight">\(p_{\mathcal{M}}(y)\)</span> secondo la distribuzione vera <span class="math notranslate nohighlight">\(p_t(y)\)</span>. In altre parole, questa equazione ci dice quanto bene il modello <span class="math notranslate nohighlight">\(\mathcal{M}\)</span> può rappresentare i dati veri generati da <span class="math notranslate nohighlight">\(p_t\)</span>.</p>
</section>
</section>
<section id="expected-log-predictive-density">
<h2>Expected Log Predictive Density<a class="headerlink" href="#expected-log-predictive-density" title="Permalink to this heading">#</a></h2>
<p>Dopo aver discusso l’equazione <a class="reference internal" href="#equation-eq-kl-div-cont-t2">(67)</a>, possiamo riformularla come segue:</p>
<div class="math notranslate nohighlight" id="equation-eq-elpd">
<span class="eqno">(68)<a class="headerlink" href="#equation-eq-elpd" title="Permalink to this equation">#</a></span>\[
\begin{equation}
elpd = \int_{\tilde{y}} p_{t}(\tilde{y}) \log p(\tilde{y} \mid y) \, \mathrm{d}\tilde{y} .
\end{equation}
\]</div>
<p>L’equazione <a class="reference internal" href="#equation-eq-elpd">(68)</a> è conosciuta come <em>Expected Log Predictive Density</em> (elpd). L’ELPD è una metrica chiave per valutare la capacità di un modello di generare previsioni accurate. È particolarmente utile in contesti dove l’obiettivo è la predizione di nuovi dati, piuttosto che semplicemente l’adattamento ai dati osservati.</p>
<p>Esaminiamo con attenzione le differenze tra la ELPD e l’eq. <a class="reference internal" href="#equation-eq-kl-div-cont-t2">(67)</a>.</p>
<ol class="arabic simple">
<li><p><strong>Variabile di Interesse</strong>: Nell’eq. <a class="reference internal" href="#equation-eq-kl-div-cont-t2">(67)</a>, <span class="math notranslate nohighlight">\(y\)</span> rappresenta i dati osservati. Nell’eq. <a class="reference internal" href="#equation-eq-elpd">(68)</a>, invece, <span class="math notranslate nohighlight">\(\tilde{y}\)</span> rappresenta potenziali nuovi dati che potrebbero essere osservati in futuro.</p></li>
<li><p><strong>Modello di Probabilità</strong>: Nell’eq. <a class="reference internal" href="#equation-eq-kl-div-cont-t2">(67)</a>, <span class="math notranslate nohighlight">\(p_{\mathcal{M}}(y) \)</span> rappresenta il modello probabilitico basato sui dati osservati. Nell’eq. <a class="reference internal" href="#equation-eq-elpd">(68)</a>, invece, <span class="math notranslate nohighlight">\(p(\tilde{y} \mid y)\)</span> è la distribuzione predittiva a posteriori, che rappresenta le previsioni del modello per nuovi dati, date le osservazioni precedenti <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
<li><p><strong>Ruolo di <span class="math notranslate nohighlight">\(p_t\)</span></strong>: In entrambe le formule, <span class="math notranslate nohighlight">\(p_t\)</span> rappresenta la “vera” distribuzione dei dati. Nella prima formula, essa valuta quanto accuratamente il modello rappresenta i dati effettivamente osservati, mentre nella seconda pesa la qualità delle previsioni del modello per nuovi dati.</p></li>
</ol>
<p>Quindi, mentre l’eq. <a class="reference internal" href="#equation-eq-kl-div-cont-t2">(67)</a> fornisce una misura di quanto bene il modello si adatta ai dati esistenti, l’eq. <a class="reference internal" href="#equation-eq-elpd">(68)</a> valuta la qualità delle previsioni del modello per dati futuri. Entrambe le formule sono collegate al concetto di divergenza <span class="math notranslate nohighlight">\(\mathbb{KL}\)</span>, ma sono applicate in contesti diversi: la prima per la valutazione dell’adattamento del modello ai dati osservati, la seconda per la sua capacità predittiva.</p>
<section id="approssimazione-e-stima">
<h3>Approssimazione e Stima<a class="headerlink" href="#approssimazione-e-stima" title="Permalink to this heading">#</a></h3>
<p>La stima dell’Expected Log Predictive Density (ELPD) si confronta con una sfida fondamentale: la distribuzione di probabilità “vera” dei dati, indicata come <span class="math notranslate nohighlight">\(p_t\)</span>, è generalmente sconosciuta. Questa incognita rende impossibile calcolare l’ELPD in modo esatto. Tuttavia, possiamo avvicinarci a una stima affidabile di questa metrica attraverso l’impiego di specifiche tecniche e metodi.</p>
<p>Nonostante la mancanza di una formulazione analitica precisa per la distribuzione predittiva a posteriori <span class="math notranslate nohighlight">\(p(\tilde{y} \mid y)\)</span>, è possibile approssimarla efficacemente tramite metodi di simulazione. Tuttavia, è importante notare che l’eq. <a class="reference internal" href="#equation-eq-elpd">(68)</a> include il termine <span class="math notranslate nohighlight">\(p_t(\tilde{y})\)</span>, che rappresenta la distribuzione dei potenziali dati futuri secondo il “vero” modello generatore. Poiché <span class="math notranslate nohighlight">\(p_t\)</span> è in pratica ignoto, l’elpd non può essere calcolata esattamente ma può solo essere stimata. Il prossimo punto di discussione in questo capitolo riguarderà le strategie per stimare efficacemente l’ELPD utilizzando un campione di osservazioni.</p>
</section>
<section id="id1">
<h3>Un Esempio Empirico<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<p>Per comprenderne meglio il funzionamento dell’ELPD, esaminiamo un esempio tratto dal testo <a class="reference external" href="https://vasishth.github.io/bayescogsci/book/expected-log-predictive-density-of-a-model.html">Bayesian Data Analysis for Cognitive Science</a>, in cui questa quantità viene calcolata sia in forma esatta che approssimata.</p>
<p>Supponiamo di avere un campione di <span class="math notranslate nohighlight">\(n\)</span> osservazioni e di conoscere il vero processo generativo dei dati, rappresentato dalla distribuzione <span class="math notranslate nohighlight">\(p_t(y) = Beta(1, 3).\)</span> Da questo campione, generato in modo artificiale, ad esempio tramite</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">75</span><span class="p">)</span>

<span class="c1"># Number of samples</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="c1"># Draw samples from a Beta distribution</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_data</span><span class="p">[:</span><span class="mi">6</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.21810354 0.05532253 0.13545025 0.42102514 0.14728608 0.11052926]
</pre></div>
</div>
</div>
</div>
<p>abbiamo adattato un modello bayesiano <span class="math notranslate nohighlight">\(\mathcal{M}\)</span> e ottenuto la distribuzione a posteriori per i parametri del modello descritta da <span class="math notranslate nohighlight">\(p(y^{rep} \mid y) \sim Beta(2, 2).\)</span></p>
<p>L’ELPD si calcola con l’integrazione:</p>
<div class="math notranslate nohighlight">
\[
\text{ELPD} = \int_{y^{rep}}p_{t}(y^{rep})\log p(y^{rep} \mid y) \,\operatorname {d}\!y^{rep}.
\]</div>
<p>Svolgendo i calcoli otteniamo:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># True distribution</span>
<span class="k">def</span> <span class="nf">p_t</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="c1"># Predictive distribution</span>
<span class="k">def</span> <span class="nf">p</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Integration</span>
<span class="k">def</span> <span class="nf">integrand</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">p_t</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>

<span class="c1"># Perform numerical integration</span>
<span class="n">result</span><span class="p">,</span> <span class="n">error</span> <span class="o">=</span> <span class="n">quad</span><span class="p">(</span><span class="n">integrand</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-0.37490719743844486
</pre></div>
</div>
</div>
</div>
<p>In pratica, non conoscendo <span class="math notranslate nohighlight">\(p_t(y)\)</span>, approssimiamo l’ELPD usando l’eq. <a class="reference internal" href="#equation-eq-elpd">(68)</a>:</p>
<div class="math notranslate nohighlight" id="equation-eq-empirical-elpd">
<span class="eqno">(69)<a class="headerlink" href="#equation-eq-empirical-elpd" title="Permalink to this equation">#</a></span>\[
\frac{1}{n} \sum_{i=1}^n \log p(y_i \mid y).
\]</div>
<p>Questo metodo è un’approssimazione Monte Carlo basata sui dati osservati, usando <span class="math notranslate nohighlight">\(\{y_1, y_2, \ldots, y_n\}\)</span> come rappresentazione empirica di possibili futuri dati.</p>
<p>Esaminiamo in dettaglio le componenti dell’eq. <a class="reference internal" href="#equation-eq-empirical-elpd">(69)</a>:</p>
<ol class="arabic simple">
<li><p><strong>Densità Logaritmica Predittiva <span class="math notranslate nohighlight">\( \log p(y_i \mid y) \)</span> per un dato <span class="math notranslate nohighlight">\( y_i \)</span></strong>: Questa misura valuta quanto efficacemente il modello predice un singolo dato <span class="math notranslate nohighlight">\(y_i\)</span>, data la distribuzione osservata <span class="math notranslate nohighlight">\(y\)</span>. Valori più alti indicano una migliore performance predittiva.</p></li>
<li><p><strong>Media della Densità Logaritmica Predittiva</strong>: La media di questi valori logaritmici predittivi fornisce una stima complessiva dell’efficacia predittiva del modello sull’intero set di dati.</p></li>
</ol>
<p>Nel caso dell’esempio in discussione, usando l’eq. <a class="reference internal" href="#equation-eq-empirical-elpd">(69)</a> otteniamo un valore approssimato a quello trovato in precedenza:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="mi">1</span> <span class="o">/</span> <span class="n">n</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">y_data</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-0.3721938430299501
</pre></div>
</div>
</div>
</div>
<p>L’ELPD funge da misura complessiva dell’efficacia di un modello nel prevedere dati non ancora osservati. Un valore elevato dell’ELPD suggerisce una maggiore efficacia del modello in termini di previsioni accurate. Tuttavia, è essenziale riconoscere che l’ELPD è una stima basata sui dati attualmente disponibili; la sua affidabilità può essere compromessa se i dati futuri si discostano significativamente da quelli su cui si basa la stima.</p>
</section>
<section id="un-secondo-esempio-empirico">
<h3>Un Secondo Esempio Empirico<a class="headerlink" href="#un-secondo-esempio-empirico" title="Permalink to this heading">#</a></h3>
<p>Generiamo un set di dati artificiali seguendo una distribuzione normale:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([3.64467434, 7.4467137 , 3.07761966, 7.50059139, 4.2730767 ,
       4.78830081, 7.12094138, 4.13637857, 6.53285326, 3.34258292])
</pre></div>
</div>
</div>
</div>
<p>Adattiamo un modello normale con media e varianza sconosciute ai dati e generiamo la distribuzione predittiva a posteriori:</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input above-output-prompt docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;mu&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;sigma&quot;</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
    <span class="c1"># Sampling from the posterior</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sampling_jax</span><span class="o">.</span><span class="n">sample_numpyro_nuts</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
    <span class="c1"># Generating posterior predictive samples</span>
    <span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span>
        <span class="n">trace</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<details class="hide below-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell output</span>
<span class="expanded">Hide code cell output</span>
</summary>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Compiling...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Compilation time = 0:00:01.032192
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sampling...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|                                                                                          | 0/2000 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Compiling.. :   0%|                                                                            | 0/2000 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|                                                                                          | 0/2000 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Compiling.. :   0%|                                                                            | 0/2000 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|                                                                                          | 0/2000 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Compiling.. :   0%|                                                                            | 0/2000 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|                                                                                          | 0/2000 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Compiling.. :   0%|                                                                            | 0/2000 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running chain 1:   0%|                                                                         | 0/2000 [00:01&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running chain 2:   0%|                                                                         | 0/2000 [00:01&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running chain 0:   0%|                                                                         | 0/2000 [00:01&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running chain 3:   0%|                                                                         | 0/2000 [00:01&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running chain 0: 100%|████████████████████████████████████████████████████████████| 2000/2000 [00:01&lt;00:00, 1464.26it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running chain 1: 100%|████████████████████████████████████████████████████████████| 2000/2000 [00:01&lt;00:00, 1465.42it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running chain 2: 100%|████████████████████████████████████████████████████████████| 2000/2000 [00:01&lt;00:00, 1466.81it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running chain 3: 100%|████████████████████████████████████████████████████████████| 2000/2000 [00:01&lt;00:00, 1468.20it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sampling time = 0:00:01.757659
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Transforming variables...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Transformation time = 0:00:00.055507
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sampling: [y]
</pre></div>
</div>
<div class="output text_html">
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div><div class="output text_html">
    <div>
      <progress value='4000' class='' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>
      100.00% [4000/4000 00:00&lt;00:00]
    </div>
    </div></div>
</details>
</div>
<section id="una-singola-osservazione">
<h4>Una Singola Osservazione<a class="headerlink" href="#una-singola-osservazione" title="Permalink to this heading">#</a></h4>
<p>Estraiamo la distribuzione predittiva a posteriori per la variabile <code class="docutils literal notranslate"><span class="pre">y</span></code> e rappresentiamo graficamente questa distribuzione per la prima osservazione del campione:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">post_pred</span> <span class="o">=</span> <span class="n">ppc</span><span class="o">.</span><span class="n">posterior_predictive</span>
<span class="n">y_i_post_pred</span> <span class="o">=</span> <span class="n">post_pred</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">y_i_post_pred</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Posterior Predictive Check for the First Observation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Value&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Frequency&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;Observed Data&quot;</span><span class="p">,</span> <span class="s2">&quot;Posterior Predictive&quot;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2a46561b23a329570a19e466fac319e1643330be2a6965a6ff9417589c97601a.png" src="../_images/2a46561b23a329570a19e466fac319e1643330be2a6965a6ff9417589c97601a.png" />
</div>
</div>
<p>Per calcolare il logaritmo della densità predittiva per la prima osservazione, valutiamo la PDF per questa osservazione utilizzando ogni coppia di parametri <span class="math notranslate nohighlight">\(\mu\)</span> e <span class="math notranslate nohighlight">\(\sigma\)</span> dalla distribuzione posteriore.</p>
<p>Selezione della prima osservazione:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_i</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">y_i</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3.644674340592355
</pre></div>
</div>
</div>
</div>
<p>Estrazione dei parametri dal campione posteriore:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mu_samples</span> <span class="o">=</span> <span class="n">trace</span><span class="o">.</span><span class="n">posterior</span><span class="p">[</span><span class="s2">&quot;mu&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">mu_samples</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([5.19830164, 5.24275865, 5.15480685, 4.95263206, 5.11349401,
       4.76912106, 4.79233227, 4.87566573, 4.69998245, 4.9544055 ])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sigma_samples</span> <span class="o">=</span> <span class="n">trace</span><span class="o">.</span><span class="n">posterior</span><span class="p">[</span><span class="s2">&quot;sigma&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">sigma_samples</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([2.17072681, 2.18317519, 2.1315437 , 2.03292531, 1.99572155,
       2.07992623, 2.09496957, 2.41966965, 1.98206425, 2.1169505 ])
</pre></div>
</div>
</div>
</div>
<p>Calcolo della densità di probabilità per ogni coppia di parametri:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pdf_values</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">y_i</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span> <span class="k">for</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">mu_samples</span><span class="p">,</span> <span class="n">sigma_samples</span><span class="p">)</span>
<span class="p">]</span>

<span class="n">pdf_values</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.14225609017175758,
 0.1397876375045475,
 0.1456208268370461,
 0.15955162440187234,
 0.15247103739636142]
</pre></div>
</div>
</div>
</div>
<p>Calcolo del logaritmo delle densità:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">log_pdf_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pdf_values</span><span class="p">)</span>
<span class="n">log_pdf_values</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-1.95012639, -1.96763088, -1.92674911, -1.83538775, -1.88078062])
</pre></div>
</div>
</div>
</div>
<p>Calcolo della media dei valori logaritmici:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mean_log_density</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">log_pdf_values</span><span class="p">)</span>
<span class="n">mean_log_density</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-1.8483750430014547
</pre></div>
</div>
</div>
</div>
<p>Questo calcolo ci fornisce una stima empirica della media della densità logaritmica predittiva (ELPD) per la prima osservazione del campione.</p>
</section>
<section id="tutte-le-osservazioni-del-campione">
<h4>Tutte le Osservazioni del Campione<a class="headerlink" href="#tutte-le-osservazioni-del-campione" title="Permalink to this heading">#</a></h4>
<p>Per calcolare la media della densità logaritmica predittiva (ELPD) per tutte le osservazioni nel  campione, è necessario seguire un processo simile a quello usato per una singola osservazione, ma esteso a tutte le osservazioni nel set di dati. Il processo consiste nel calcolare la densità logaritmica predittiva per ciascuna osservazione, come visto in precedenza, e poi fare la media su tutte queste densità.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calcolo della densità logaritmica predittiva per ogni osservazione</span>
<span class="n">all_log_densities</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">y_i</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
    <span class="n">pdf_values</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">y_i</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span> <span class="k">for</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">mu_samples</span><span class="p">,</span> <span class="n">sigma_samples</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="n">log_pdf_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pdf_values</span><span class="p">)</span>
    <span class="n">mean_log_density</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">log_pdf_values</span><span class="p">)</span>
    <span class="n">all_log_densities</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_log_density</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calcolo della media su tutte le osservazioni</span>
<span class="n">overall_mean_log_density</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_log_densities</span><span class="p">)</span>
<span class="n">overall_mean_log_density</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-2.1184822919005852
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="leave-one-out-cross-validation-loo-cv">
<h2>Leave One Out Cross-Validation (LOO-CV)<a class="headerlink" href="#leave-one-out-cross-validation-loo-cv" title="Permalink to this heading">#</a></h2>
<p>In termini pratici, per la stima dell’Expected Log Predictive Density (ELPD), l’approccio più comune non si basa sulla media della densità logaritmica predittiva calcolata direttamente. Piuttosto, si preferisce utilizzare un metodo di validazione incrociata più robusto, noto come Leave One Out Cross-Validation (LOO-CV). Questa procedura si articola in diverse fasi:</p>
<ol class="arabic simple">
<li><p>Rimozione di un’osservazione dal set di dati.</p></li>
<li><p>Adattamento del modello ai dati restanti.</p></li>
<li><p>Valutazione della predizione per l’osservazione esclusa.</p></li>
<li><p>Ripetizione del processo per ciascuna osservazione nel dataset, calcolando la media dei valori logaritmici predittivi per arrivare alla stima LOO-CV dell’ELPD.</p></li>
</ol>
<p>Sia la media della LPD che la LOO-CV si focalizzano sulla valutazione della capacità predittiva di un modello, ma differiscono nel loro approccio:</p>
<ul class="simple">
<li><p><strong>Media della LPD</strong>: Questa misura valuta la capacità predittiva del modello sull’intero dataset, basandosi sulla probabilità complessiva dei dati osservati secondo il modello.</p></li>
<li><p><strong>LOO-CV</strong>: Invece, la LOO-CV offre una valutazione più robusta e generalizzabile della capacità predittiva, eliminando una osservazione per volta dal dataset. Questo metodo è particolarmente vantaggioso per valutare la capacità di generalizzazione del modello oltre i dati su cui è stato addestrato, fornendo una stima più realistica della sua efficacia nel predire nuovi dati.</p></li>
</ul>
<p>Esaminiamo il calcolo di LOO-CV per i dati dell’esempio precedente. Iniziamo a calcolare la log-verosimiglianza.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">pm</span><span class="o">.</span><span class="n">compute_log_likelihood</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div><div class="output text_html">
    <div>
      <progress value='4000' class='' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>
      100.00% [4000/4000 00:00&lt;00:00]
    </div>
    </div></div>
</div>
<p>Ora usiamo la funzione <code class="docutils literal notranslate"><span class="pre">loo()</span></code> del pacchetto Arviz per calcolare LOO-CV:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loo_result</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">loo</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">pointwise</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">loo_result</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Computed from 4000 posterior samples and 100 observations log-likelihood matrix.

         Estimate       SE
elpd_loo  -212.96     7.48
p_loo        2.13        -
------

Pareto k diagnostic values:
                         Count   Pct.
(-Inf, 0.5]   (good)      100  100.0%
 (0.5, 0.7]   (ok)          0    0.0%
   (0.7, 1]   (bad)         0    0.0%
   (1, Inf)   (very bad)    0    0.0%
</pre></div>
</div>
</div>
</div>
<p>Si noti che il valore assoluto di <code class="docutils literal notranslate"><span class="pre">overall_mean_log_density</span></code> (-2.286) non è direttamente confrontabile con il valore di <code class="docutils literal notranslate"><span class="pre">elpd_loo</span></code> (-226.64) a causa delle differenze nella loro scala e nel modo in cui vengono calcolati. Mentre <code class="docutils literal notranslate"><span class="pre">overall_mean_log_density</span></code> è la media del logaritmo delle densità per ogni osservazione, <code class="docutils literal notranslate"><span class="pre">elpd_loo</span></code> è una stima complessiva dell’adattamento del modello che tiene conto della validazione incrociata e della capacità predittiva. Tuttavia, in entrambi i casi, valori più alti indicano prestazioni migliori.</p>
<p>In conclusione, mentre la media della LPD fornisce una valutazione della performance del modello basata sull’intero set di dati, la LOO-CV offre una prospettiva più rigorosa e generalizzabile sulla capacità predittiva del modello. È importante notare che strumenti come PyMC e Arviz facilitano il calcolo della LOO-CV, specialmente in scenari che coinvolgono l’utilizzo di tecniche di campionamento MCMC. Questi strumenti possono semplificare notevolmente l’analisi in contesti di statistica bayesiana.</p>
<section id="valori-diagnostici-pareto-k">
<h3>Valori Diagnostici Pareto <span class="math notranslate nohighlight">\(k\)</span><a class="headerlink" href="#valori-diagnostici-pareto-k" title="Permalink to this heading">#</a></h3>
<p>Per stabilire quanto sia affidabile l’approssimazione effettuata attraverso LOO-CV si ricorre a un parametro diagnostico noto come valore Pareto <span class="math notranslate nohighlight">\( k \)</span>. Nel contesto della validazione incrociata di tipo Leave-One-Out (LOO), il valore Pareto <span class="math notranslate nohighlight">\( k \)</span> serve come un barometro per giudicare la precisione dell’approssimazione LOO. Il valore di <span class="math notranslate nohighlight">\( k \)</span> funge da indicatore della qualità dell’approssimazione e ha le seguenti interpretazioni:</p>
<ul class="simple">
<li><p><strong><span class="math notranslate nohighlight">\( k &lt; 0.5 \)</span></strong>: In questo caso, l’approssimazione è generalmente ottima e l’errore associato è trascurabile.</p></li>
<li><p><strong><span class="math notranslate nohighlight">\( 0.5 \leq k &lt; 0.7 \)</span></strong>: L’approssimazione rimane buona, sebbene con qualche motivo di cautela. Un’analisi più approfondita del modello e dei dati potrebbe essere vantaggiosa.</p></li>
<li><p><strong><span class="math notranslate nohighlight">\( 0.7 \leq k &lt; 1 \)</span></strong>: La qualità dell’approssimazione è mediocre. I risultati derivanti da LOO possono essere poco affidabili; potrebbe quindi essere utile considerare metodi di validazione alternativi o apportare modifiche al modello.</p></li>
<li><p><strong><span class="math notranslate nohighlight">\( k \geq 1 \)</span></strong>: Qui, l’approssimazione è inadeguata e i risultati da LOO sono molto probabilmente inaffidabili. Questo è un campanello d’allarme che indica potenziali problemi con il modello o con il metodo di approssimazione adottato.</p></li>
</ul>
<p>Il valore diagnostico Pareto <span class="math notranslate nohighlight">\( k \)</span> è basato sulla distribuzione di Pareto e serve a valutare in che misura le code della distribuzione delle differenze di log-verosimiglianza (tra la log-verosimiglianza del punto dati escluso e quella relativa all’intero set di dati) divergono da una distribuzione esponenziale. Un valore di <span class="math notranslate nohighlight">\( k \)</span> elevato suggerisce che le code della distribuzione sono più pesanti del previsto, indicando che l’approssimazione LOO-CV potrebbe essere imprecisa.</p>
<p>In sintesi, il valore Pareto <span class="math notranslate nohighlight">\( k \)</span> offre uno strumento pratico per verificare l’affidabilità dell’approssimazione LOO-CV. Permette inoltre ai ricercatori di identificare e indagare su potenziali problematiche relative sia al modello che alla metodologia di validazione incrociata adottata.</p>
<p>Come abbiamo visto nell’esempio precedente, i valori diagnostici Pareto <span class="math notranslate nohighlight">\(k\)</span> sono stampati dalla funzione <code class="docutils literal notranslate"><span class="pre">loo()</span></code> del pacchetto Arviz.</p>
</section>
<section id="ruolo-dell-elpd-nella-valutazione-comparativa-dei-modelli">
<h3>Ruolo dell’ELPD nella Valutazione Comparativa dei Modelli<a class="headerlink" href="#ruolo-dell-elpd-nella-valutazione-comparativa-dei-modelli" title="Permalink to this heading">#</a></h3>
<p>L’ELPD è una metrica essenziale per effettuare confronti tra vari modelli statistici. Ottenendo una stima dell’ELPD attraverso metodologie come la Validazione Incrociata Leave-One-Out (LOO-CV), si può acquisire una valutazione obiettiva dell’adeguatezza di ciascun modello rispetto ai dati a disposizione. Questo aspetto assume particolare importanza quando si è chiamati a selezionare il modello più idoneo tra diverse alternative, o quando si desidera determinare se un modello di maggiore complessità offra un migliore adattamento rispetto a uno più semplice.</p>
<p>In sintesi, l’ELPD funge da indicatore affidabile della capacità predittiva di un modello. Allo stesso tempo, la tecnica del LOO-CV fornisce un mezzo efficace per approssimare tale metrica. Grazie alla LOO-CV, gli analisti possono condurre una valutazione sia accurata che robusta delle prestazioni di differenti modelli, agevolando la decisione relativa al modello più appropriato per uno specifico contesto.</p>
<p>L’opportunità di automatizzare queste procedure di valutazione attraverso l’uso di librerie software come PyMC e Arviz rende l’intero approccio ancor più accessibile e pratico, consolidando la sua utilità come strumento per l’accurata selezione e validazione di modelli statistici.</p>
</section>
<section id="simulazione">
<h3>Simulazione<a class="headerlink" href="#simulazione" title="Permalink to this heading">#</a></h3>
<p>Per illustrare questa metodologia di confronto tra modelli utilizzando la LOO-CV, procediamo con una simulazione. Genereremo dati sintetici in cui esiste una relazione lineare tra le variabili <span class="math notranslate nohighlight">\(x\)</span> e <span class="math notranslate nohighlight">\(y\)</span>. In questo scenario, potremmo essere interessati a confrontare un modello lineare con un modello più semplice, che considera solo il termine di intercetta. Utilizzeremo la LOO-CV per stabilire quale dei due modelli si adatta meglio ai dati in questione. La stima dell’ELPD servirà come criterio quantitativo per orientare questa scelta di modello.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate synthetic data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">X</span>
<span class="n">y_obs</span> <span class="o">=</span> <span class="n">y_true</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Adattiamo ai dati un modello che rispecchia il vero meccanismo generativo dei dati.</p>
<p>Si noti che, per calcolare LOO e WAIC, ArviZ ha bisogno di accedere alla log-likelihood per ogni campione posteriore. Possiamo trovarla tramite <code class="docutils literal notranslate"><span class="pre">compute_log_likelihood()</span></code>. In alternativa, possiamo passare <code class="docutils literal notranslate"><span class="pre">idata_kwargs={&quot;log_likelihood&quot;:</span> <span class="pre">True}</span></code> a <code class="docutils literal notranslate"><span class="pre">sample()</span></code> per farla calcolare automaticamente alla fine del campionamento.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Linear model</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">linear_model</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;sigma&quot;</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">X</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y_obs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-output docutils container">
<div class="cell_input above-output-prompt docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">linear_model</span><span class="p">:</span>
    <span class="n">linear_trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sampling_jax</span><span class="o">.</span><span class="n">sample_numpyro_nuts</span><span class="p">(</span><span class="n">idata_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;log_likelihood&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
</pre></div>
</div>
</div>
<details class="hide below-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell output</span>
<span class="expanded">Hide code cell output</span>
</summary>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Compiling...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Compilation time = 0:00:00.406534
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sampling...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|                                                                                          | 0/2000 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Compiling.. :   0%|                                                                            | 0/2000 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|                                                                                          | 0/2000 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Compiling.. :   0%|                                                                            | 0/2000 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|                                                                                          | 0/2000 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Compiling.. :   0%|                                                                            | 0/2000 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|                                                                                          | 0/2000 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Compiling.. :   0%|                                                                            | 0/2000 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running chain 1:   0%|                                                                         | 0/2000 [00:01&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running chain 2:   0%|                                                                         | 0/2000 [00:01&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running chain 3:   0%|                                                                         | 0/2000 [00:01&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running chain 0:   0%|                                                                         | 0/2000 [00:01&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running chain 0: 100%|████████████████████████████████████████████████████████████| 2000/2000 [00:01&lt;00:00, 1464.16it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running chain 1: 100%|████████████████████████████████████████████████████████████| 2000/2000 [00:01&lt;00:00, 1465.04it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running chain 2: 100%|████████████████████████████████████████████████████████████| 2000/2000 [00:01&lt;00:00, 1466.25it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running chain 3: 100%|████████████████████████████████████████████████████████████| 2000/2000 [00:01&lt;00:00, 1467.53it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sampling time = 0:00:01.429689
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Transforming variables...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Transformation time = 0:00:00.036165
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Computing Log Likelihood...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Log Likelihood time = 0:00:00.226404
</pre></div>
</div>
</div>
</details>
</div>
<p>Adattiamo ora un secondo modello che non tiene conto della relazione lineare tra x e y.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Intercept model</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">intercept_model</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">alpha</span> 
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y_obs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-output docutils container">
<div class="cell_input above-output-prompt docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">intercept_model</span><span class="p">:</span>
    <span class="n">intercept_trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sampling_jax</span><span class="o">.</span><span class="n">sample_numpyro_nuts</span><span class="p">(</span><span class="n">idata_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;log_likelihood&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
</pre></div>
</div>
</div>
<details class="hide below-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell output</span>
<span class="expanded">Hide code cell output</span>
</summary>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Compiling...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Compilation time = 0:00:00.225940
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sampling...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|                                                                                          | 0/2000 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Compiling.. :   0%|                                                                            | 0/2000 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|                                                                                          | 0/2000 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Compiling.. :   0%|                                                                            | 0/2000 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|                                                                                          | 0/2000 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Compiling.. :   0%|                                                                            | 0/2000 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|                                                                                          | 0/2000 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Compiling.. :   0%|                                                                            | 0/2000 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running chain 1:   0%|                                                                         | 0/2000 [00:01&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running chain 3:   0%|                                                                         | 0/2000 [00:01&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running chain 2:   0%|                                                                         | 0/2000 [00:01&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running chain 0:   0%|                                                                         | 0/2000 [00:01&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running chain 0: 100%|████████████████████████████████████████████████████████████| 2000/2000 [00:01&lt;00:00, 1970.32it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running chain 1: 100%|████████████████████████████████████████████████████████████| 2000/2000 [00:01&lt;00:00, 1971.91it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running chain 2: 100%|████████████████████████████████████████████████████████████| 2000/2000 [00:01&lt;00:00, 1974.09it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running chain 3: 100%|████████████████████████████████████████████████████████████| 2000/2000 [00:01&lt;00:00, 1976.45it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sampling time = 0:00:01.053268
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Transforming variables...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Transformation time = 0:00:00.025166
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Computing Log Likelihood...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Log Likelihood time = 0:00:00.061378
</pre></div>
</div>
</div>
</details>
</div>
<p>Troviamo ora elpd con il metodo LOO.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">linear_loo</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">loo</span><span class="p">(</span><span class="n">linear_trace</span><span class="p">)</span>
<span class="n">linear_loo</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Computed from 4000 posterior samples and 100 observations log-likelihood matrix.

         Estimate       SE
elpd_loo  -244.45     6.90
p_loo        2.75        -
------

Pareto k diagnostic values:
                         Count   Pct.
(-Inf, 0.5]   (good)      100  100.0%
 (0.5, 0.7]   (ok)          0    0.0%
   (0.7, 1]   (bad)         0    0.0%
   (1, Inf)   (very bad)    0    0.0%
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">intercept_loo</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">loo</span><span class="p">(</span><span class="n">intercept_trace</span><span class="p">)</span>
<span class="n">intercept_loo</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Computed from 4000 posterior samples and 100 observations log-likelihood matrix.

         Estimate       SE
elpd_loo -2253.10   195.98
p_loo       44.63        -
------

Pareto k diagnostic values:
                         Count   Pct.
(-Inf, 0.5]   (good)      100  100.0%
 (0.5, 0.7]   (ok)          0    0.0%
   (0.7, 1]   (bad)         0    0.0%
   (1, Inf)   (very bad)    0    0.0%
</pre></div>
</div>
</div>
</div>
<p>Infine, calcoliamo <code class="docutils literal notranslate"><span class="pre">eldp_diff</span></code>. L’incertezza di questa quantità è espressa dall’errore standard. Se il rapporto tra <code class="docutils literal notranslate"><span class="pre">eldp_diff</span></code> e il suo errore standard è almeno uguale a 2, allora possiamo concludere che vi è una differenza credibile tra di due modelli.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_comp_loo</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">compare</span><span class="p">({</span><span class="s2">&quot;linear_model&quot;</span><span class="p">:</span> <span class="n">linear_trace</span><span class="p">,</span> <span class="s2">&quot;intercept_model&quot;</span><span class="p">:</span> <span class="n">intercept_trace</span><span class="p">})</span>
<span class="n">df_comp_loo</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/corrado/opt/anaconda3/envs/pymc_env/lib/python3.11/site-packages/arviz/stats/stats.py:307: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value &#39;False&#39; has dtype incompatible with float64, please explicitly cast to a compatible dtype first.
  df_comp.loc[val] = (
/Users/corrado/opt/anaconda3/envs/pymc_env/lib/python3.11/site-packages/arviz/stats/stats.py:307: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value &#39;log&#39; has dtype incompatible with float64, please explicitly cast to a compatible dtype first.
  df_comp.loc[val] = (
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>rank</th>
      <th>elpd_loo</th>
      <th>p_loo</th>
      <th>elpd_diff</th>
      <th>weight</th>
      <th>se</th>
      <th>dse</th>
      <th>warning</th>
      <th>scale</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>linear_model</th>
      <td>0</td>
      <td>-244.454127</td>
      <td>2.749929</td>
      <td>0.000000</td>
      <td>1.0</td>
      <td>6.895352</td>
      <td>0.00000</td>
      <td>False</td>
      <td>log</td>
    </tr>
    <tr>
      <th>intercept_model</th>
      <td>1</td>
      <td>-2253.095052</td>
      <td>44.632889</td>
      <td>2008.640924</td>
      <td>0.0</td>
      <td>195.984692</td>
      <td>195.64589</td>
      <td>False</td>
      <td>log</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Nel caso presente, sappiamo che il modello che include una relazione lineare tra le due variabili è quello che rispecchia il modo in cui i dati sono stati generati. Infatti, troviamo che il rapporto tra <code class="docutils literal notranslate"><span class="pre">eldp_diff</span></code> e il suo errore standard è molto maggiore di 2, il che conferma che, per questi dati, il modello lineare è da preferire al modello che include solo l’intercetta.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_compare</span><span class="p">(</span><span class="n">df_comp_loo</span><span class="p">,</span> <span class="n">insample_dev</span><span class="o">=</span><span class="kc">False</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/corrado/opt/anaconda3/envs/pymc_env/lib/python3.11/site-packages/arviz/plots/backends/matplotlib/compareplot.py:87: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  scale = comp_df[&quot;scale&quot;][0]
</pre></div>
</div>
<img alt="../_images/286bf6dbe45b6df467bf5b4b217165b62a0c05b9b01d7dd9b5b613b5d77b90bb.png" src="../_images/286bf6dbe45b6df467bf5b4b217165b62a0c05b9b01d7dd9b5b613b5d77b90bb.png" />
</div>
</div>
</section>
</section>
<section id="commenti-e-considerazioni-finali">
<h2>Commenti e Considerazioni Finali<a class="headerlink" href="#commenti-e-considerazioni-finali" title="Permalink to this heading">#</a></h2>
<p>Nel corso di questo capitolo, abbiamo esplorato in profondità il concetto della Densità Logaritmica Predittiva Prevista (ELPD), una metrica fondamentale nell’ambito dell’analisi statistica bayesiana. L’ELPD si distingue non solo come strumento per valutare le prestazioni di un modello statistico, ma assume un ruolo cruciale nel confronto tra diversi modelli.</p>
<p>L’adozione dell’ELPD come criterio di confronto offre una prospettiva oggettiva e basata sui dati per determinare quale modello sia più aderente e rappresentativo rispetto alle informazioni a nostra disposizione. In un panorama scientifico e applicativo di crescente complessità, dove le alternative di modellazione si moltiplicano e divergono, l’ELPD emerge come un faro guida per orientare la scelta verso il modello più appropriato. Questa metrica si rivela particolarmente utile sia nel confronto tra modelli di diversa complessità, sia nella selezione tra vari approcci modellistici, fornendo una valutazione quantitativa e imparziale della loro capacità predittiva.</p>
<p>In sintesi, l’ELPD rappresenta uno strumento imprescindibile nell’arsenale dell’analista dei dati moderno. Il suo impiego in fase di scelta e valutazione dei modelli contribuisce in modo significativo a chiarire e guidare le decisioni analitiche, portando a risultati più robusti e modelli statistici maggiormente efficaci. In definitiva, l’ELPD non solo facilita la comprensione delle performance dei modelli, ma promuove anche una maggiore fiducia nelle previsioni che essi generano, un aspetto fondamentale in qualsiasi campo di applicazione statistica.</p>
</section>
<section id="watermark">
<h2>Watermark<a class="headerlink" href="#watermark" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">load_ext</span> watermark
<span class="o">%</span><span class="k">watermark</span> -n -u -v -iv -w
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Last updated: Sun Dec 10 2023

Python implementation: CPython
Python version       : 3.11.6
IPython version      : 8.18.0

arviz      : 0.16.1
pandas     : 2.1.3
scipy      : 1.11.4
seaborn    : 0.13.0
numpy      : 1.26.2
matplotlib : 3.8.2
pymc       : 5.10.0
statsmodels: 0.14.0

Watermark: 2.4.3
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_4"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="30_entropy.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Entropia</p>
      </div>
    </a>
    <a class="right-next"
       href="40_hier_beta_binom.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Modello gerarchico beta-binomiale</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#perdita-di-informazione-e-divergenza-di-kullback-leibler">Perdita di Informazione e Divergenza di Kullback-Leibler</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#un-esempio-empirico">Un Esempio Empirico</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#la-divergenza-dipende-dalla-direzione">La Divergenza Dipende dalla Direzione</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confronto-tra-modelli-tramite-la-divergenza-mathbb-kl">Confronto tra Modelli Tramite la Divergenza <span class="math notranslate nohighlight">\(\mathbb{KL}\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distribuzione-predittiva-a-posteriori">Distribuzione Predittiva a Posteriori</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#valutazione-della-somiglianza-tra-distribuzioni">Valutazione della Somiglianza tra Distribuzioni</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#confronto-tra-modelli-multipli">Confronto tra Modelli Multipli</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expected-log-predictive-density">Expected Log Predictive Density</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approssimazione-e-stima">Approssimazione e Stima</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Un Esempio Empirico</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#un-secondo-esempio-empirico">Un Secondo Esempio Empirico</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#una-singola-osservazione">Una Singola Osservazione</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tutte-le-osservazioni-del-campione">Tutte le Osservazioni del Campione</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#leave-one-out-cross-validation-loo-cv">Leave One Out Cross-Validation (LOO-CV)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#valori-diagnostici-pareto-k">Valori Diagnostici Pareto <span class="math notranslate nohighlight">\(k\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ruolo-dell-elpd-nella-valutazione-comparativa-dei-modelli">Ruolo dell’ELPD nella Valutazione Comparativa dei Modelli</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simulazione">Simulazione</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#commenti-e-considerazioni-finali">Commenti e Considerazioni Finali</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#watermark">Watermark</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Corrado Caudek
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>