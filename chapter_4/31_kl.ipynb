{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://colab.research.google.com/github/ccaudek/ds4psy_2023/blob/main/kl.ipynb\">![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)</a>\n",
    "\n",
    "\n",
    "(kl_notebook)=\n",
    "# Kullback-Leibler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel contesto di molti ambiti della statistica e del machine learning, non è raro trovare la necessità di utilizzare una distribuzione di probabilità $ q $ per approssimare un'altra distribuzione $ p $. Questo può essere particolarmente utile quando la distribuzione $ p $ è sconosciuta o troppo complessa per essere trattata direttamente. Di conseguenza, emerge una questione cruciale: quanta informazione viene perduta utilizzando $ q $ al posto di $ p $? Qual è l'incremento di incertezza introdotto nell'analisi statistica a causa di questa approssimazione?\n",
    "\n",
    "Per rispondere a questa domanda, una misura chiave è la *Divergenza di Kullback-Leibler* (KL). La Divergenza KL, denotata come $ \\mathbb{KL}(p \\mid\\mid q) $, rappresenta la discrepanza tra le due distribuzioni di probabilità. Essa assume il valore di zero quando $ q $ è identica a $ p $ e aumenta con la differenza tra le due distribuzioni. Pertanto, la Divergenza KL funge da criterio per valutare la 'vicinanza' tra due distribuzioni di probabilità.\n",
    "\n",
    "Oltre alla Divergenza KL, un altro concetto rilevante in questo contesto è l'*Expected Log Predictive Density* (elpd). L'elpd è una misura di quanto bene un modello si adatta ai dati osservati e può essere utilizzata per confrontare diversi modelli, fornendo un equilibrio tra adattamento e complessità del modello.\n",
    "\n",
    "Questo capitolo mira ad introdurre ed esplorare in profondità sia la Divergenza KL che l'elpd, evidenziando la loro centralità nella valutazione e nel confronto dei modelli statistici, in particolare nel contesto bayesiano. Attraverso una comprensione dettagliata di questi concetti, il lettore sarà in grado di applicare queste metriche in modo efficace nella selezione e nell'analisi di modelli appropriati per specifici set di dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics import tsaplots\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc as pm\n",
    "import arviz as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext watermark\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perdita di informazione e divergenza di Kullback-Leibler\n",
    "\n",
    "Per quantificare la perdita di informazione che si verifica quando utilizziamo una distribuzione di probabilità approssimata $ q $ al posto della distribuzione corretta $ p $, abbiamo bisogno di una misura che sia zero quando $ q = p $, e positiva quando $ q \\neq p $. \n",
    "\n",
    "Una tale misura è fornita dalla *divergenza di Kullback-Leibler* (o *entropia relativa*), che è definita come il valore atteso della differenza tra $ \\log(p) $ e $ \\log(q) $, calcolato rispetto alla distribuzione $ p $. In formula:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbb{KL} (p \\mid\\mid q) = \\mathbb{E}_p [\\log p - \\log q] = \\sum_i^n p_i (\\log p_i - \\log q_i),\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "dove $ \\mathbb{KL} (p \\mid\\mid q) $ quantifica la differenza media nelle probabilità logaritmiche quando utilizziamo $ q $ per approssimare $ p $. \n",
    "\n",
    "È importante notare che la divergenza di Kullback-Leibler non è simmetrica: $ \\mathbb{KL} (p \\mid\\mid q) \\neq \\mathbb{KL} (q \\mid\\mid p) $.\n",
    "\n",
    "### Relazione con l'entropia\n",
    "\n",
    "La divergenza di Kullback-Leibler può essere riscritta in termini dell'entropia $ h(p) $ di $ p $ e dell'entropia incrociata $ h(p, q) $ tra $ p $ e $ q $, come segue:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbb{KL} (p \\mid\\mid q) = h(p, q) - h(p),\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "dove\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "h(p) & = -\\sum_i^n p_i \\log p_i, \\\\\n",
    "h(p, q) & = -\\sum_i^n p_i \\log q_i.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In questa espressione, $ h(p) $ è l'entropia di $ p $, che misura l'incertezza media associata agli eventi secondo la distribuzione $ p $. $ h(p, q) $ è l'entropia incrociata tra $ p $ e $ q $, che rappresenta l'incertezza media quando si usano le log-probabilità di $ q $, ma ponderate con le probabilità di $ p $.\n",
    "\n",
    "Per chiarire: l'entropia $h(p)$ di una distribuzione $p$ è una misura dell'incertezza intrinseca in quella distribuzione. L'entropia incrociata $h(p, q)$ tra due distribuzioni $p$ e $q$ misura l'incertezza media quando si usano le log-probabilità di $q$, ma ponderate con le probabilità di $p$. Essa rappresenta la quantità di informazione media necessaria per codificare gli eventi di $p$ usando il codice ottimale per $q$.\n",
    "\n",
    "La divergenza di Kullback-Leibler (KL) tra $p$ e $q$ è una misura della differenza tra queste due distribuzioni. Non è simmetrica e può essere vista come la quantità di informazione \"persa\" quando si usa $q$ per approssimare $p$.\n",
    "\n",
    "### Interpretazione\n",
    "\n",
    "La divergenza di Kullback-Leibler può essere interpretata come la differenza tra l'entropia incrociata $ h(p, q) $ e l'entropia $ h(p) $. In altre parole, rappresenta quanto l'entropia (o incertezza) aumenta quando utilizziamo la distribuzione $ q $ per approssimare la distribuzione $ p $ invece di utilizzare $ p $ stessa.\n",
    "\n",
    "Se $ \\mathbb{KL} (p \\mid\\mid q) = 0 $, questo indica che le due distribuzioni sono identiche e non c'è perdita di informazione nell'utilizzare $ q $ al posto di $ p $. Se $ \\mathbb{KL} (p \\mid\\mid q) $ è positiva, indica la quantità di informazione che viene persa quando $ q $ viene utilizzata per rappresentare $ p $.\n",
    "\n",
    "### Un esempio empirico\n",
    "\n",
    "Ora vediamo un esempio numerico in Python per illustrare meglio questi concetti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropia di p:  1.4854752972273344\n",
      "Entropia incrociata tra p e q:  1.5539580943104374\n",
      "Divergenza KL da p a q:  0.06848279708310301\n"
     ]
    }
   ],
   "source": [
    "# Definiamo due distribuzioni di probabilità discreta p e q\n",
    "p = np.array([0.2, 0.5, 0.3])\n",
    "q = np.array([0.1, 0.6, 0.3])\n",
    "\n",
    "# Calcoliamo l'entropia di p\n",
    "h_p = -np.sum(p * np.log2(p))\n",
    "\n",
    "# Calcoliamo l'entropia incrociata tra p e q\n",
    "h_pq = -np.sum(p * np.log2(q))\n",
    "\n",
    "# Calcoliamo la divergenza di Kullback-Leibler da p a q\n",
    "kl_pq = h_pq - h_p\n",
    "\n",
    "# Stampiamo i risultati\n",
    "print(\"Entropia di p: \", h_p)\n",
    "print(\"Entropia incrociata tra p e q: \", h_pq)\n",
    "print(\"Divergenza KL da p a q: \", kl_pq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideriamo un secondo esempio. Sia $p$ una distribuzione binomiale di parametri $\\theta = 0.2$ e $n = 5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4096 0.4096 0.1536 0.0256 0.0016]\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters\n",
    "n = 4\n",
    "p = 0.2\n",
    "\n",
    "# Compute the probability mass function\n",
    "true_py = stats.binom.pmf(range(n + 1), n, p)\n",
    "print(true_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sia $q_1$ una approssimazione a $p$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46 0.42 0.1  0.01 0.01]\n"
     ]
    }
   ],
   "source": [
    "q1 = np.array([0.46, 0.42, 0.10, 0.01, 0.01])\n",
    "print(q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sia $q_2$ una distribuzione uniforme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2, 0.2, 0.2, 0.2, 0.2]\n"
     ]
    }
   ],
   "source": [
    "q2 = [0.2] * 5\n",
    "print(q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La divergenza $\\mathbb{KL}$ di $q_1$ da $p$ è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.029251990333458988"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(true_py * np.log(true_py / q1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La divergenza $\\mathbb{KL}$ di $q_2$ da $p$ è:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4863577787141544"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(true_py * np.log(true_py / q2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "È chiaro che perdiamo una quantità maggiore di informazioni se, per descrivere la distribuzione binomiale $p$, usiamo la distribuzione uniforme $q_2$ anziché $q_1$.\n",
    "\n",
    "### La divergenza dipende dalla direzione\n",
    "\n",
    "La divergenza $\\mathbb{KL}$ non è una vera e propria metrica: per esempio, non è simmetrica. In generale, $\\mathbb{KL}(p \\mid\\mid q) \\neq \\mathbb{KL}(q \\mid\\mid p)$, ovvero la $\\mathbb{KL}$ da $p$ a $q$ è diversa dalla $\\mathbb{KL}$ da $q$ a $p$.\n",
    "\n",
    "Consideriamo il seguente esempio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  direction   p_1   q_1   p_2   q_2      d_kl\n",
      "0  Da q a p  0.01  0.70  0.99  0.30  1.139498\n",
      "1  Da p a q  0.70  0.01  0.30  0.99  2.615770\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame\n",
    "data = {\"direction\": [\"Da q a p\", \"Da p a q\"], \"p_1\": [0.01, 0.7], \"q_1\": [0.7, 0.01]}\n",
    "\n",
    "# Initialize the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Compute KL\n",
    "df[\"p_2\"] = 1 - df[\"p_1\"]\n",
    "df[\"q_2\"] = 1 - df[\"q_1\"]\n",
    "df[\"d_kl\"] = (df[\"p_1\"] * (df[\"p_1\"] / df[\"q_1\"]).apply(np.log)) + (\n",
    "    df[\"p_2\"] * (df[\"p_2\"] / df[\"q_2\"]).apply(np.log)\n",
    ")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confronto tra modelli\n",
    "\n",
    "La divergenza $\\mathbb{KL}$ viene utilizzata nel confronto tra modelli, ovvero ci consente di quantificare l'informazione che viene perduta quando utilizziamo la distribuzione di probabilità ipotizzata da un modello, chiamiamola $p_{\\mathcal{M}}$, per approssimare la distribuzione di probabilità del vero modello generatore dei dati, $p_t$.\n",
    "\n",
    "In precedenza abbiamo introdotto il concetto di distribuzione predittiva a posteriori:\n",
    "\n",
    "$$\n",
    "p(\\tilde{y} \\mid y) = \\int_\\Theta p(\\tilde{y} \\mid \\theta) p(\\theta \\mid y) \\,\\operatorname {d}\\!\\theta .\n",
    "$$\n",
    "\n",
    "La distribuzione predittiva a posteriori descrive il tipo di dati che ci aspettiamo vengano prodotti dal modello generativo $\\mathcal{M}$, alla luce delle nostre credenze iniziali, $p(\\theta)$, e dei dati osservati, $y$. Quando valutiamo un modello ci chiediamo in che misura $p_{\\mathcal{M}}(\\tilde{y} \\mid y)$ approssimi $p_t(\\tilde{y})$. Cioè, ci chiediamo quanto siano simili i dati $p_{\\mathcal{M}}(\\cdot)$ prodotti dal modello $\\mathcal{M}$ ai dati prodotti dal vero processo generatore dei dati $p_t(\\cdot)$.\n",
    "\n",
    "Una misura della \"somiglianza\" tra la distribuzione $q_{\\mathcal{M}}$ ipotizzata dal modello $\\mathcal{M}$ e la distribuzione $p_t$ del vero modello generatore dei dati è appunto fornita dalla divergenza di Kullback-Leibler $\\mathbb{KL}(p_t \\mid\\mid q_{\\mathcal{M}})$. \n",
    "\n",
    "Supponendo di avere $k$ modelli della distribuzione a posteriori, $\\{q_{\\mathcal{M}_1}, q_{\\mathcal{M}_2}, \\dots, q_{\\mathcal{M}_k}\\}$, e di conoscere il vero modello generatore dei dati, possiamo scrivere\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{KL} (p_t \\mid\\mid q_{\\mathcal{M}_1}) &= \\mathbb{E} (\\log p_{\\mathcal{M}_0}) - \\mathbb{E} (\\log q_{\\mathcal{M}_1})\\notag\\\\\n",
    "\\mathbb{KL} (p_t \\mid\\mid q_{\\mathcal{M}_2}) &= \\mathbb{E} (\\log p_{\\mathcal{M}_0}) - \\mathbb{E} (\\log q_{\\mathcal{M}_2})\\notag\\\\\n",
    "&\\cdots\\notag\\\\\n",
    "\\mathbb{KL} (p_t \\mid\\mid q_{\\mathcal{M}_k}) &= \\mathbb{E} (\\log p_{\\mathcal{M}_0}) - \\mathbb{E} (\\log q_{\\mathcal{M}_k}).\n",
    "\\end{align}\n",
    "$$ (eq-kl-mod-comp)\n",
    "\n",
    "La {eq}`eq-kl-mod-comp` può sembrare un esercizio futile poiché nella vita reale non conosciamo il vero modello generatore dei dati. È però facile rendersi conto che, poiché $p_t$ è la stessa per tutti i confronti, diventa possibile costruire un ordinamento dei modelli basato unicamente sul secondo termine della {eq}`eq-kl-mod-comp`, ovvero senza nessun riferimento al vero modello generatore dei dati. Per un generico modello $\\mathcal{M}$, il secondo termine della {eq}`eq-kl-mod-comp` può essere scritto come:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbb{E} \\log p_{\\mathcal{M}}(y) = \\int_{-\\infty}^{+\\infty}p_{t}(y)\\log p_{\\mathcal{M}}(y) \\,\\operatorname {d}\\!y .\n",
    "\\end{equation} \n",
    "$$ (eq-kl-div-cont-t2)\n",
    "\n",
    "## Expected log predictive density\n",
    "\n",
    "Le previsioni del modello $\\mathcal{M}$ sui nuovi dati futuri sono date dalla distribuzione predittiva a posteriori. Possiamo dunque riscrivere la {eq}`eq-kl-div-cont-t2` come\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "elpd = \\int_{\\tilde{y}} p_{t}(\\tilde{y}) \\log p(\\tilde{y} \\mid y) \\,\\operatorname {d}\\!\\tilde{y}.\n",
    "\\end{equation}\n",
    "$$ (eq-elpd)\n",
    "\n",
    "L'eq. {eq}`eq-elpd` è chiamata *expected log predictive density* ($elpd$) e fornisce la risposta al problema che ci eravamo posti: nel confronto tra modelli, come è possibile scegliere il modello più simile al vero meccanismo generatore dei dati? Possiamo pensare alla {eq}`eq:elpd` dicendo che descrive la distribuzione predittiva a posteriori del modello ponderando la verosimiglianza dei possibili (sconosciuti) dati futuri ($\\tilde{y}$) con la vera distribuzione $p_t$. Di conseguenza, valori $elpd$ più grandi identificano il modello che risulta più simile al vero meccanismo generatore dei dati.\n",
    "\n",
    "Non dobbiamo preoccuparci di trovare una formulazione analitica della distribuzione predittiva a posteriori $p(\\tilde{y} \\mid y)$ perché è possibile approssimare tale distribuzione mediante simulazione. Notiamo però che la {eq}`eq:elpd` include un termine, $p_t(\\tilde{y})$, il quale descrive la distribuzione dei dati futuri $\\tilde{y}$ secondo il vero modello generatore dei dati. Il termine $p_t$, ovviamente, è ignoto. Di conseguenza, la quantità $elpd$ non può mai essere calcolata in maniera esatta, ma può solo essere stimata. Il secondo problema di questo capitolo è capire come la {eq}`eq-elpd` possa essere stimata utilizzando un campione di osservazioni.\n",
    "\n",
    "### Un esempio pratico\n",
    "\n",
    "Esaminiamo un esempio tratto da [Bayesian Data Analysis for Cognitive Science](https://vasishth.github.io/bayescogsci/book/expected-log-predictive-density-of-a-model.html) nel quale la $lppd$ viene calcolata in forma esatta oppure mediante approssimazione. Supponiamo di disporre di un campione di $n$ osservazioni. Supponiamo inoltre di conoscere il vero processo generativo dei dati (qualcosa che in pratica non è mai possibile), ovvero:\n",
    "\n",
    "$$\n",
    "p_t(y) = Beta(1, 3).\n",
    "$$ \n",
    "\n",
    "I dati sono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21810354 0.05532253 0.13545025 0.42102514 0.14728608 0.11052926]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(75)\n",
    "\n",
    "# Number of samples\n",
    "n = 10000\n",
    "\n",
    "# Draw samples from a Beta distribution\n",
    "y_data = np.random.beta(1, 3, n)\n",
    "\n",
    "# Print the first 6 elements of the array, equivalent to R's head function\n",
    "print(y_data[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Supponiamo inoltre di avere adattato ai dati un modello bayesiano $\\mathcal{M}$ e di avere ottenuto la distribuzione a posteriori per i parametri del modello. Inoltre, supponiamo di avere derivato la forma analitica della distribuzione predittiva a posteriori per il modello:\n",
    "\n",
    "$$\n",
    "p(y^{rep} \\mid y) \\sim Beta(2, 2).\n",
    "$$\n",
    "\n",
    "Questa distribuzione ci dice quanto sono credibili i possibili dati futuri.\n",
    "\n",
    "Conoscendo la vera distribuzione dei dati $p_t(y)$ possiamo calcolare in forma esatta la quantità $elpd$, ovvero\n",
    "\n",
    "$$\n",
    "elpd = \\int_{y^{rep}}p_{t}(y^{rep})\\log p(y^{rep} \\mid y) \\,\\operatorname {d}\\!y^{rep}.\n",
    "$$\n",
    "\n",
    "Svolgiamo i calcoli otteniamo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3749071974384449\n",
      "3.0236368964153826e-12\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import beta\n",
    "from scipy.integrate import quad\n",
    "\n",
    "# True distribution\n",
    "def p_t(y):\n",
    "    return beta.pdf(y, 1, 3)\n",
    "\n",
    "# Predictive distribution\n",
    "def p(y):\n",
    "    return beta.pdf(y, 2, 2)\n",
    "\n",
    "# Integration\n",
    "def integrand(y):\n",
    "    return p_t(y) * np.log(p(y))\n",
    "\n",
    "# Perform numerical integration\n",
    "result, error = quad(integrand, 0, 1)\n",
    "\n",
    "print(result)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuttavia, in pratica non conosciamo mai $p_t(y)$. Quindi approssimiamo $elpd$ usando la {eq}`eq-elpd`:\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^n \\log p(y_i \\mid y).\n",
    "$$\n",
    "\n",
    "Così facendo otteniamo un valore approssimato a quello trovato in precedenza:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3721938430299501"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/n * np.sum(np.log(p(y_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave One Out Cross-Validation \n",
    "\n",
    "Nell'ambito della statistica bayesiana, il confronto tra modelli può essere eseguito attraverso una stima dell'Expected Log Predictive Density (elpd), utilizzando una procedura nota come *Leave One Out Cross-Validation* (LOO-CV). PyMC è uno degli strumenti che possono facilitare questo processo. Di seguito, illustreremo come funziona:\n",
    "\n",
    "1. **Si rimuove un'osservazione**: si inizia rimuovendo una singola osservazione $ y_i $ dal set di dati.\n",
    "2. **Si adatta il modello**: si adatta il modello statistico alle restanti $ N-1 $ osservazioni. Questo significa che il modello viene adattato senza quella specifica osservazione.\n",
    "3. **Si predice l'osservazione rimossa**: si utilizza il modello adattato per prevedere l'osservazione rimossa $ y_i $. Si calcola il logaritmo della densità predittiva per l'osservazione effettivamente osservata.\n",
    "4. **Si ripete per ogni osservazione**: Si ripetono i passaggi 1-3 per ogni osservazione nel set di dati.\n",
    "5. **Si calcola l'elpd**: Si sommano tutti i logaritmi delle densità predittive calcolate. Questa somma rappresenta una stima dell'Expected Log Predictive Density (elpd).\n",
    "\n",
    "In un contesto Bayesiano, il calcolo della distribuzione predittiva per ogni osservazione può essere più complesso, in quanto richiede l'integrazione su tutte le possibili combinazioni dei parametri, pesate dalle loro probabilità posteriori. Tuttavia, la libreria PyMC automatizza questo processo.\n",
    "\n",
    "### Utilizzo dell'elpd nel confronto di modelli\n",
    "\n",
    "L'elpd calcolato può essere utilizzato per il confronto tra modelli diversi, fornendo una misura oggettiva per determinare quale modello si adatta meglio ai dati osservati. Questa misura può essere particolarmente preziosa quando si deve scegliere il modello più appropriato tra diverse alternative o valutare se un modello più complesso fornisce un miglior adattamento rispetto a uno più semplice.\n",
    "\n",
    "### Simulazione\n",
    "\n",
    "Per illustrare questa procedura, eseguiamo una simulazione. Generiamo dei dati sintetici nei quali esiste una relazione lineare tra x e y. In questo contesto, potremmo voler confrontare un modello lineare con un modello più semplice, che include solo l'intercetta, ad esempio, e utilizzare la procedura LOO-CV per determinare quale modello si adatta meglio ai dati. La stima dell'elpd fornirà un criterio quantitativo per questo confronto, guidando la selezione del modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 100)\n",
    "y_true = 3 + 2 * X\n",
    "y_obs = y_true + np.random.normal(scale=3, size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adattiamo ai dati un modello che rispecchia il vero meccanismo generativo dei dati.\n",
    "\n",
    "Si noti che, per calcolare LOO e WAIC, ArviZ ha bisogno di accedere alla log-likelihood per ogni campione posteriore. Possiamo trovarla tramite `compute_log_likelihood()`. In alternativa, possiamo passare `idata_kwargs={\"log_likelihood\": True}` a `sample()` per farla calcolare automaticamente alla fine del campionamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [alpha, beta, sigma]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='8000' class='' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [8000/8000 00:03&lt;00:00 Sampling 4 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 18 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Linear model\n",
    "with pm.Model() as linear_model:\n",
    "    alpha = pm.Normal(\"alpha\", mu=0, sigma=10)\n",
    "    beta = pm.Normal(\"beta\", mu=0, sigma=10)\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=10)\n",
    "    mu = alpha + beta * X\n",
    "    y = pm.Normal(\"y\", mu=mu, sigma=sigma, observed=y_obs)\n",
    "    linear_trace = pm.sample(1000, tune=1000, idata_kwargs={\"log_likelihood\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adattiamo ora un secondo modello che non tiene conto della relazione lineare tra x e y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [alpha]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='8000' class='' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [8000/8000 00:01&lt;00:00 Sampling 4 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 19 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Intercept model\n",
    "with pm.Model() as intercept_model:\n",
    "    alpha = pm.Normal(\"alpha\", mu=0, sigma=10)\n",
    "    mu = alpha \n",
    "    y = pm.Normal(\"y\", mu=mu, observed=y_obs)\n",
    "    intercept_trace = pm.sample(1000, tune=1000, idata_kwargs={\"log_likelihood\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Troviamo ora elpd con il metodo LOO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Computed from 4000 posterior samples and 100 observations log-likelihood matrix.\n",
       "\n",
       "         Estimate       SE\n",
       "elpd_loo  -244.62     6.92\n",
       "p_loo        2.91        -\n",
       "------\n",
       "\n",
       "Pareto k diagnostic values:\n",
       "                         Count   Pct.\n",
       "(-Inf, 0.5]   (good)      100  100.0%\n",
       " (0.5, 0.7]   (ok)          0    0.0%\n",
       "   (0.7, 1]   (bad)         0    0.0%\n",
       "   (1, Inf)   (very bad)    0    0.0%"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_loo = az.loo(linear_trace)\n",
    "linear_loo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Computed from 4000 posterior samples and 100 observations log-likelihood matrix.\n",
       "\n",
       "         Estimate       SE\n",
       "elpd_loo -2251.69   195.87\n",
       "p_loo       41.90        -\n",
       "------\n",
       "\n",
       "Pareto k diagnostic values:\n",
       "                         Count   Pct.\n",
       "(-Inf, 0.5]   (good)      100  100.0%\n",
       " (0.5, 0.7]   (ok)          0    0.0%\n",
       "   (0.7, 1]   (bad)         0    0.0%\n",
       "   (1, Inf)   (very bad)    0    0.0%"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intercept_loo = az.loo(intercept_trace)\n",
    "intercept_loo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infine, calcoliamo `eldp_diff`. L'incertezza di questa quantità è espressa dall'errore standard. Se il rapporto tra `eldp_diff` e il suo errore standard è almeno uguale a 2, allora possiamo concludere che vi è una differenza credibile tra di due modelli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>elpd_loo</th>\n",
       "      <th>p_loo</th>\n",
       "      <th>elpd_diff</th>\n",
       "      <th>weight</th>\n",
       "      <th>se</th>\n",
       "      <th>dse</th>\n",
       "      <th>warning</th>\n",
       "      <th>scale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>linear_model</th>\n",
       "      <td>0</td>\n",
       "      <td>-244.585681</td>\n",
       "      <td>2.872374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.898450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept_model</th>\n",
       "      <td>1</td>\n",
       "      <td>-2251.690908</td>\n",
       "      <td>41.901099</td>\n",
       "      <td>2007.105227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>195.868502</td>\n",
       "      <td>195.548862</td>\n",
       "      <td>False</td>\n",
       "      <td>log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 rank     elpd_loo      p_loo    elpd_diff  weight  \\\n",
       "linear_model        0  -244.585681   2.872374     0.000000     1.0   \n",
       "intercept_model     1 -2251.690908  41.901099  2007.105227     0.0   \n",
       "\n",
       "                         se         dse  warning scale  \n",
       "linear_model       6.898450    0.000000    False   log  \n",
       "intercept_model  195.868502  195.548862    False   log  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comp_loo = az.compare({\"linear_model\": linear_trace, \"intercept_model\": intercept_trace})\n",
    "df_comp_loo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel caso presente, sappiamo che il modello che include una relazione lineare tra le due variabili è quello che rispecchia il modo in cui i dati sono stati generati. Infatti, troviamo che il rapporto tra `eldp_diff` e il suo errore standard è molto maggiore di 2, il che conferma che, per questi dati, il modello lineare è da preferire al modello che include solo l'intercetta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commenti e considerazioni finali \n",
    "\n",
    "Il capitolo ha illustrato l'importanza e l'applicabilità dell'Expected Log Predictive Density (elpd) nel contesto della modellazione statistica. Questa misura emerge come uno strumento cruciale, non solo per l'analisi delle proprietà di un singolo modello ma anche per il confronto tra modelli diversi.\n",
    "\n",
    "L'utilizzo dell'elpd come metrica di confronto offre una prospettiva oggettiva, permettendo di determinare con precisione quale modello si adatta meglio ai dati osservati. In un panorama sempre più complesso, dove spesso si presentano molteplici alternative di modellazione, l'elpd diventa un alleato prezioso nella selezione del modello più appropriato. Che si tratti di valutare l'efficacia di un modello più complesso rispetto a uno più semplice o di scegliere tra varianti sostanzialmente differenti, l'elpd fornisce una guida solida e basata sui dati.\n",
    "\n",
    "In sintesi, il calcolo e l'interpretazione dell'elpd rivestono un ruolo centrale nella moderna analisi dei dati. La sua applicazione può ridurre l'ambiguità nelle scelte di modellazione, conducendo a conclusioni più robuste e a modelli più efficaci. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc5_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
